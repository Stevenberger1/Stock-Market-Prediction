{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook i will create and train the first prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code reads the dataframe from the saved csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True\n",
    ")\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data quality checker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_data_problems(df):\n",
    "    problems_summary = {}\n",
    "\n",
    "    # 1) Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\">> MISSING VALUES found per column:\")\n",
    "        print(missing[missing > 0])\n",
    "        problems_summary['missing_values'] = missing[missing > 0].to_dict()\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "        problems_summary['missing_values'] = {}\n",
    "    \n",
    "    # 2) Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\">> DUPLICATE ROWS found: {duplicate_count}\")\n",
    "        problems_summary['duplicate_rows'] = duplicate_count\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "        problems_summary['duplicate_rows'] = 0\n",
    "    \n",
    "    # 3) Negative or zero price/volume checks\n",
    "    price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in df.columns]\n",
    "    volume_cols = [col for col in ['Volume BTC', 'Volume USDT', 'Volume USD', 'volume'] if col in df.columns]\n",
    "\n",
    "    invalid_prices = {}\n",
    "    for col in price_cols:\n",
    "        non_pos = (df[col] <= 0).sum()\n",
    "        if non_pos > 0:\n",
    "            invalid_prices[col] = non_pos\n",
    "    \n",
    "    invalid_volumes = {}\n",
    "    for col in volume_cols:\n",
    "        non_pos = (df[col] <= 0).sum()\n",
    "        if non_pos > 0:\n",
    "            invalid_volumes[col] = non_pos\n",
    "\n",
    "    if invalid_prices:\n",
    "        print(\">> INVALID (≤0) PRICE VALUES found:\")\n",
    "        for c, count in invalid_prices.items():\n",
    "            print(f\"   Column '{c}': {count} rows\")\n",
    "        problems_summary['invalid_prices'] = invalid_prices\n",
    "    else:\n",
    "        print(\"No invalid (zero/negative) price values found.\")\n",
    "        problems_summary['invalid_prices'] = {}\n",
    "    \n",
    "    if invalid_volumes:\n",
    "        print(\">> INVALID (≤0) VOLUME VALUES found:\")\n",
    "        for c, count in invalid_volumes.items():\n",
    "            print(f\"   Column '{c}': {count} rows\")\n",
    "        problems_summary['invalid_volumes'] = invalid_volumes\n",
    "    else:\n",
    "        print(\"No invalid (zero/negative) volume values found.\")\n",
    "        problems_summary['invalid_volumes'] = {}\n",
    "\n",
    "    # 4) Out-of-order date index check (only if index is datetime-like)\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "            print(\">> The date index is NOT strictly increasing. Some timestamps may be out of order.\")\n",
    "            problems_summary['date_order'] = \"Not strictly increasing\"\n",
    "        else:\n",
    "            print(\"Date index is in ascending order (strictly increasing).\")\n",
    "            problems_summary['date_order'] = \"Ascending\"\n",
    "    else:\n",
    "        print(\"Index is not a DatetimeIndex (skipping date-order check).\")\n",
    "        problems_summary['date_order'] = None\n",
    "\n",
    "    # 5) Data-type checks for numeric columns\n",
    "    numeric_checks = {}\n",
    "    for col in price_cols + volume_cols:\n",
    "        if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numeric_checks[col] = \"Non-numeric type\"\n",
    "    if numeric_checks:\n",
    "        print(\">> NON-NUMERIC COLUMNS found (expected numeric):\")\n",
    "        for c, msg in numeric_checks.items():\n",
    "            print(f\"   Column '{c}' => {msg}\")\n",
    "    else:\n",
    "        print(\"All price/volume columns have numeric types.\")\n",
    "    problems_summary['non_numeric_columns'] = numeric_checks\n",
    "\n",
    "    print(\"\\n=== DATA QUALITY CHECK COMPLETE ===\\n\")\n",
    "    return problems_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run the data-quality checker using your df_prices DataFrame\n",
    "problems_report = check_data_problems(df)\n",
    "\n",
    "# 4. If you want to do something programmatic with the results:\n",
    "print(\"Problems Summary (as dict):\")\n",
    "print(problems_report)\n",
    "\n",
    "# Additionally, display the rows where invalid (≤0) PRICE VALUES are found\n",
    "if 'open' in df.columns:\n",
    "    invalid_open = df[df['open'] <= 0]\n",
    "    print(\"\\nRows with invalid 'open' values (≤0):\")\n",
    "    print(invalid_open)\n",
    "\n",
    "if 'low' in df.columns:\n",
    "    invalid_low = df[df['low'] <= 0]\n",
    "    print(\"\\nRows with invalid 'low' values (≤0):\")\n",
    "    print(invalid_low)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code adds a column for the next hour price ,\n",
    "#  in order for us to do the prediction calculation\n",
    "df[\"next_hour\"] = df[\"close\"].shift(-1)\n",
    "\n",
    "#When you create df[\"next_hour\"] = df[\"close\"].shift(-1), you are explicitly telling your model\n",
    "# (or your data pipeline) that your prediction target is the close price of the next row (i.e., the next hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code checks if the price went up from a ceratin hour to its next hour\n",
    "# and the  result type was converted to int for our ML model.\n",
    "df[\"Target\"] = (df[\"next_hour\"] > df[\"close\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code removes all the data that came before a certain date \n",
    "#(I used a copy of our dataframe just to check that it works) , the code for our df is \n",
    "# df=df.loc[\"2016-01-30\":].copy()\n",
    "#dfcopy=df.loc[\"2016-01-30\":].copy()\n",
    "#print(dfcopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a sample code snippet that uses scikit-learn’s metrices to search for the best combination of n_estimators and min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_estimators:\n",
    "The number of trees in the forest. Increasing this number can improve performance (by reducing variance) up to a certain point, but it also increases computation time.\n",
    "\n",
    "min_samples_split:\n",
    "The minimum number of samples required to split an internal node. Higher values make the trees shallower (simpler), which can help prevent overfitting.\n",
    "'''\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "n_estimators_list = [100, 200, 300]\n",
    "min_samples_split_list = [2, 5, 10, 20]\n",
    "\n",
    "print(\"Starting manual hyperparameter evaluation:\\n\")\n",
    "\n",
    "# Loop through each combination one-by-one\n",
    "for n_est in n_estimators_list:\n",
    "    for min_split in min_samples_split_list:\n",
    "        print(f\"Evaluating combination: n_estimators = {n_est}, min_samples_split = {min_split}\")\n",
    "        model = RandomForestClassifier(n_estimators=n_est, min_samples_split=min_split, random_state=1)\n",
    "        \n",
    "        # Start timing the training and evaluation\n",
    "        start_time = time.time()\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        test_pred = model.predict(test[predictors])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(test[\"Target\"], test_pred)\n",
    "        prec = precision_score(test[\"Target\"], test_pred)\n",
    "        rec = recall_score(test[\"Target\"], test_pred)\n",
    "        f1 = f1_score(test[\"Target\"], test_pred)\n",
    "        \n",
    "        # For ROC-AUC, we need probability estimates\n",
    "        test_proba = model.predict_proba(test[predictors])[:, 1]\n",
    "        roc_auc = roc_auc_score(test[\"Target\"], test_proba)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(test[\"Target\"], test_pred)\n",
    "        class_report = classification_report(test[\"Target\"], test_pred)\n",
    "        \n",
    "        # Print out the metrics\n",
    "        print(f\"Accuracy:      {acc:.4f}\")\n",
    "        print(f\"Precision:     {prec:.4f}\")\n",
    "        print(f\"Recall:        {rec:.4f}\")\n",
    "        print(f\"F1 Score:      {f1:.4f}\")\n",
    "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"Classification Report:\")\n",
    "        print(class_report)\n",
    "        print(f\"Training & evaluation time: {elapsed_time:.2f} seconds\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "'''\n",
    " Accuracy:\n",
    "Overall, how many predictions are correct.\n",
    "\n",
    "Precision:\n",
    "Of the instances predicted as positive (price went up), how many were actually positive. This is critical if false positives are costly.\n",
    "\n",
    "Recall (Sensitivity):\n",
    "Of all the actual positive instances, how many did the model correctly capture. This is key if missing a positive event is expensive.\n",
    "\n",
    "F1-Score:\n",
    "The harmonic mean of precision and recall. It balances the two, which is especially useful when there’s an imbalance between classes.\n",
    "\n",
    "ROC-AUC Score:\n",
    "Provides an aggregate measure of performance across all classification thresholds, indicating how well the model distinguishes between classes.\n",
    "\n",
    "Confusion Matrix:\n",
    "Gives you a breakdown of true positives, false positives, true negatives, and false negatives. This is useful for understanding the types of errors your model makes. '\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameter grid for this evaluation\n",
    "n_estimators_list = [100, 200, 300]\n",
    "min_samples_split_list = [50, 100]\n",
    "\n",
    "print(\"Starting manual hyperparameter evaluation:\\n\")\n",
    "\n",
    "# Loop through each combination one by one\n",
    "for n_est in n_estimators_list:\n",
    "    for min_split in min_samples_split_list:\n",
    "        print(f\"Evaluating combination: n_estimators = {n_est}, min_samples_split = {min_split}\")\n",
    "        model = RandomForestClassifier(n_estimators=n_est, min_samples_split=min_split, random_state=1)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        test_pred = model.predict(test[predictors])\n",
    "        acc = accuracy_score(test[\"Target\"], test_pred)\n",
    "        prec = precision_score(test[\"Target\"], test_pred)\n",
    "        rec = recall_score(test[\"Target\"], test_pred)\n",
    "        f1 = f1_score(test[\"Target\"], test_pred)\n",
    "        test_proba = model.predict_proba(test[predictors])[:, 1]\n",
    "        roc_auc = roc_auc_score(test[\"Target\"], test_proba)\n",
    "        conf_matrix = confusion_matrix(test[\"Target\"], test_pred)\n",
    "        class_report = classification_report(test[\"Target\"], test_pred)\n",
    "        \n",
    "        # Print out the metrics\n",
    "        print(f\"Accuracy:      {acc:.4f}\")\n",
    "        print(f\"Precision:     {prec:.4f}\")\n",
    "        print(f\"Recall:        {rec:.4f}\")\n",
    "        print(f\"F1 Score:      {f1:.4f}\")\n",
    "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"Classification Report:\")\n",
    "        print(class_report)\n",
    "        print(f\"Training & evaluation time: {elapsed_time:.2f} seconds\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finlizaiton of the data accuracy mesarmuments table\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    # First batch\n",
    "    {\"n_estimators\":100, \"min_samples_split\": 2,  \"Accuracy\":0.5016, \"Precision\":0.5163, \"Recall\":0.3598, \"F1\":0.4241, \"ROC-AUC\":0.5037},\n",
    "    {\"n_estimators\":100, \"min_samples_split\": 5,  \"Accuracy\":0.5032, \"Precision\":0.5185, \"Recall\":0.3605, \"F1\":0.4253, \"ROC-AUC\":0.5075},\n",
    "    {\"n_estimators\":100, \"min_samples_split\":10,  \"Accuracy\":0.4959, \"Precision\":0.5090, \"Recall\":0.3241, \"F1\":0.3961, \"ROC-AUC\":0.5053},\n",
    "    {\"n_estimators\":100, \"min_samples_split\":20,  \"Accuracy\":0.4990, \"Precision\":0.5136, \"Recall\":0.3302, \"F1\":0.4020, \"ROC-AUC\":0.5041},\n",
    "\n",
    "    {\"n_estimators\":200, \"min_samples_split\": 2,  \"Accuracy\":0.4991, \"Precision\":0.5127, \"Recall\":0.3580, \"F1\":0.4216, \"ROC-AUC\":0.5039},\n",
    "    {\"n_estimators\":200, \"min_samples_split\": 5,  \"Accuracy\":0.5024, \"Precision\":0.5173, \"Recall\":0.3601, \"F1\":0.4247, \"ROC-AUC\":0.5063},\n",
    "    {\"n_estimators\":200, \"min_samples_split\":10,  \"Accuracy\":0.4968, \"Precision\":0.5104, \"Recall\":0.3249, \"F1\":0.3970, \"ROC-AUC\":0.5052},\n",
    "    {\"n_estimators\":200, \"min_samples_split\":20,  \"Accuracy\":0.4988, \"Precision\":0.5135, \"Recall\":0.3262, \"F1\":0.3989, \"ROC-AUC\":0.5045},\n",
    "\n",
    "    {\"n_estimators\":300, \"min_samples_split\": 2,  \"Accuracy\":0.4987, \"Precision\":0.5122, \"Recall\":0.3571, \"F1\":0.4208, \"ROC-AUC\":0.5049},\n",
    "    {\"n_estimators\":300, \"min_samples_split\": 5,  \"Accuracy\":0.5015, \"Precision\":0.5160, \"Recall\":0.3596, \"F1\":0.4238, \"ROC-AUC\":0.5063},\n",
    "    {\"n_estimators\":300, \"min_samples_split\":10, \"Accuracy\":0.4972, \"Precision\":0.5109, \"Recall\":0.3258, \"F1\":0.3979, \"ROC-AUC\":0.5060},\n",
    "    {\"n_estimators\":300, \"min_samples_split\":20, \"Accuracy\":0.4988, \"Precision\":0.5135, \"Recall\":0.3270, \"F1\":0.3996, \"ROC-AUC\":0.5051},\n",
    "\n",
    "    # Second batch\n",
    "    {\"n_estimators\":100, \"min_samples_split\": 50, \"Accuracy\":0.4973, \"Precision\":0.5117, \"Recall\":0.3131, \"F1\":0.3885, \"ROC-AUC\":0.5030},\n",
    "    {\"n_estimators\":100, \"min_samples_split\":100,\"Accuracy\":0.4990, \"Precision\":0.5155, \"Recall\":0.2930, \"F1\":0.3736, \"ROC-AUC\":0.5046},\n",
    "\n",
    "    {\"n_estimators\":200, \"min_samples_split\": 50, \"Accuracy\":0.4995, \"Precision\":0.5151, \"Recall\":0.3132, \"F1\":0.3896, \"ROC-AUC\":0.5043},\n",
    "    {\"n_estimators\":200, \"min_samples_split\":100,\"Accuracy\":0.4987, \"Precision\":0.5151, \"Recall\":0.2875, \"F1\":0.3691, \"ROC-AUC\":0.5047},\n",
    "\n",
    "    {\"n_estimators\":300, \"min_samples_split\": 50, \"Accuracy\":0.4993, \"Precision\":0.5149, \"Recall\":0.3135, \"F1\":0.3897, \"ROC-AUC\":0.5049},\n",
    "    {\"n_estimators\":300, \"min_samples_split\":100,\"Accuracy\":0.4995, \"Precision\":0.5166, \"Recall\":0.2891, \"F1\":0.3707, \"ROC-AUC\":0.5047},\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(data)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for visuilsaution of the meamrents datafarme\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame (optional, just for tidiness)\n",
    "df_results.sort_values([\"n_estimators\",\"min_samples_split\"], inplace=True)\n",
    "\n",
    "# Metrics we want to plot (excluding the parameters themselves)\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\"]\n",
    "\n",
    "# Reshape (melt) so we can do a grouped bar chart easily\n",
    "df_melted = df_results.melt(\n",
    "    id_vars=[\"n_estimators\",\"min_samples_split\"], \n",
    "    value_vars=metrics, \n",
    "    var_name=\"Metric\", \n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "# Create a \"label\" column to represent each combination on the x-axis\n",
    "df_melted[\"Combo\"] = df_melted.apply(\n",
    "    lambda row: f\"n={row['n_estimators']}, split={row['min_samples_split']}\", axis=1\n",
    ")\n",
    "\n",
    "# Now we pivot so that each (Combo, Metric) is mapped to a Score\n",
    "# (This step can be done in different ways, but pivot_table is convenient)\n",
    "pivot_df = df_melted.pivot_table(\n",
    "    index=\"Combo\", \n",
    "    columns=\"Metric\", \n",
    "    values=\"Score\"\n",
    ")\n",
    "\n",
    "# Sort combos by n_estimators then min_samples_split in the row index\n",
    "# (They should already be in that order from the prior sorting.)\n",
    "pivot_df = pivot_df.loc[pivot_df.index]\n",
    "\n",
    "# Plot: Grouped bar chart\n",
    "plt.figure(figsize=(12, 6))  # Make the figure a bit larger\n",
    "x = range(len(pivot_df.index))  # each index is one group\n",
    "bar_width = 0.15  # or 0.1, depending on how many metrics you have\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # shift each bar group by i*bar_width\n",
    "    plt.bar(\n",
    "        [pos + i*bar_width for pos in x],\n",
    "        pivot_df[metric],\n",
    "        width=bar_width,\n",
    "        label=metric\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"n_estimators, min_samples_split\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Random Forest Performance by Hyperparameters\")\n",
    "\n",
    "# Ticks in the middle of the group\n",
    "plt.xticks(\n",
    "    [pos + bar_width*(len(metrics)-1)/2 for pos in x],\n",
    "    pivot_df.index, \n",
    "    rotation=45, \n",
    "    ha='right'\n",
    ")\n",
    "\n",
    "plt.ylim(0, 1)  # all these metrics range from 0 to 1 in classification tasks\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of the visuilzation code\n",
    "\n",
    "for x, y, val in zip(\n",
    "    df_results[\"min_samples_split\"], \n",
    "    df_results[\"n_estimators\"], \n",
    "    df_results[metric]\n",
    "):\n",
    "    ax.text(x, y, f\"{val:.3f}\", ha=\"center\", va=\"center\", fontsize=8, color=\"white\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data: 80% for training, 20% for testing\n",
    "# Setting shuffle=False preserves the sequential order (important for time series)\n",
    "train, test = train_test_split(dfcopy, test_size=0.2, shuffle=False)\n",
    "\n",
    "# List of predictor columns to be used for training\n",
    "predictors = [\"close\", \"Volume BTC\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "# Set learning parameters:\n",
    "# - n_estimators: number of decision trees (experiment with this for optimal performance)\n",
    "# - min_samples_split: minimum samples required to split an internal node (helps protect against overfitting)\n",
    "# - random_state: ensures reproducible results\n",
    "\n",
    "#from the check we did we got that the best paramters where n_esimator = 100 and min split = 5\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, min_samples_split=5, random_state=1)\n",
    "\n",
    "# Train the model using the predictor columns to predict the target\n",
    "model.fit(train[predictors], train[\"Target\"])\n",
    "\n",
    "# Optionally, you can print the shapes to verify the split\n",
    "print(\"Training set shape:\", train.shape)\n",
    "print(\"Test set shape:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will check our model , (when we said the market would go up did it acctualy go up)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "test = dfcopy.iloc[:-100] # The last 100 rows will be put in the test set\n",
    "predictors = [\"close\",\"Volume BTC\",\"open\",\"high\",\"low\"]\n",
    "#this will generate predictions using our model\n",
    "preds = model.predict(test[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will convert our predictions into a series fromat (easier to read)\n",
    "import pandas as pd \n",
    "preds = pd.Series(preds,index=test.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the prediction array\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7617517256987666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculation of the precission score\n",
    "precision_score(test[\"Target\"],preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will save the model so u can keep working on it later\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'models/random_forest_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model back up \n",
    "import joblib\n",
    "\n",
    "# Load the saved model from file\n",
    "model = joblib.load('models/random_forest_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function wraps up everything we did before into a function\n",
    "import pandas as pd\n",
    "def predict(train, test, predictors, model):\n",
    "    model.fit(train[predictors], train[\"Target\"]) #fitting of the model using the trainig and target\n",
    "    preds = model.predict(test[predictors]) #genrating predictions\n",
    "    preds = pd.Series(preds, index=test.index, name=\"Predictions\") #combining hte model into a series\n",
    "    combined = pd.concat([test[\"Target\"], preds], axis=1) # combinig everything togehter to a data frame\n",
    "    return combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a backtesting function , which takes the data , ml model , preddictors , start and step value\n",
    "#start = the amount of that data that will train the model\n",
    "# start=48000 - > learn on the first 48000 rows (first 8 years of data)\n",
    "#step = the amount of data that will be learned in each step (we will go from a year to another year)\n",
    "def backtest(data, model, predictors, start=48000, step=6000):\n",
    "    all_predictions = [] # list of dataframes of predictions for a singel year\n",
    "\n",
    "    for i in range(start, data.shape[0], step): # loop across the data year by year\n",
    "        train = data.iloc[:i].copy() #(writing .copy to avoid the copy warning)\n",
    "        test = data.iloc[i:i+step].copy() \n",
    "        predictions = predict(train, test, predictors, model) # predict function to genreate predictions\n",
    "        all_predictions.append(predictions) #append the predictions for a giving year\n",
    "\n",
    "    return pd.concat(all_predictions) #concat all the dataframes togheter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtest the data with the model and predictors we calcaulted earlier\n",
    "predictions = backtest(dfcopy, model, predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictions\n",
       "0    18970\n",
       "1    13213\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many days we predicted the market would go up vs down \n",
    "#value_count will count how many times each type of prediction was made\n",
    "predictions[\"Predictions\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5085900249754031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the precision score\n",
    "precision_score(predictions[\"Target\"],predictions[\"Predictions\"])\n",
    "\n",
    "#0.5085900249754031\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "1    0.505484\n",
       "0    0.494516\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#benchmark to check the prectange of days where the market acctualy went up\n",
    "predictions[\"Target\"].value_counts() / predictions.shape[0]\n",
    " \n",
    "#1    0.505484\n",
    "#0    0.494516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variety of rolling avreges , to help identify \n",
    "#trends of wheter the stock will go up or down\n",
    "\n",
    "# trends for the last 2 days , 5 days , 60 days , 250 days , 1000 days\n",
    "horizons = [24*2,24*5,24*60,24*250,24*1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons: # we will loop through the horizions and calcaulte a rolling avrege\n",
    "    rolling_averages = dfcopy.rolling(horizon).mean()\n",
    "\n",
    "    #Column for that will contatin the close value of each horizion\n",
    "    ratio_column = f\"close_ratio_{horizon}\"\n",
    "\n",
    "    #calculating the ratio bettwen our current close value and each of the horzions\n",
    "    #(and adding it to the data frame)\n",
    "    dfcopy[ratio_column] = dfcopy[\"close\"] / rolling_averages[\"close\"]\n",
    "\n",
    "    #the number of hours in the current horrizion that the stock price went up\n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    dfcopy[trend_column] = dfcopy.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "\n",
    "    new_predictors += [ratio_column,trend_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy\n",
    "# why we have Nans?\n",
    "#when panads cant find enough rows prior to our current row in order to compute the rolling avrege it gives us nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows where we have na in the columns\n",
    "dfcopy = dfcopy.dropna()\n",
    "\n",
    "#we can see now that we dropped a few years since we needed them to calcualte hte trend 1000 and close 1000 days of trading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200,min_samples_split=50,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing a little bit our predict fucntion that now will return the probability that the stoock price would go up or down\n",
    "\n",
    "import pandas as pd\n",
    "def predict(train, test, predictors, model):\n",
    "    model.fit(train[predictors], train[\"Target\"]) \n",
    "    preds = model.predict_proba(test[predictors])[:,1] #we get the second column of this (the prob that the stock would go up) \n",
    "    #now we would set a trehshold that only if the probability is 60% to go up then the model would predict the increase\n",
    "    preds[preds>=.6] = 1 # it will reduce the number of days we can predict the price to go up , but we would increase the the prediction accuracy\n",
    "    preds[preds<.6] = 0\n",
    "    preds = pd.Series(preds, index=test.index, name=\"Predictions\") \n",
    "    combined = pd.concat([test[\"Target\"], preds], axis=1) \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we let the predictions to realy on the ratio and the trends , since the value or the prices themself dont really give us too much inforamtion\n",
    "#compared to the ratio or the value compared to the days before \n",
    "predictions = backtest(dfcopy,model,new_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictions\n",
       "1    4219\n",
       "0    3963\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"Predictions\"].value_counts()\n",
    "\n",
    "#the print\n",
    "#Predictions\n",
    "#1    4219\n",
    "#0    3963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5117326380658924"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(predictions[\"Target\"],predictions[\"Predictions\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
