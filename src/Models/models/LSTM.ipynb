{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5e0dd8",
   "metadata": {},
   "source": [
    "# In this notebook we will train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ab63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_seq_4h = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'bollinger_upper', 'bollinger_lower', 'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "    'EMA_21', 'SMA_20',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'rsi_oversold', 'rsi_overbought', 'stoch_overbought',\n",
    "    'stoch_oversold', 'cci_overbought', 'cci_oversold', 'near_upper_band',\n",
    "    'near_lower_band', 'overbought_reversal', 'oversold_reversal',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'macd_positive', 'momentum_alignment', 'macd_rising', 'obv_rising_24h',\n",
    "    'trending_market', 'trend_alignment',\n",
    "    'support_level', 'resistance_level', 'volatility_regime',\n",
    "    'close_daily', 'rsi_daily'\n",
    "]\n",
    "\n",
    "✅ DROP_COLS['4H']['LSTM']\n",
    "✅ DROP_COLS['4H']['GRU']\n",
    "✅ DROP_COLS['4H']['CNN']\n",
    "✅ DROP_COLS['4H']['CNN_LSTM']\n",
    "✅ DROP_COLS['4H']['TCN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e941d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop columns catnboost\n",
    "drop_catboost_4h = [\n",
    "    'open', 'high', 'low',\n",
    "    'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'EMA_21', 'SMA_20', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3', \n",
    "    'bearish_scenario_6',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'rsi_oversold', 'rsi_overbought', 'stoch_overbought', 'stoch_oversold',\n",
    "    'cci_overbought', 'cci_oversold', 'overbought_reversal', 'oversold_reversal',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'trending_market', 'trend_alignment', 'momentum_alignment',\n",
    "    'ema7_above_ema21', 'obv_rising_24h', 'above_sma20', 'above_sma50',\n",
    "    'macd_positive', 'macd_rising'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns of xgboost + LightGBM are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b28a701",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Deep Learning\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout, BatchNormalization\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  IMPROVED LSTM HYPERPARAMETER TUNER (precision-weighted Fβ=0.5)\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Sklearn for metrics and preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "print(\"🔧 GPU Setup:\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ Found {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ GPU setup error: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU found, using CPU\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1) CONFIG  – EDIT HERE\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "START_DATE = \"2016-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC = 0.15\n",
    "\n",
    "# Sequence parameters\n",
    "SEQUENCE_LENGTH = 24  # 24 * 4h = 4 days of history\n",
    "\n",
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "            'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "            'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "            'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "            'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "            'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment', 'obv_rising_24h',\n",
    "            'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "# Bounded columns that should use MinMax scaling (0-1 range)\n",
    "BOUNDED_COLS = ['rsi', 'stoch_%K', 'bb_position', 'williams_%R']  # Add your bounded features\n",
    "\n",
    "# IMPROVED Hyperparameter search space\n",
    "PARAM_GRID = {\n",
    "    'lstm_units': [32, 64, 128],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'scaler_type': ['standard', 'robust', 'minmax', 'mixed'],  # Added minmax and mixed\n",
    "    'l1_reg': [0.0, 0.001, 0.01],\n",
    "    'l2_reg': [0.0, 0.001, 0.01],\n",
    "    'threshold': [0.3, 0.4, 0.5, 0.6, 0.7]  # Added threshold as hyperparameter\n",
    "}\n",
    "\n",
    "# Search strategy\n",
    "MAX_TRIALS = 50\n",
    "EARLY_STOPPING_PATIENCE = 8  # Reduced for faster trials\n",
    "REDUCE_LR_PATIENCE = 4\n",
    "MAX_EPOCHS = 80  # Reduced for faster search\n",
    "\n",
    "# Results save path\n",
    "RESULTS_PATH = Path(\"results/lstm_hyperparameter_results.json\")\n",
    "RESULTS_PATH.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2) IMPROVED DATA PREPROCESSING\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the dataset\"\"\"\n",
    "    if not CSV_FILE.exists():\n",
    "        raise FileNotFoundError(f\"❌ File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(\"📊 Loading data...\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL])\n",
    "    df = df.set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        raise KeyError(f\"❌ '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    features_to_drop = [c for c in DROP_COLS if c in df.columns] + [TARGET_COL]\n",
    "    X = df.drop(columns=features_to_drop, errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Features: {len(X.columns)}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    if X.isnull().sum().sum() > 0:\n",
    "        print(\"⚠️ Warning: Missing values detected - forward filling\")\n",
    "        X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(X)):\n",
    "        X_seq.append(X.iloc[i-sequence_length:i].values)\n",
    "        y_seq.append(y.iloc[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def get_mixed_scaler(X_train, bounded_cols):\n",
    "    \"\"\"Create mixed scaler: MinMax for bounded, Standard for others\"\"\"\n",
    "    class MixedScaler:\n",
    "        def __init__(self, bounded_cols):\n",
    "            self.bounded_cols = bounded_cols\n",
    "            self.standard_scaler = StandardScaler()\n",
    "            self.minmax_scaler = MinMaxScaler()\n",
    "            self.feature_names = None\n",
    "            \n",
    "        def fit(self, X):\n",
    "            self.feature_names = X.columns if hasattr(X, 'columns') else range(X.shape[1])\n",
    "            bounded_mask = [col in self.bounded_cols for col in self.feature_names]\n",
    "            \n",
    "            if np.any(bounded_mask):\n",
    "                self.minmax_scaler.fit(X.iloc[:, bounded_mask] if hasattr(X, 'iloc') else X[:, bounded_mask])\n",
    "            if np.any(~np.array(bounded_mask)):\n",
    "                self.standard_scaler.fit(X.iloc[:, ~np.array(bounded_mask)] if hasattr(X, 'iloc') else X[:, ~np.array(bounded_mask)])\n",
    "            return self\n",
    "            \n",
    "        def transform(self, X):\n",
    "            result = np.zeros_like(X)\n",
    "            bounded_mask = [col in self.bounded_cols for col in self.feature_names]\n",
    "            \n",
    "            if np.any(bounded_mask):\n",
    "                bounded_indices = np.where(bounded_mask)[0]\n",
    "                result[:, bounded_indices] = self.minmax_scaler.transform(\n",
    "                    X.iloc[:, bounded_mask] if hasattr(X, 'iloc') else X[:, bounded_mask]\n",
    "                )\n",
    "            if np.any(~np.array(bounded_mask)):\n",
    "                standard_indices = np.where(~np.array(bounded_mask))[0]\n",
    "                result[:, standard_indices] = self.standard_scaler.transform(\n",
    "                    X.iloc[:, ~np.array(bounded_mask)] if hasattr(X, 'iloc') else X[:, ~np.array(bounded_mask)]\n",
    "                )\n",
    "            return result\n",
    "    \n",
    "    return MixedScaler(bounded_cols)\n",
    "\n",
    "def prepare_lstm_data(X, y, sequence_length, test_frac, val_frac, scaler_type='standard'):\n",
    "    \"\"\"Prepare data for LSTM training with improved scaling\"\"\"\n",
    "    \n",
    "    # Time-based splits\n",
    "    test_split = int(len(X) * (1 - test_frac))\n",
    "    train_val_split = int(test_split * (1 - val_frac))\n",
    "    \n",
    "    X_train_val = X.iloc[:test_split]\n",
    "    X_test = X.iloc[test_split:]\n",
    "    y_train_val = y.iloc[:test_split]\n",
    "    y_test = y.iloc[test_split:]\n",
    "    \n",
    "    X_train = X_train_val.iloc[:train_val_split]\n",
    "    X_val = X_train_val.iloc[train_val_split:]\n",
    "    y_train = y_train_val.iloc[:train_val_split]\n",
    "    y_val = y_train_val.iloc[train_val_split:]\n",
    "    \n",
    "    # Choose scaler - TRAIN ONLY FIT\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:  # mixed\n",
    "        scaler = get_mixed_scaler(X_train, BOUNDED_COLS)\n",
    "    \n",
    "    # Fit ONLY on training data\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_val_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_val),\n",
    "        columns=X_val.columns,\n",
    "        index=X_val.index\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, sequence_length)\n",
    "    \n",
    "    return (X_train_seq, y_train_seq), (X_val_seq, y_val_seq), (X_test_seq, y_test_seq), scaler\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3) IMPROVED MODEL AND CALLBACKS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def f_beta_score(y_true, y_pred, beta=0.5, threshold=0.5):\n",
    "    \"\"\"Calculate F-beta score with custom threshold\"\"\"\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    p = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "class FBetaEarlyStopping(Callback):\n",
    "    \"\"\"Custom callback to stop training based on F-beta score\"\"\"\n",
    "    def __init__(self, validation_data, threshold=0.5, patience=10, min_delta=0.001):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.best_score = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = self.model.predict(X_val, verbose=0)\n",
    "        current_score = f_beta_score(y_val, y_pred.flatten(), threshold=self.threshold)\n",
    "        \n",
    "        if current_score > self.best_score + self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        if self.wait >= self.patience:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def build_lstm_model(input_shape, params):\n",
    "    \"\"\"Build LSTM model with IMPROVED architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer - REMOVED recurrent_dropout for GPU efficiency\n",
    "    if params['num_layers'] == 1:\n",
    "        model.add(LSTM(\n",
    "            params['lstm_units'],\n",
    "            input_shape=input_shape,\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "        ))\n",
    "    else:\n",
    "        model.add(LSTM(\n",
    "            params['lstm_units'],\n",
    "            input_shape=input_shape,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "        ))\n",
    "        \n",
    "        # Additional LSTM layers\n",
    "        for i in range(1, params['num_layers']):\n",
    "            return_seq = (i < params['num_layers'] - 1)\n",
    "            model.add(LSTM(\n",
    "                params['lstm_units'],\n",
    "                return_sequences=return_seq,\n",
    "                kernel_regularizer=l1_l2(l1=params['l1_reg'], l2=params['l2_reg'])\n",
    "            ))\n",
    "    \n",
    "    # Dropout layer AFTER LSTM\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    else:  # rmsprop\n",
    "        optimizer = RMSprop(learning_rate=params['learning_rate'])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"Evaluate model with custom threshold\"\"\"\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > threshold).astype(int).flatten()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f_beta_0_5': f_beta_score(y_test, y_pred_prob.flatten(), threshold=threshold),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_prob),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4) IMPROVED SEARCH STRATEGY\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def efficient_param_generator(param_grid, max_trials):\n",
    "    \"\"\"Memory-efficient parameter generation using itertools\"\"\"\n",
    "    # Get all parameter names and values\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = [param_grid[name] for name in param_names]\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total_combinations = np.prod([len(values) for values in param_values])\n",
    "    print(f\"Total possible combinations: {total_combinations:,}\")\n",
    "    \n",
    "    if total_combinations <= max_trials:\n",
    "        # Generate all combinations if small enough\n",
    "        for combination in itertools.product(*param_values):\n",
    "            yield dict(zip(param_names, combination))\n",
    "    else:\n",
    "        # Randomly sample combinations\n",
    "        sampled_indices = np.random.choice(total_combinations, max_trials, replace=False)\n",
    "        sampled_indices.sort()  # Sort for better memory access patterns\n",
    "        \n",
    "        current_index = 0\n",
    "        for i, combination in enumerate(itertools.product(*param_values)):\n",
    "            if current_index < len(sampled_indices) and i == sampled_indices[current_index]:\n",
    "                yield dict(zip(param_names, combination))\n",
    "                current_index += 1\n",
    "                if current_index >= len(sampled_indices):\n",
    "                    break\n",
    "\n",
    "def run_hyperparameter_search():\n",
    "    \"\"\"IMPROVED hyperparameter search function\"\"\"\n",
    "    print(\"🚀 Starting IMPROVED LSTM Hyperparameter Search\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X, y = load_and_prepare_data()\n",
    "    \n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    trial = 0\n",
    "    for params in efficient_param_generator(PARAM_GRID, MAX_TRIALS):\n",
    "        trial += 1\n",
    "        print(f\"\\n📊 Trial {trial}/{MAX_TRIALS}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data with current scaler\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler = prepare_lstm_data(\n",
    "                X, y, SEQUENCE_LENGTH, TEST_FRAC, VAL_FRAC, params['scaler_type']\n",
    "            )\n",
    "            \n",
    "            # Build model\n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            model = build_lstm_model(input_shape, params)\n",
    "            \n",
    "            # IMPROVED Callbacks - stop on F-beta instead of loss\n",
    "            callbacks = [\n",
    "                FBetaEarlyStopping(\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    threshold=params['threshold'],\n",
    "                    patience=EARLY_STOPPING_PATIENCE\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor='val_accuracy',  # Changed from val_loss\n",
    "                    factor=0.5,\n",
    "                    patience=REDUCE_LR_PATIENCE,\n",
    "                    min_lr=1e-7,\n",
    "                    verbose=0\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=MAX_EPOCHS,\n",
    "                batch_size=params['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate model with custom threshold\n",
    "            val_metrics = evaluate_model(model, X_val, y_val, params['threshold'])\n",
    "            test_metrics = evaluate_model(model, X_test, y_test, params['threshold'])\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'trial': trial,\n",
    "                'params': params,\n",
    "                'val_metrics': val_metrics,\n",
    "                'test_metrics': test_metrics,\n",
    "                'training_time': training_time,\n",
    "                'epochs_trained': len(history.history['loss']),\n",
    "                'final_train_loss': float(history.history['loss'][-1]),\n",
    "                'final_val_loss': float(history.history['val_loss'][-1])\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Check if this is the best model\n",
    "            current_score = val_metrics['f_beta_0_5']\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_params = params\n",
    "                print(f\"🌟 New best F-beta score: {current_score:.4f}\")\n",
    "            \n",
    "            print(f\"Val F-beta: {val_metrics['f_beta_0_5']:.4f}, \"\n",
    "                  f\"Test F-beta: {test_metrics['f_beta_0_5']:.4f}, \"\n",
    "                  f\"Time: {training_time:.1f}s\")\n",
    "            \n",
    "            # IMPROVED Memory cleanup\n",
    "            del model, history\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Trial failed: {str(e)}\")\n",
    "            # Clean up even on failure\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    return results, best_params, best_score\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5) MAIN EXECUTION\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"🔍 IMPROVED LSTM Hyperparameter Tuning\")\n",
    "    print(\"=\"*60)\n",
    "    total_combinations = np.prod([len(v) for v in PARAM_GRID.values()])\n",
    "    print(f\"Search space size: {total_combinations:,}\")\n",
    "    print(f\"Max trials: {MAX_TRIALS}\")\n",
    "    print(f\"Sequence length: {SEQUENCE_LENGTH}\")\n",
    "    print(f\"Target metric: F-beta (β=0.5) with threshold optimization\")\n",
    "    print(f\"Improvements: Mixed scaling, F-beta early stopping, threshold tuning\")\n",
    "    \n",
    "    # Run hyperparameter search\n",
    "    results, best_params, best_score = run_hyperparameter_search()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏆 IMPROVED HYPERPARAMETER SEARCH RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Completed trials: {len(results)}\")\n",
    "        print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"Average time per trial: {total_time/len(results):.1f} seconds\")\n",
    "        \n",
    "        print(f\"\\n🌟 BEST PARAMETERS (F-beta = {best_score:.4f}):\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"   {param:<15}: {value}\")\n",
    "        \n",
    "        # Sort results by validation F-beta score\n",
    "        results_sorted = sorted(results, key=lambda x: x['val_metrics']['f_beta_0_5'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n📊 TOP 5 MODELS:\")\n",
    "        print(\"Rank | Val F-beta | Test F-beta | Threshold | Time(s) | Scaler | Params\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results_sorted[:5]):\n",
    "            val_score = result['val_metrics']['f_beta_0_5']\n",
    "            test_score = result['test_metrics']['f_beta_0_5']\n",
    "            threshold = result['params']['threshold']\n",
    "            time_taken = result['training_time']\n",
    "            scaler = result['params']['scaler_type']\n",
    "            params_str = f\"units={result['params']['lstm_units']}, layers={result['params']['num_layers']}\"\n",
    "            print(f\"{i+1:>4} | {val_score:>10.4f} | {test_score:>11.4f} | {threshold:>9.1f} | {time_taken:>7.1f} | {scaler:>6} | {params_str}\")\n",
    "        \n",
    "        # Save results\n",
    "        save_data = {\n",
    "            'search_config': {\n",
    "                'param_grid': PARAM_GRID,\n",
    "                'max_trials': MAX_TRIALS,\n",
    "                'sequence_length': SEQUENCE_LENGTH,\n",
    "                'test_frac': TEST_FRAC,\n",
    "                'val_frac': VAL_FRAC,\n",
    "                'bounded_cols': BOUNDED_COLS,\n",
    "                'improvements': [\n",
    "                    'Mixed/MinMax scaling for bounded features',\n",
    "                    'F-beta early stopping instead of loss',\n",
    "                    'Threshold as hyperparameter',\n",
    "                    'Improved memory management',\n",
    "                    'Removed recurrent_dropout for GPU efficiency'\n",
    "                ]\n",
    "            },\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'all_results': results,\n",
    "            'search_time': total_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(RESULTS_PATH, 'w') as f:\n",
    "            json.dump(save_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved to: {RESULTS_PATH}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No successful trials completed!\")\n",
    "    \n",
    "    print(\"\\n✅ IMPROVED Hyperparameter search completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
