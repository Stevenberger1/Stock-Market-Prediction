{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a75b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep \n",
    "atr_14  , OBV\n",
    "\n",
    "drop\n",
    "\n",
    "'ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal', 'trending_market'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7131221",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lstm_cnn = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_21', 'SMA_20',\n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower',\n",
    "    'resistance_level', 'support_level', 'high_low', 'high_close', 'low_close',\n",
    "    'true_range', 'volume_mean_20', 'MACD_line', 'MACD_signal',\n",
    "    'volatility_regime', 'trending_market', 'above_sma50', 'ema7_above_ema21',\n",
    "    'rsi_overbought', 'stoch_oversold', 'cci_oversold',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'break_upper_band', 'break_lower_band', 'rsi_oversold',\n",
    "    'above_sma20', 'macd_positive', 'volume_breakout', 'volume_breakdown',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6','ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e801781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:26:29,625] A new study created in memory with name: no-name-7c072783-b210-43ed-a4cc-e469a26c0a4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€  Starting 100-trial search (big batches, cuDNN, XLA)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T00 | FÎ±2=0.595 P=0.521 R=0.640 lstm_conv win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:28:59,301] Trial 0 finished with value: -0.5948424068767908 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 128, 'kernel': 5, 'units': 96, 'conv_drop': 0.05454749016213018, 'dense': 128, 'dropout': 0.17277800745684632, 'l2': 7.4763120622522945e-06, 'lr': 0.000612261560602803, 'batch': 256, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   1%|          | 1/100 [02:31<4:09:18, 151.10s/it, 151.10/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T01 | FÎ±2=0.391 P=0.522 R=0.348 lstm_conv win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   1%|          | 1/100 [06:24<4:09:18, 151.10s/it, 151.10/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:32:53,658] Trial 1 finished with value: -0.3912740775121838 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 32, 'kernel': 2, 'units': 96, 'conv_drop': 0.09351332282682329, 'dense': 64, 'dropout': 0.38783385110582347, 'l2': 0.00021154290797261214, 'lr': 0.0023417537373087377, 'batch': 256, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   2%|â–         | 2/100 [06:25<5:26:48, 200.09s/it, 385.48/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T02 | FÎ±2=0.547 P=0.518 R=0.563 lstm_conv win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   2%|â–         | 2/100 [06:50<5:26:48, 200.09s/it, 385.48/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:33:20,153] Trial 2 finished with value: -0.5472773736976606 and parameters: {'arch': 'lstm_conv', 'window': 18, 'filters': 64, 'kernel': 2, 'units': 32, 'conv_drop': 0.10753971856328177, 'dense': 64, 'dropout': 0.13235920994105968, 'l2': 1.5512259126484753e-06, 'lr': 0.00017862556214123332, 'batch': 128, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   3%|â–Ž         | 3/100 [06:51<3:15:20, 120.83s/it, 411.99/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T03 | FÎ±2=0.527 P=0.533 R=0.524 conv_lstm win=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   3%|â–Ž         | 3/100 [07:34<3:15:20, 120.83s/it, 411.99/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:34:03,842] Trial 3 finished with value: -0.5269035532994923 and parameters: {'arch': 'conv_lstm', 'window': 12, 'filters': 64, 'kernel': 7, 'units': 96, 'conv_drop': 0.0869254358741304, 'dense': 64, 'dropout': 0.2533615026041694, 'l2': 0.00041151130495610907, 'lr': 0.001342828384653336, 'batch': 128, 'act': 'elu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 0 with value: -0.5948424068767908.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   4%|â–         | 4/100 [07:35<2:24:36, 90.38s/it, 455.69/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T04 | FÎ±2=0.677 P=0.518 R=0.800 lstm_conv win=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.594842:   4%|â–         | 4/100 [09:37<2:24:36, 90.38s/it, 455.69/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:36:07,021] Trial 4 finished with value: -0.677185721676151 and parameters: {'arch': 'lstm_conv', 'window': 36, 'filters': 96, 'kernel': 5, 'units': 96, 'conv_drop': 0.14917455176771563, 'dense': 32, 'dropout': 0.24382573359195875, 'l2': 3.2213437409123405e-05, 'lr': 6.173152563090738e-05, 'batch': 128, 'act': 'elu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 4 with value: -0.677185721676151.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.677186:   5%|â–Œ         | 5/100 [11:40<2:41:49, 102.21s/it, 578.87/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:38:10,590] Trial 5 pruned. Trial was pruned at epoch 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.677186:   6%|â–Œ         | 6/100 [11:55<2:51:31, 109.48s/it, 702.47/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:38:25,192] Trial 6 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.677186:   7%|â–‹         | 7/100 [11:57<2:01:36, 78.46s/it, 717.05/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T07 | FÎ±2=0.691 P=0.519 R=0.828 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.677186:   7%|â–‹         | 7/100 [14:14<2:01:36, 78.46s/it, 717.05/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:40:44,050] Trial 7 finished with value: -0.6909967292132897 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 5, 'units': 96, 'conv_drop': 0.19364168877215035, 'dense': 128, 'dropout': 0.3659457560881794, 'l2': 1.2896625857891445e-05, 'lr': 5.326650551390532e-05, 'batch': 256, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:   8%|â–Š         | 8/100 [14:39<2:29:51, 97.73s/it, 856.05/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:41:09,220] Trial 8 pruned. Trial was pruned at epoch 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:   9%|â–‰         | 9/100 [14:52<1:53:49, 75.05s/it, 881.23/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:41:22,606] Trial 9 pruned. Trial was pruned at epoch 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  10%|â–ˆ         | 10/100 [14:54<1:24:01, 56.01s/it, 894.61/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T10 | FÎ±2=0.685 P=0.532 R=0.801 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  10%|â–ˆ         | 10/100 [16:22<1:24:01, 56.01s/it, 894.61/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:42:51,866] Trial 10 finished with value: -0.6853813559322035 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 32, 'conv_drop': 0.14914175826424267, 'dense': 128, 'dropout': 0.36311711503988925, 'l2': 2.4631096227367697e-05, 'lr': 9.919404853103473e-05, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  11%|â–ˆ         | 11/100 [16:35<1:38:11, 66.19s/it, 983.89/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:43:04,653] Trial 11 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  12%|â–ˆâ–        | 12/100 [16:36<1:13:17, 49.97s/it, 996.75/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T12 | FÎ±2=0.588 P=0.527 R=0.625 conv_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  12%|â–ˆâ–        | 12/100 [18:20<1:13:17, 49.97s/it, 996.75/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:44:50,201] Trial 12 finished with value: -0.5884861407249468 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 6, 'units': 96, 'conv_drop': 0.22606400599215065, 'dense': 128, 'dropout': 0.22176648138502159, 'l2': 4.5679942859048045e-06, 'lr': 5.5459545060680684e-05, 'batch': 128, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  13%|â–ˆâ–Ž        | 13/100 [18:31<1:36:52, 66.82s/it, 1102.33/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:45:01,423] Trial 13 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  14%|â–ˆâ–        | 14/100 [18:33<1:11:41, 50.01s/it, 1113.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T14 | FÎ±2=0.659 P=0.530 R=0.750 conv_lstm win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  14%|â–ˆâ–        | 14/100 [20:00<1:11:41, 50.01s/it, 1113.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:46:29,946] Trial 14 finished with value: -0.6591442498645965 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 48, 'kernel': 7, 'units': 96, 'conv_drop': 0.16854365802480042, 'dense': 128, 'dropout': 0.34212635055038104, 'l2': 5.34107900020877e-06, 'lr': 8.139951880089569e-05, 'batch': 256, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 7 with value: -0.6909967292132897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  15%|â–ˆâ–Œ        | 15/100 [20:10<1:27:16, 61.60s/it, 1201.98/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:46:40,476] Trial 15 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  16%|â–ˆâ–Œ        | 16/100 [20:26<1:04:43, 46.24s/it, 1212.53/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:46:56,098] Trial 16 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  17%|â–ˆâ–‹        | 17/100 [20:28<51:14, 37.04s/it, 1228.20/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T17 | FÎ±2=0.709 P=0.519 R=0.868 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.690997:  17%|â–ˆâ–‹        | 17/100 [21:57<51:14, 37.04s/it, 1228.20/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:48:26,804] Trial 17 finished with value: -0.7087967644084935 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 2, 'units': 32, 'conv_drop': 0.2020859487428539, 'dense': 128, 'dropout': 0.3311343605595314, 'l2': 5.446410711918106e-06, 'lr': 0.0002933566355803551, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 17 with value: -0.7087967644084935.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -0.708797:  18%|â–ˆâ–Š        | 18/100 [21:58<1:12:35, 53.11s/it, 1318.71/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T18 | FÎ±2=0.729 P=0.529 R=0.898 conv_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -0.708797:  18%|â–ˆâ–Š        | 18/100 [26:04<1:12:35, 53.11s/it, 1318.71/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:52:34,085] Trial 18 finished with value: -0.7285785869383664 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.1829073485357999, 'dense': 64, 'dropout': 0.3367267654373487, 'l2': 1.9949353471421903e-06, 'lr': 0.0010969018481585208, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 18 with value: -0.7285785869383664.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: -0.728579:  19%|â–ˆâ–‰        | 19/100 [26:19<2:30:26, 111.43s/it, 1566.01/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:52:48,629] Trial 19 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: -0.728579:  20%|â–ˆâ–ˆ        | 20/100 [26:20<1:49:48, 82.35s/it, 1580.59/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T20 | FÎ±2=0.732 P=0.524 R=0.914 conv_lstm win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: -0.728579:  20%|â–ˆâ–ˆ        | 20/100 [30:17<1:49:48, 82.35s/it, 1580.59/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:56:46,797] Trial 20 finished with value: -0.7319723411261113 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 96, 'kernel': 2, 'units': 128, 'conv_drop': 0.23169116379282378, 'dense': 128, 'dropout': 0.22993558336668984, 'l2': 1.1980316309987873e-06, 'lr': 0.0010514052377099624, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  21%|â–ˆâ–ˆ        | 21/100 [30:18<2:50:00, 129.12s/it, 1818.73/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T21 | FÎ±2=0.597 P=0.533 R=0.636 conv_lstm win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  21%|â–ˆâ–ˆ        | 21/100 [33:25<2:50:00, 129.12s/it, 1818.73/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 14:59:54,707] Trial 21 finished with value: -0.5973476840284452 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 96, 'kernel': 3, 'units': 128, 'conv_drop': 0.13417079120549671, 'dense': 128, 'dropout': 0.20486313559166633, 'l2': 1.3625466836519646e-06, 'lr': 0.0009418760303623328, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  22%|â–ˆâ–ˆâ–       | 22/100 [33:26<3:10:46, 146.75s/it, 2006.62/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T22 | FÎ±2=0.672 P=0.528 R=0.778 conv_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  22%|â–ˆâ–ˆâ–       | 22/100 [37:25<3:10:46, 146.75s/it, 2006.62/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:03:55,538] Trial 22 finished with value: -0.6717611515905456 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 64, 'kernel': 2, 'units': 128, 'conv_drop': 0.20725498441456727, 'dense': 64, 'dropout': 0.1514327494157322, 'l2': 3.4372389137912706e-06, 'lr': 0.0011623426402264853, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [37:58<3:44:35, 175.01s/it, 2247.53/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:04:28,071] Trial 23 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  24%|â–ˆâ–ˆâ–       | 24/100 [38:00<2:47:31, 132.26s/it, 2280.06/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T24 | FÎ±2=0.717 P=0.523 R=0.880 lstm_conv win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  24%|â–ˆâ–ˆâ–       | 24/100 [40:47<2:47:31, 132.26s/it, 2280.06/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:07:16,928] Trial 24 finished with value: -0.7172275238573581 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 48, 'kernel': 3, 'units': 128, 'conv_drop': 0.22458452100672904, 'dense': 64, 'dropout': 0.27475311523806734, 'l2': 1.162732997626243e-06, 'lr': 0.0012399873702159075, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [41:06<2:59:02, 143.23s/it, 2448.89/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:07:35,721] Trial 25 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [41:07<2:10:37, 105.91s/it, 2467.72/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T26 | FÎ±2=0.708 P=0.524 R=0.859 lstm_conv win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [43:12<2:10:37, 105.91s/it, 2467.72/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:09:42,598] Trial 26 finished with value: -0.7082767978290366 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 48, 'kernel': 3, 'units': 64, 'conv_drop': 0.18808609956374508, 'dense': 128, 'dropout': 0.3124217042135821, 'l2': 1.701627483058166e-06, 'lr': 0.0020810000414228608, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  27%|â–ˆâ–ˆâ–‹       | 27/100 [43:28<2:16:33, 112.24s/it, 2594.73/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:09:58,292] Trial 27 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  28%|â–ˆâ–ˆâ–Š       | 28/100 [43:30<1:39:55, 83.27s/it, 2610.43/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T28 | FÎ±2=0.703 P=0.526 R=0.845 lstm_conv win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  28%|â–ˆâ–ˆâ–Š       | 28/100 [47:08<1:39:55, 83.27s/it, 2610.43/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:13:38,257] Trial 28 finished with value: -0.703036541430777 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 128, 'kernel': 5, 'units': 128, 'conv_drop': 0.17053482179312465, 'dense': 64, 'dropout': 0.33831466588119824, 'l2': 1.6450611097511145e-06, 'lr': 0.0010824559227546788, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  29%|â–ˆâ–ˆâ–‰       | 29/100 [47:10<2:27:05, 124.30s/it, 2830.44/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T29 | FÎ±2=0.692 P=0.521 R=0.829 conv_lstm win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  29%|â–ˆâ–ˆâ–‰       | 29/100 [49:44<2:27:05, 124.30s/it, 2830.44/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:16:14,025] Trial 29 finished with value: -0.6923076923076923 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 32, 'kernel': 2, 'units': 128, 'conv_drop': 0.17655258017029987, 'dense': 64, 'dropout': 0.3750434027374682, 'l2': 1.5304402108416722e-06, 'lr': 0.0018149865587916117, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [49:58<2:36:01, 133.73s/it, 2986.19/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:16:28,624] Trial 30 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [50:11<1:52:41, 97.99s/it, 3000.77/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:16:40,861] Trial 31 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [50:32<1:21:55, 72.29s/it, 3013.09/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:17:02,199] Trial 32 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [50:34<1:03:38, 57.00s/it, 3034.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T33 | FÎ±2=0.628 P=0.531 R=0.691 conv_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [51:37<1:03:38, 57.00s/it, 3034.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:18:07,065] Trial 33 finished with value: -0.628154795288839 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 64, 'kernel': 4, 'units': 32, 'conv_drop': 0.25301886959374464, 'dense': 128, 'dropout': 0.3266113420716731, 'l2': 2.565111517777266e-06, 'lr': 0.0004553361197772574, 'batch': 128, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [51:58<1:05:16, 59.34s/it, 3099.23/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:18:28,032] Trial 34 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [52:00<51:50, 47.85s/it, 3120.26/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T35 | FÎ±2=0.602 P=0.539 R=0.639 conv_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [57:05<51:50, 47.85s/it, 3120.26/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:23:34,952] Trial 35 finished with value: -0.6018608257414227 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.2290901400745524, 'dense': 128, 'dropout': 0.35586673619835707, 'l2': 3.4370979194197407e-06, 'lr': 0.0002580404573248242, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [57:29<2:13:53, 125.52s/it, 3427.00/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:23:58,720] Trial 36 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [57:50<1:39:44, 95.00s/it, 3450.79/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:24:20,498] Trial 37 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [58:02<1:15:28, 73.04s/it, 3472.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:24:32,309] Trial 38 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [58:16<55:34, 54.67s/it, 3484.39/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:24:45,997] Trial 39 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [58:18<42:23, 42.39s/it, 3498.12/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T40 | FÎ±2=0.636 P=0.530 R=0.706 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [59:58<42:23, 42.39s/it, 3498.12/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:26:27,891] Trial 40 finished with value: -0.6355365763089491 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 32, 'conv_drop': 0.2991414066265154, 'dense': 128, 'dropout': 0.3298017348264688, 'l2': 7.605803562468741e-06, 'lr': 0.00012155054083183725, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 20 with value: -0.7319723411261113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [1:00:09<59:17, 60.30s/it, 3600.21/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:26:39,012] Trial 41 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [1:00:26<44:00, 45.52s/it, 3611.26/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:26:56,197] Trial 42 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [1:00:28<35:09, 37.01s/it, 3628.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T43 | FÎ±2=0.763 P=0.521 R=0.994 lstm_conv win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -0.731972:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [1:02:17<35:09, 37.01s/it, 3628.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:28:47,316] Trial 43 finished with value: -0.7631537367672618 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 32, 'kernel': 4, 'units': 64, 'conv_drop': 0.09979414734158484, 'dense': 128, 'dropout': 0.3879128314126981, 'l2': 2.073472158914153e-06, 'lr': 0.0025838646128367514, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [1:02:32<55:15, 59.21s/it, 3739.41/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:29:02,165] Trial 44 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [1:02:56<42:06, 45.94s/it, 3754.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:29:26,136] Trial 45 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [1:03:08<35:26, 39.38s/it, 3778.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:29:37,875] Trial 46 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [1:03:18<27:27, 31.08s/it, 3790.18/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:29:47,671] Trial 47 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [1:03:19<21:24, 24.70s/it, 3799.98/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T48 | FÎ±2=0.555 P=0.524 R=0.572 lstm_conv win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [1:04:38<21:24, 24.70s/it, 3799.98/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:31:08,037] Trial 48 finished with value: -0.555245313123255 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 96, 'kernel': 3, 'units': 32, 'conv_drop': 0.17899377038959471, 'dense': 128, 'dropout': 0.22152697641509517, 'l2': 1.3640561599870486e-06, 'lr': 0.00042314293757819894, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [1:04:54<35:12, 41.41s/it, 3880.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:31:24,574] Trial 49 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [1:05:27<28:14, 33.89s/it, 3896.73/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:31:57,236] Trial 50 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [1:05:43<27:24, 33.56s/it, 3929.52/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:32:13,485] Trial 51 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [1:06:03<22:41, 28.37s/it, 3945.78/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:32:33,364] Trial 52 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [1:06:05<20:13, 25.82s/it, 3965.64/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T53 | FÎ±2=0.753 P=0.524 R=0.963 conv_lstm win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [1:07:20<20:13, 25.82s/it, 3965.64/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:33:49,953] Trial 53 finished with value: -0.7526332588573252 and parameters: {'arch': 'conv_lstm', 'window': 42, 'filters': 48, 'kernel': 4, 'units': 64, 'conv_drop': 0.1852717446722201, 'dense': 128, 'dropout': 0.38016448336315367, 'l2': 1.5211958873031164e-06, 'lr': 0.001880948993648861, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [1:07:38<31:26, 41.02s/it, 4042.12/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:34:08,476] Trial 54 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [1:07:52<25:42, 34.28s/it, 4060.69/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:34:21,987] Trial 55 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [1:07:54<20:35, 28.08s/it, 4074.31/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T56 | FÎ±2=0.754 P=0.523 R=0.968 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [1:11:23<20:35, 28.08s/it, 4074.31/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:37:53,323] Trial 56 finished with value: -0.7543408360128617 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 4, 'units': 128, 'conv_drop': 0.13797735437663072, 'dense': 64, 'dropout': 0.23075557455386583, 'l2': 1.49581959499061e-06, 'lr': 0.001599498160491714, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [1:11:49<59:32, 83.07s/it, 4285.70/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:38:19,342] Trial 57 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [1:12:15<46:10, 65.96s/it, 4311.72/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:38:45,117] Trial 58 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [1:12:27<36:49, 53.90s/it, 4337.47/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:38:57,152] Trial 59 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [1:12:29<27:33, 41.34s/it, 4349.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T60 | FÎ±2=0.756 P=0.521 R=0.978 lstm_conv win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [1:16:00<27:33, 41.34s/it, 4349.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:42:29,909] Trial 60 finished with value: -0.7564387917329093 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.1786845807647062, 'dense': 64, 'dropout': 0.359900026827392, 'l2': 3.3000499138330928e-06, 'lr': 0.00193590456968632, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [1:16:16<1:00:15, 92.71s/it, 4562.09/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:42:45,953] Trial 61 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [1:16:29<44:11, 69.78s/it, 4578.37/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:42:58,972] Trial 62 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [1:16:40<32:31, 52.76s/it, 4591.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:43:10,621] Trial 63 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [1:17:09<24:15, 40.43s/it, 4603.07/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:43:38,816] Trial 64 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [1:17:11<21:26, 36.76s/it, 4631.27/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T65 | FÎ±2=0.738 P=0.521 R=0.932 lstm_conv win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [1:23:29<21:26, 36.76s/it, 4631.27/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:49:59,618] Trial 65 finished with value: -0.7381146871426237 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 128, 'conv_drop': 0.2616057646267626, 'dense': 64, 'dropout': 0.2801565061718615, 'l2': 2.814305011848096e-06, 'lr': 0.0009199291392622619, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [1:24:20<1:19:17, 139.92s/it, 5011.88/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:50:50,310] Trial 66 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [1:25:02<1:02:14, 113.17s/it, 5062.66/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:51:32,298] Trial 67 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [1:25:33<48:57, 91.79s/it, 5104.55/10800 seconds]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:52:03,261] Trial 68 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [1:26:22<37:59, 73.54s/it, 5135.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:52:52,527] Trial 69 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [1:26:24<33:07, 66.27s/it, 5184.80/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T70 | FÎ±2=0.605 P=0.524 R=0.655 conv_lstm win=72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [1:28:54<33:07, 66.27s/it, 5184.80/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:55:23,937] Trial 70 finished with value: -0.604912414318355 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 4, 'units': 64, 'conv_drop': 0.2676245859513692, 'dense': 32, 'dropout': 0.23038576915445977, 'l2': 2.0192902389379724e-06, 'lr': 0.0004206138733297379, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [1:29:22<44:22, 91.80s/it, 5336.19/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:55:52,158] Trial 71 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [1:29:38<33:56, 72.74s/it, 5364.43/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:56:08,311] Trial 72 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [1:29:55<25:05, 55.76s/it, 5380.60/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:56:24,930] Trial 73 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [1:30:32<19:04, 44.02s/it, 5397.23/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:57:02,346] Trial 74 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [1:31:14<17:31, 42.04s/it, 5434.65/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:57:43,718] Trial 75 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [1:31:49<16:44, 41.84s/it, 5476.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:58:18,996] Trial 76 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [1:32:13<15:17, 39.87s/it, 5511.31/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:58:42,773] Trial 77 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [1:32:49<12:51, 35.05s/it, 5535.09/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:59:19,229] Trial 78 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [1:33:26<12:24, 35.47s/it, 5571.56/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 15:59:56,466] Trial 79 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [1:33:28<12:00, 36.00s/it, 5608.80/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T80 | FÎ±2=0.766 P=0.522 R=1.000 conv_lstm win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 43. Best value: -0.763154:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [1:38:31<12:00, 36.00s/it, 5608.80/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:05:01,249] Trial 80 finished with value: -0.765867418899859 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 96, 'kernel': 4, 'units': 128, 'conv_drop': 0.25353715468406557, 'dense': 128, 'dropout': 0.2400109393045205, 'l2': 1.2330234316739454e-06, 'lr': 0.0027549451673990243, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [1:38:33<36:56, 116.64s/it, 5913.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T81 | FÎ±2=0.754 P=0.522 R=0.970 conv_lstm win=54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [1:40:18<36:56, 116.64s/it, 5913.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:06:48,279] Trial 81 finished with value: -0.7538265306122449 and parameters: {'arch': 'conv_lstm', 'window': 54, 'filters': 32, 'kernel': 6, 'units': 32, 'conv_drop': 0.24780265564248816, 'dense': 128, 'dropout': 0.15203249635957786, 'l2': 1.4605959616680251e-06, 'lr': 0.0014485959437495195, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [1:40:20<34:07, 113.75s/it, 6020.61/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T82 | FÎ±2=0.745 P=0.525 R=0.943 conv_lstm win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [1:48:08<34:07, 113.75s/it, 6020.61/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:14:37,891] Trial 82 finished with value: -0.7454929348708786 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 32, 'kernel': 5, 'units': 192, 'conv_drop': 0.28405085280328335, 'dense': 128, 'dropout': 0.07610006940561165, 'l2': 2.0952605078380426e-06, 'lr': 0.002044950575846103, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [1:48:10<1:02:28, 220.52s/it, 6490.24/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T83 | FÎ±2=0.758 P=0.522 R=0.978 conv_lstm win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [1:49:33<1:02:28, 220.52s/it, 6490.24/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:16:03,581] Trial 83 finished with value: -0.7576093849080532 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 32, 'kernel': 5, 'units': 32, 'conv_drop': 0.2021224153022909, 'dense': 128, 'dropout': 0.13554867341584192, 'l2': 3.0130866982184018e-06, 'lr': 0.0008022301262628734, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [1:49:35<48:01, 180.07s/it, 6575.93/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T84 | FÎ±2=0.686 P=0.525 R=0.811 conv_lstm win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [1:51:35<48:01, 180.07s/it, 6575.93/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:18:04,923] Trial 84 finished with value: -0.686471505283215 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 32, 'kernel': 6, 'units': 64, 'conv_drop': 0.16140691771117316, 'dense': 128, 'dropout': 0.12281630692639198, 'l2': 2.706615571231907e-06, 'lr': 0.0011431980447298225, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [1:52:25<40:36, 162.45s/it, 6697.28/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:18:55,338] Trial 85 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [1:52:36<30:05, 128.94s/it, 6748.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:19:06,526] Trial 86 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [1:52:50<20:16, 93.60s/it, 6759.16/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:19:19,794] Trial 87 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [1:53:04<13:53, 69.49s/it, 6772.39/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:19:34,223] Trial 88 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [1:53:28<09:42, 52.97s/it, 6786.82/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:19:58,066] Trial 89 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [1:53:40<07:22, 44.24s/it, 6810.68/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:20:09,873] Trial 90 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [1:54:04<05:10, 34.51s/it, 6822.49/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:20:33,917] Trial 91 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [1:54:16<04:11, 31.38s/it, 6846.55/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:20:46,310] Trial 92 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [1:54:38<02:59, 25.69s/it, 6858.96/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:21:08,391] Trial 93 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [1:54:41<02:27, 24.60s/it, 6881.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T94 | FÎ±2=0.766 P=0.522 R=1.000 lstm_conv win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 80. Best value: -0.765867:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [2:00:14<02:27, 24.60s/it, 6881.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:26:44,439] Trial 94 finished with value: -0.7659574468085105 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 32, 'kernel': 4, 'units': 128, 'conv_drop': 0.17402498680568057, 'dense': 64, 'dropout': 0.38389775614113647, 'l2': 2.3729148086507226e-06, 'lr': 0.0028800779177212033, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 94 with value: -0.7659574468085105.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [2:00:41<09:49, 117.98s/it, 7216.89/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:27:10,685] Trial 95 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [2:01:13<06:01, 90.46s/it, 7243.16/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:27:43,134] Trial 96 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [2:01:53<03:39, 73.06s/it, 7275.62/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:28:23,150] Trial 97 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [2:02:09<02:06, 63.15s/it, 7315.63/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:28:39,182] Trial 98 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [2:02:42<00:49, 49.01s/it, 7331.66/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-11 16:29:11,961] Trial 99 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 94. Best value: -0.765957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [2:02:44<00:00, 73.64s/it, 7364.45/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"arch\": \"lstm_conv\",\n",
      "  \"window\": 66,\n",
      "  \"filters\": 32,\n",
      "  \"kernel\": 4,\n",
      "  \"units\": 128,\n",
      "  \"conv_drop\": 0.17402498680568057,\n",
      "  \"dense\": 64,\n",
      "  \"dropout\": 0.38389775614113647,\n",
      "  \"l2\": 2.3729148086507226e-06,\n",
      "  \"lr\": 0.0028800779177212033,\n",
      "  \"batch\": 64,\n",
      "  \"act\": \"relu\",\n",
      "  \"pool\": \"gap\",\n",
      "  \"conv_blocks\": 2,\n",
      "  \"optim\": \"nadam\",\n",
      "  \"F\\u03b12\": 0.7659574468085105,\n",
      "  \"P\": 0.5217391304347826,\n",
      "  \"R\": 1.0\n",
      "}\n",
      "ðŸ“œ Done â€“ scaler saved â†’ cnn_lstm_scaler.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  CNNâ€“LSTM Optuna Trainer  (GPU, XLA, big batches)\n",
    "# =============================================================\n",
    "import os, json, gc, warnings, optuna\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from optuna_integration.tfkeras import TFKerasPruningCallback\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Runtime setup â”€â”€â”€â”€â”€\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.config.optimizer.set_jit(True)            # XLA on\n",
    "\n",
    "SEED, VAL_FRAC, ALPHA = 42, 0.20, 2.0\n",
    "N_TRIALS, TIMEOUT      = 100, 3 * 60 * 60    # 2h wall limit\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "SCALER_PKL = \"cnn_lstm_scaler.pkl\"\n",
    "\n",
    "DROP_COLS = [  # duplicates removed\n",
    "    \"open\",\"high\",\"low\",\"typical_price\",\"EMA_21\",\"SMA_20\",\"close_4h\",\n",
    "    \"bollinger_upper\",\"bollinger_lower\",\"resistance_level\",\"support_level\",\n",
    "    \"high_low\",\"high_close\",\"low_close\",\"true_range\",\"volume_mean_20\",\n",
    "    \"MACD_line\",\"MACD_signal\",\"volatility_regime\",\"trending_market\",\n",
    "    \"above_sma50\",\"ema7_above_ema21\",\"rsi_overbought\",\"stoch_oversold\",\n",
    "    \"cci_oversold\",\"near_upper_band\",\"near_lower_band\",\"break_upper_band\",\n",
    "    \"break_lower_band\",\"rsi_oversold\",\"above_sma20\",\"macd_positive\",\n",
    "    \"volume_breakout\",\"volume_breakdown\",\"bullish_scenario_1\",\"bullish_scenario_2\",\n",
    "    \"bullish_scenario_3\",\"bullish_scenario_4\",\"bullish_scenario_5\",\"bullish_scenario_6\",\n",
    "    \"bearish_scenario_1\",\"bearish_scenario_2\",\"bearish_scenario_3\",\"bearish_scenario_4\",\n",
    "    \"bearish_scenario_6\",\"ema_cross_up\",\"macd_cross_up\",\n",
    "    \"oversold_reversal\",\"overbought_reversal\",\"close\"\n",
    "]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Data â”€â”€â”€â”€â”€\n",
    "df = (pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "        .loc[\"2018-01-01\":]\n",
    "        .drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        .dropna(subset=[\"target\"]).dropna())\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "split = int(len(df) * (1 - VAL_FRAC))\n",
    "scaler = StandardScaler().fit(X_raw[:split])\n",
    "joblib.dump(scaler, SCALER_PKL)\n",
    "\n",
    "X_tr_raw = scaler.transform(X_raw[:split]);  y_tr_raw = y_raw[:split]\n",
    "X_va_raw = scaler.transform(X_raw[split:]);   y_va_raw = y_raw[split:]\n",
    "\n",
    "pos   = y_tr_raw.mean()\n",
    "W0, W1 = 1.0, np.float32((1 - pos) / pos) if pos else np.float32(1.0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Window cache â”€â”€â”€â”€â”€\n",
    "_CACHE: Dict[Tuple[int,int,int], Tuple[np.ndarray,np.ndarray]] = {}\n",
    "def make_windows(arr, lab, win):\n",
    "    k = (len(arr), win, arr.shape[1])\n",
    "    if k in _CACHE: return _CACHE[k]\n",
    "    xs, ys = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        xs.append(arr[i-win:i]); ys.append(lab[i])\n",
    "    Xw, yw = np.asarray(xs,np.float32), np.asarray(ys,np.int8)\n",
    "    if Xw.nbytes + yw.nbytes < 1e9:\n",
    "        _CACHE[k] = (Xw, yw)\n",
    "    return Xw, yw\n",
    "\n",
    "def wf1(y, p, alpha=ALPHA):\n",
    "    hat = (p >= .5).astype(int)\n",
    "    pr  = precision_score(y, hat, zero_division=0)\n",
    "    rc  = recall_score   (y, hat, zero_division=0)\n",
    "    return 0 if pr+rc == 0 else (1+alpha)*pr*rc/(alpha*pr+rc)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Model â”€â”€â”€â”€â”€\n",
    "def build(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "\n",
    "    if cfg[\"arch\"] == \"conv_lstm\":\n",
    "        x = inp\n",
    "        for _ in range(cfg[\"conv_blocks\"]):\n",
    "            x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                              activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "            x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "        x = layers.LSTM(cfg[\"units\"], dropout=0.0)(x)             # cuDNN fast\n",
    "    else:  # lstm_conv\n",
    "        x = layers.LSTM(cfg[\"units\"], dropout=0.0,\n",
    "                        return_sequences=True)(inp)\n",
    "        x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"same\",\n",
    "                          activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "        x = (layers.GlobalMaxPooling1D() if cfg[\"pool\"]==\"gmp\"\n",
    "             else layers.GlobalAveragePooling1D())(x)\n",
    "\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    w0 = tf.constant(W0, dtype=tf.float32)\n",
    "    w1 = tf.constant(W1, dtype=tf.float32)\n",
    "    def loss(y_t, y_p):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        weights = tf.where(tf.equal(y_t, 1), w1, w0)\n",
    "        return tf.reduce_mean(weights * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    opt = (keras.optimizers.AdamW(cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "           if cfg[\"optim\"] == \"adamw\"\n",
    "           else keras.optimizers.Nadam(cfg[\"lr\"]) if cfg[\"optim\"] == \"nadam\"\n",
    "           else keras.optimizers.Adam(cfg[\"lr\"]))\n",
    "    model.compile(opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Optuna objective â”€â”€â”€â”€â”€\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    cfg = dict(\n",
    "        arch        = trial.suggest_categorical(\"arch\", [\"conv_lstm\",\"lstm_conv\"]),\n",
    "        window      = trial.suggest_int(\"window\", 12, 72, step=6),\n",
    "        filters     = trial.suggest_categorical(\"filters\",[32,48,64,96,128]),\n",
    "        kernel      = trial.suggest_int(\"kernel\", 2, 7),\n",
    "        units       = trial.suggest_categorical(\"units\",[32,64,96,128,192]),\n",
    "        conv_drop   = trial.suggest_float(\"conv_drop\", 0, .3),\n",
    "        dense       = trial.suggest_categorical(\"dense\",[32,64,128]),\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0, .4),\n",
    "        l2          = trial.suggest_float(\"l2\", 1e-6, 1e-3, log=True),\n",
    "        lr          = trial.suggest_float(\"lr\", 5e-5, 3e-3, log=True),\n",
    "        batch       = trial.suggest_categorical(\"batch\",[64,128,256]),\n",
    "        act         = trial.suggest_categorical(\"act\", [\"relu\",\"elu\"]),\n",
    "        pool        = trial.suggest_categorical(\"pool\",[\"gmp\",\"gap\"]),\n",
    "        conv_blocks = trial.suggest_int(\"conv_blocks\", 1, 2),\n",
    "        optim       = trial.suggest_categorical(\"optim\", [\"adam\",\"adamw\",\"nadam\"]),\n",
    "        units_dummy = 0  # placeholder so cfg dict has same keys in both branches\n",
    "    )\n",
    "    Xtr, ytr = make_windows(X_tr_raw, y_tr_raw, cfg[\"window\"])\n",
    "    Xva, yva = make_windows(X_va_raw, y_va_raw, cfg[\"window\"])\n",
    "    if len(Xtr) < cfg[\"batch\"] * 4:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    ds_tr = (tf.data.Dataset.from_tensor_slices((Xtr, ytr))\n",
    "             .shuffle(10_000).batch(cfg[\"batch\"]).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_va = (tf.data.Dataset.from_tensor_slices((Xva, yva))\n",
    "             .batch(cfg[\"batch\"]).prefetch(tf.data.AUTOTUNE))  # not shuffled â€“ label order preserved\n",
    "\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    model = build(cfg)\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0),\n",
    "          keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, min_lr=1e-6, verbose=0),\n",
    "          TFKerasPruningCallback(trial, \"val_loss\")]\n",
    "    model.fit(ds_tr, validation_data=ds_va, epochs=100, callbacks=cb, verbose=0)\n",
    "\n",
    "    prob  = model.predict(ds_va, verbose=0).ravel()\n",
    "    y_hat = (prob >= .5).astype(int)\n",
    "    score = wf1(yva, prob)\n",
    "\n",
    "    pr = precision_score(yva, y_hat, zero_division=0)\n",
    "    rc = recall_score   (yva, y_hat, zero_division=0)\n",
    "    trial.set_user_attr(\"precision\", pr); trial.set_user_attr(\"recall\", rc)\n",
    "\n",
    "    print(f\"T{trial.number:02d} | FÎ±2={score:.3f} P={pr:.3f} R={rc:.3f} \"\n",
    "          f\"{cfg['arch']} win={cfg['window']}\")\n",
    "\n",
    "    del model; tf.keras.backend.clear_session(); gc.collect()\n",
    "    return -score\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Search â”€â”€â”€â”€â”€\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED, multivariate=True),\n",
    "    pruner =optuna.pruners.MedianPruner(n_startup_trials=5))\n",
    "print(f\"ðŸš€  Starting {N_TRIALS}-trial search (big batches, cuDNN, XLA)â€¦\")\n",
    "study.optimize(objective, n_trials=N_TRIALS,\n",
    "               timeout=TIMEOUT, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "best, ts = study.best_trial, datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(json.dumps({**best.params, \"FÎ±2\": -best.value,\n",
    "                  \"P\": best.user_attrs[\"precision\"],\n",
    "                  \"R\": best.user_attrs[\"recall\"]}, indent=2))\n",
    "\n",
    "json.dump(best.params, open(f\"best_params_cnn_lstm_{ts}.json\",\"w\"), indent=2)\n",
    "study.trials_dataframe().to_csv(f\"trials_cnn_lstm_{ts}.csv\", index=False)\n",
    "print(f\"ðŸ“œ Done â€“ scaler saved â†’ {SCALER_PKL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "[I 2025-06-11 14:26:29,625] A new study created in memory with name: no-name-7c072783-b210-43ed-a4cc-e469a26c0a4b\n",
    "ðŸš€  Starting 100-trial search (big batches, cuDNN, XLA)â€¦\n",
    "  0%|          | 0/100 [00:00<?, ?it/s]T00 | FÎ±2=0.595 P=0.521 R=0.640 lstm_conv win=60\n",
    "  0%|          | 0/100 [02:29<?, ?it/s][I 2025-06-11 14:28:59,301] Trial 0 finished with value: -0.5948424068767908 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 128, 'kernel': 5, 'units': 96, 'conv_drop': 0.05454749016213018, 'dense': 128, 'dropout': 0.17277800745684632, 'l2': 7.4763120622522945e-06, 'lr': 0.000612261560602803, 'batch': 256, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n",
    "Best trial: 0. Best value: -0.594842:   1%|          | 1/100 [02:31<4:09:18, 151.10s/it, 151.10/10800 seconds]T01 | FÎ±2=0.391 P=0.522 R=0.348 lstm_conv win=72\n",
    "Best trial: 0. Best value: -0.594842:   1%|          | 1/100 [06:24<4:09:18, 151.10s/it, 151.10/10800 seconds][I 2025-06-11 14:32:53,658] Trial 1 finished with value: -0.3912740775121838 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 32, 'kernel': 2, 'units': 96, 'conv_drop': 0.09351332282682329, 'dense': 64, 'dropout': 0.38783385110582347, 'l2': 0.00021154290797261214, 'lr': 0.0023417537373087377, 'batch': 256, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n",
    "Best trial: 0. Best value: -0.594842:   2%|â–         | 2/100 [06:25<5:26:48, 200.09s/it, 385.48/10800 seconds]T02 | FÎ±2=0.547 P=0.518 R=0.563 lstm_conv win=18\n",
    "Best trial: 0. Best value: -0.594842:   2%|â–         | 2/100 [06:50<5:26:48, 200.09s/it, 385.48/10800 seconds][I 2025-06-11 14:33:20,153] Trial 2 finished with value: -0.5472773736976606 and parameters: {'arch': 'lstm_conv', 'window': 18, 'filters': 64, 'kernel': 2, 'units': 32, 'conv_drop': 0.10753971856328177, 'dense': 64, 'dropout': 0.13235920994105968, 'l2': 1.5512259126484753e-06, 'lr': 0.00017862556214123332, 'batch': 128, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 0 with value: -0.5948424068767908.\n",
    "Best trial: 0. Best value: -0.594842:   3%|â–Ž         | 3/100 [06:51<3:15:20, 120.83s/it, 411.99/10800 seconds]T03 | FÎ±2=0.527 P=0.533 R=0.524 conv_lstm win=12\n",
    "Best trial: 0. Best value: -0.594842:   3%|â–Ž         | 3/100 [07:34<3:15:20, 120.83s/it, 411.99/10800 seconds][I 2025-06-11 14:34:03,842] Trial 3 finished with value: -0.5269035532994923 and parameters: {'arch': 'conv_lstm', 'window': 12, 'filters': 64, 'kernel': 7, 'units': 96, 'conv_drop': 0.0869254358741304, 'dense': 64, 'dropout': 0.2533615026041694, 'l2': 0.00041151130495610907, 'lr': 0.001342828384653336, 'batch': 128, 'act': 'elu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 0 with value: -0.5948424068767908.\n",
    "Best trial: 0. Best value: -0.594842:   4%|â–         | 4/100 [07:35<2:24:36, 90.38s/it, 455.69/10800 seconds] T04 | FÎ±2=0.677 P=0.518 R=0.800 lstm_conv win=36\n",
    "Best trial: 0. Best value: -0.594842:   4%|â–         | 4/100 [09:37<2:24:36, 90.38s/it, 455.69/10800 seconds][I 2025-06-11 14:36:07,021] Trial 4 finished with value: -0.677185721676151 and parameters: {'arch': 'lstm_conv', 'window': 36, 'filters': 96, 'kernel': 5, 'units': 96, 'conv_drop': 0.14917455176771563, 'dense': 32, 'dropout': 0.24382573359195875, 'l2': 3.2213437409123405e-05, 'lr': 6.173152563090738e-05, 'batch': 128, 'act': 'elu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 4 with value: -0.677185721676151.\n",
    "Best trial: 4. Best value: -0.677186:   5%|â–Œ         | 5/100 [11:40<2:41:49, 102.21s/it, 578.87/10800 seconds][I 2025-06-11 14:38:10,590] Trial 5 pruned. Trial was pruned at epoch 16.\n",
    "Best trial: 4. Best value: -0.677186:   6%|â–Œ         | 6/100 [11:55<2:51:31, 109.48s/it, 702.47/10800 seconds][I 2025-06-11 14:38:25,192] Trial 6 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 4. Best value: -0.677186:   7%|â–‹         | 7/100 [11:57<2:01:36, 78.46s/it, 717.05/10800 seconds] T07 | FÎ±2=0.691 P=0.519 R=0.828 conv_lstm win=72\n",
    "Best trial: 4. Best value: -0.677186:   7%|â–‹         | 7/100 [14:14<2:01:36, 78.46s/it, 717.05/10800 seconds][I 2025-06-11 14:40:44,050] Trial 7 finished with value: -0.6909967292132897 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 5, 'units': 96, 'conv_drop': 0.19364168877215035, 'dense': 128, 'dropout': 0.3659457560881794, 'l2': 1.2896625857891445e-05, 'lr': 5.326650551390532e-05, 'batch': 256, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n",
    "Best trial: 7. Best value: -0.690997:   8%|â–Š         | 8/100 [14:39<2:29:51, 97.73s/it, 856.05/10800 seconds][I 2025-06-11 14:41:09,220] Trial 8 pruned. Trial was pruned at epoch 4.\n",
    "Best trial: 7. Best value: -0.690997:   9%|â–‰         | 9/100 [14:52<1:53:49, 75.05s/it, 881.23/10800 seconds][I 2025-06-11 14:41:22,606] Trial 9 pruned. Trial was pruned at epoch 4.\n",
    "Best trial: 7. Best value: -0.690997:  10%|â–ˆ         | 10/100 [14:54<1:24:01, 56.01s/it, 894.61/10800 seconds]T10 | FÎ±2=0.685 P=0.532 R=0.801 conv_lstm win=72\n",
    "Best trial: 7. Best value: -0.690997:  10%|â–ˆ         | 10/100 [16:22<1:24:01, 56.01s/it, 894.61/10800 seconds][I 2025-06-11 14:42:51,866] Trial 10 finished with value: -0.6853813559322035 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 32, 'conv_drop': 0.14914175826424267, 'dense': 128, 'dropout': 0.36311711503988925, 'l2': 2.4631096227367697e-05, 'lr': 9.919404853103473e-05, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n",
    "Best trial: 7. Best value: -0.690997:  11%|â–ˆ         | 11/100 [16:35<1:38:11, 66.19s/it, 983.89/10800 seconds][I 2025-06-11 14:43:04,653] Trial 11 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 7. Best value: -0.690997:  12%|â–ˆâ–        | 12/100 [16:36<1:13:17, 49.97s/it, 996.75/10800 seconds]T12 | FÎ±2=0.588 P=0.527 R=0.625 conv_lstm win=66\n",
    "Best trial: 7. Best value: -0.690997:  12%|â–ˆâ–        | 12/100 [18:20<1:13:17, 49.97s/it, 996.75/10800 seconds][I 2025-06-11 14:44:50,201] Trial 12 finished with value: -0.5884861407249468 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 6, 'units': 96, 'conv_drop': 0.22606400599215065, 'dense': 128, 'dropout': 0.22176648138502159, 'l2': 4.5679942859048045e-06, 'lr': 5.5459545060680684e-05, 'batch': 128, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 7 with value: -0.6909967292132897.\n",
    "Best trial: 7. Best value: -0.690997:  13%|â–ˆâ–Ž        | 13/100 [18:31<1:36:52, 66.82s/it, 1102.33/10800 seconds][I 2025-06-11 14:45:01,423] Trial 13 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 7. Best value: -0.690997:  14%|â–ˆâ–        | 14/100 [18:33<1:11:41, 50.01s/it, 1113.51/10800 seconds]T14 | FÎ±2=0.659 P=0.530 R=0.750 conv_lstm win=60\n",
    "Best trial: 7. Best value: -0.690997:  14%|â–ˆâ–        | 14/100 [20:00<1:11:41, 50.01s/it, 1113.51/10800 seconds][I 2025-06-11 14:46:29,946] Trial 14 finished with value: -0.6591442498645965 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 48, 'kernel': 7, 'units': 96, 'conv_drop': 0.16854365802480042, 'dense': 128, 'dropout': 0.34212635055038104, 'l2': 5.34107900020877e-06, 'lr': 8.139951880089569e-05, 'batch': 256, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 7 with value: -0.6909967292132897.\n",
    "Best trial: 7. Best value: -0.690997:  15%|â–ˆâ–Œ        | 15/100 [20:10<1:27:16, 61.60s/it, 1201.98/10800 seconds][I 2025-06-11 14:46:40,476] Trial 15 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 7. Best value: -0.690997:  16%|â–ˆâ–Œ        | 16/100 [20:26<1:04:43, 46.24s/it, 1212.53/10800 seconds][I 2025-06-11 14:46:56,098] Trial 16 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 7. Best value: -0.690997:  17%|â–ˆâ–‹        | 17/100 [20:28<51:14, 37.04s/it, 1228.20/10800 seconds]  T17 | FÎ±2=0.709 P=0.519 R=0.868 conv_lstm win=72\n",
    "Best trial: 7. Best value: -0.690997:  17%|â–ˆâ–‹        | 17/100 [21:57<51:14, 37.04s/it, 1228.20/10800 seconds][I 2025-06-11 14:48:26,804] Trial 17 finished with value: -0.7087967644084935 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 2, 'units': 32, 'conv_drop': 0.2020859487428539, 'dense': 128, 'dropout': 0.3311343605595314, 'l2': 5.446410711918106e-06, 'lr': 0.0002933566355803551, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 17 with value: -0.7087967644084935.\n",
    "Best trial: 17. Best value: -0.708797:  18%|â–ˆâ–Š        | 18/100 [21:58<1:12:35, 53.11s/it, 1318.71/10800 seconds]T18 | FÎ±2=0.729 P=0.529 R=0.898 conv_lstm win=66\n",
    "Best trial: 17. Best value: -0.708797:  18%|â–ˆâ–Š        | 18/100 [26:04<1:12:35, 53.11s/it, 1318.71/10800 seconds][I 2025-06-11 14:52:34,085] Trial 18 finished with value: -0.7285785869383664 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.1829073485357999, 'dense': 64, 'dropout': 0.3367267654373487, 'l2': 1.9949353471421903e-06, 'lr': 0.0010969018481585208, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 18 with value: -0.7285785869383664.\n",
    "Best trial: 18. Best value: -0.728579:  19%|â–ˆâ–‰        | 19/100 [26:19<2:30:26, 111.43s/it, 1566.01/10800 seconds][I 2025-06-11 14:52:48,629] Trial 19 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 18. Best value: -0.728579:  20%|â–ˆâ–ˆ        | 20/100 [26:20<1:49:48, 82.35s/it, 1580.59/10800 seconds] T20 | FÎ±2=0.732 P=0.524 R=0.914 conv_lstm win=60\n",
    "Best trial: 18. Best value: -0.728579:  20%|â–ˆâ–ˆ        | 20/100 [30:17<1:49:48, 82.35s/it, 1580.59/10800 seconds][I 2025-06-11 14:56:46,797] Trial 20 finished with value: -0.7319723411261113 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 96, 'kernel': 2, 'units': 128, 'conv_drop': 0.23169116379282378, 'dense': 128, 'dropout': 0.22993558336668984, 'l2': 1.1980316309987873e-06, 'lr': 0.0010514052377099624, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  21%|â–ˆâ–ˆ        | 21/100 [30:18<2:50:00, 129.12s/it, 1818.73/10800 seconds]T21 | FÎ±2=0.597 P=0.533 R=0.636 conv_lstm win=48\n",
    "Best trial: 20. Best value: -0.731972:  21%|â–ˆâ–ˆ        | 21/100 [33:25<2:50:00, 129.12s/it, 1818.73/10800 seconds][I 2025-06-11 14:59:54,707] Trial 21 finished with value: -0.5973476840284452 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 96, 'kernel': 3, 'units': 128, 'conv_drop': 0.13417079120549671, 'dense': 128, 'dropout': 0.20486313559166633, 'l2': 1.3625466836519646e-06, 'lr': 0.0009418760303623328, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  22%|â–ˆâ–ˆâ–       | 22/100 [33:26<3:10:46, 146.75s/it, 2006.62/10800 seconds]T22 | FÎ±2=0.672 P=0.528 R=0.778 conv_lstm win=66\n",
    "Best trial: 20. Best value: -0.731972:  22%|â–ˆâ–ˆâ–       | 22/100 [37:25<3:10:46, 146.75s/it, 2006.62/10800 seconds][I 2025-06-11 15:03:55,538] Trial 22 finished with value: -0.6717611515905456 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 64, 'kernel': 2, 'units': 128, 'conv_drop': 0.20725498441456727, 'dense': 64, 'dropout': 0.1514327494157322, 'l2': 3.4372389137912706e-06, 'lr': 0.0011623426402264853, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [37:58<3:44:35, 175.01s/it, 2247.53/10800 seconds][I 2025-06-11 15:04:28,071] Trial 23 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  24%|â–ˆâ–ˆâ–       | 24/100 [38:00<2:47:31, 132.26s/it, 2280.06/10800 seconds]T24 | FÎ±2=0.717 P=0.523 R=0.880 lstm_conv win=60\n",
    "Best trial: 20. Best value: -0.731972:  24%|â–ˆâ–ˆâ–       | 24/100 [40:47<2:47:31, 132.26s/it, 2280.06/10800 seconds][I 2025-06-11 15:07:16,928] Trial 24 finished with value: -0.7172275238573581 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 48, 'kernel': 3, 'units': 128, 'conv_drop': 0.22458452100672904, 'dense': 64, 'dropout': 0.27475311523806734, 'l2': 1.162732997626243e-06, 'lr': 0.0012399873702159075, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [41:06<2:59:02, 143.23s/it, 2448.89/10800 seconds][I 2025-06-11 15:07:35,721] Trial 25 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [41:07<2:10:37, 105.91s/it, 2467.72/10800 seconds]T26 | FÎ±2=0.708 P=0.524 R=0.859 lstm_conv win=66\n",
    "Best trial: 20. Best value: -0.731972:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [43:12<2:10:37, 105.91s/it, 2467.72/10800 seconds][I 2025-06-11 15:09:42,598] Trial 26 finished with value: -0.7082767978290366 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 48, 'kernel': 3, 'units': 64, 'conv_drop': 0.18808609956374508, 'dense': 128, 'dropout': 0.3124217042135821, 'l2': 1.701627483058166e-06, 'lr': 0.0020810000414228608, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  27%|â–ˆâ–ˆâ–‹       | 27/100 [43:28<2:16:33, 112.24s/it, 2594.73/10800 seconds][I 2025-06-11 15:09:58,292] Trial 27 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  28%|â–ˆâ–ˆâ–Š       | 28/100 [43:30<1:39:55, 83.27s/it, 2610.43/10800 seconds] T28 | FÎ±2=0.703 P=0.526 R=0.845 lstm_conv win=72\n",
    "Best trial: 20. Best value: -0.731972:  28%|â–ˆâ–ˆâ–Š       | 28/100 [47:08<1:39:55, 83.27s/it, 2610.43/10800 seconds][I 2025-06-11 15:13:38,257] Trial 28 finished with value: -0.703036541430777 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 128, 'kernel': 5, 'units': 128, 'conv_drop': 0.17053482179312465, 'dense': 64, 'dropout': 0.33831466588119824, 'l2': 1.6450611097511145e-06, 'lr': 0.0010824559227546788, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  29%|â–ˆâ–ˆâ–‰       | 29/100 [47:10<2:27:05, 124.30s/it, 2830.44/10800 seconds]T29 | FÎ±2=0.692 P=0.521 R=0.829 conv_lstm win=60\n",
    "Best trial: 20. Best value: -0.731972:  29%|â–ˆâ–ˆâ–‰       | 29/100 [49:44<2:27:05, 124.30s/it, 2830.44/10800 seconds][I 2025-06-11 15:16:14,025] Trial 29 finished with value: -0.6923076923076923 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 32, 'kernel': 2, 'units': 128, 'conv_drop': 0.17655258017029987, 'dense': 64, 'dropout': 0.3750434027374682, 'l2': 1.5304402108416722e-06, 'lr': 0.0018149865587916117, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [49:58<2:36:01, 133.73s/it, 2986.19/10800 seconds][I 2025-06-11 15:16:28,624] Trial 30 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [50:11<1:52:41, 97.99s/it, 3000.77/10800 seconds] [I 2025-06-11 15:16:40,861] Trial 31 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [50:32<1:21:55, 72.29s/it, 3013.09/10800 seconds][I 2025-06-11 15:17:02,199] Trial 32 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [50:34<1:03:38, 57.00s/it, 3034.42/10800 seconds]T33 | FÎ±2=0.628 P=0.531 R=0.691 conv_lstm win=66\n",
    "Best trial: 20. Best value: -0.731972:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [51:37<1:03:38, 57.00s/it, 3034.42/10800 seconds][I 2025-06-11 15:18:07,065] Trial 33 finished with value: -0.628154795288839 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 64, 'kernel': 4, 'units': 32, 'conv_drop': 0.25301886959374464, 'dense': 128, 'dropout': 0.3266113420716731, 'l2': 2.565111517777266e-06, 'lr': 0.0004553361197772574, 'batch': 128, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [51:58<1:05:16, 59.34s/it, 3099.23/10800 seconds][I 2025-06-11 15:18:28,032] Trial 34 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [52:00<51:50, 47.85s/it, 3120.26/10800 seconds]  T35 | FÎ±2=0.602 P=0.539 R=0.639 conv_lstm win=66\n",
    "Best trial: 20. Best value: -0.731972:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [57:05<51:50, 47.85s/it, 3120.26/10800 seconds][I 2025-06-11 15:23:34,952] Trial 35 finished with value: -0.6018608257414227 and parameters: {'arch': 'conv_lstm', 'window': 66, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.2290901400745524, 'dense': 128, 'dropout': 0.35586673619835707, 'l2': 3.4370979194197407e-06, 'lr': 0.0002580404573248242, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [57:29<2:13:53, 125.52s/it, 3427.00/10800 seconds][I 2025-06-11 15:23:58,720] Trial 36 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [57:50<1:39:44, 95.00s/it, 3450.79/10800 seconds] [I 2025-06-11 15:24:20,498] Trial 37 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [58:02<1:15:28, 73.04s/it, 3472.58/10800 seconds][I 2025-06-11 15:24:32,309] Trial 38 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [58:16<55:34, 54.67s/it, 3484.39/10800 seconds]  [I 2025-06-11 15:24:45,997] Trial 39 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [58:18<42:23, 42.39s/it, 3498.12/10800 seconds]T40 | FÎ±2=0.636 P=0.530 R=0.706 conv_lstm win=72\n",
    "Best trial: 20. Best value: -0.731972:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [59:58<42:23, 42.39s/it, 3498.12/10800 seconds][I 2025-06-11 15:26:27,891] Trial 40 finished with value: -0.6355365763089491 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 32, 'conv_drop': 0.2991414066265154, 'dense': 128, 'dropout': 0.3298017348264688, 'l2': 7.605803562468741e-06, 'lr': 0.00012155054083183725, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 20 with value: -0.7319723411261113.\n",
    "Best trial: 20. Best value: -0.731972:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [1:00:09<59:17, 60.30s/it, 3600.21/10800 seconds][I 2025-06-11 15:26:39,012] Trial 41 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [1:00:26<44:00, 45.52s/it, 3611.26/10800 seconds][I 2025-06-11 15:26:56,197] Trial 42 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 20. Best value: -0.731972:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [1:00:28<35:09, 37.01s/it, 3628.42/10800 seconds]T43 | FÎ±2=0.763 P=0.521 R=0.994 lstm_conv win=66\n",
    "Best trial: 20. Best value: -0.731972:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [1:02:17<35:09, 37.01s/it, 3628.42/10800 seconds][I 2025-06-11 15:28:47,316] Trial 43 finished with value: -0.7631537367672618 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 32, 'kernel': 4, 'units': 64, 'conv_drop': 0.09979414734158484, 'dense': 128, 'dropout': 0.3879128314126981, 'l2': 2.073472158914153e-06, 'lr': 0.0025838646128367514, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [1:02:32<55:15, 59.21s/it, 3739.41/10800 seconds][I 2025-06-11 15:29:02,165] Trial 44 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [1:02:56<42:06, 45.94s/it, 3754.40/10800 seconds][I 2025-06-11 15:29:26,136] Trial 45 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [1:03:08<35:26, 39.38s/it, 3778.46/10800 seconds][I 2025-06-11 15:29:37,875] Trial 46 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [1:03:18<27:27, 31.08s/it, 3790.18/10800 seconds][I 2025-06-11 15:29:47,671] Trial 47 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [1:03:19<21:24, 24.70s/it, 3799.98/10800 seconds]T48 | FÎ±2=0.555 P=0.524 R=0.572 lstm_conv win=60\n",
    "Best trial: 43. Best value: -0.763154:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [1:04:38<21:24, 24.70s/it, 3799.98/10800 seconds][I 2025-06-11 15:31:08,037] Trial 48 finished with value: -0.555245313123255 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 96, 'kernel': 3, 'units': 32, 'conv_drop': 0.17899377038959471, 'dense': 128, 'dropout': 0.22152697641509517, 'l2': 1.3640561599870486e-06, 'lr': 0.00042314293757819894, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [1:04:54<35:12, 41.41s/it, 3880.40/10800 seconds][I 2025-06-11 15:31:24,574] Trial 49 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [1:05:27<28:14, 33.89s/it, 3896.73/10800 seconds][I 2025-06-11 15:31:57,236] Trial 50 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [1:05:43<27:24, 33.56s/it, 3929.52/10800 seconds][I 2025-06-11 15:32:13,485] Trial 51 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [1:06:03<22:41, 28.37s/it, 3945.78/10800 seconds][I 2025-06-11 15:32:33,364] Trial 52 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [1:06:05<20:13, 25.82s/it, 3965.64/10800 seconds]T53 | FÎ±2=0.753 P=0.524 R=0.963 conv_lstm win=42\n",
    "Best trial: 43. Best value: -0.763154:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [1:07:20<20:13, 25.82s/it, 3965.64/10800 seconds][I 2025-06-11 15:33:49,953] Trial 53 finished with value: -0.7526332588573252 and parameters: {'arch': 'conv_lstm', 'window': 42, 'filters': 48, 'kernel': 4, 'units': 64, 'conv_drop': 0.1852717446722201, 'dense': 128, 'dropout': 0.38016448336315367, 'l2': 1.5211958873031164e-06, 'lr': 0.001880948993648861, 'batch': 256, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [1:07:38<31:26, 41.02s/it, 4042.12/10800 seconds][I 2025-06-11 15:34:08,476] Trial 54 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [1:07:52<25:42, 34.28s/it, 4060.69/10800 seconds][I 2025-06-11 15:34:21,987] Trial 55 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [1:07:54<20:35, 28.08s/it, 4074.31/10800 seconds]T56 | FÎ±2=0.754 P=0.523 R=0.968 conv_lstm win=72\n",
    "Best trial: 43. Best value: -0.763154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [1:11:23<20:35, 28.08s/it, 4074.31/10800 seconds][I 2025-06-11 15:37:53,323] Trial 56 finished with value: -0.7543408360128617 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 4, 'units': 128, 'conv_drop': 0.13797735437663072, 'dense': 64, 'dropout': 0.23075557455386583, 'l2': 1.49581959499061e-06, 'lr': 0.001599498160491714, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [1:11:49<59:32, 83.07s/it, 4285.70/10800 seconds][I 2025-06-11 15:38:19,342] Trial 57 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [1:12:15<46:10, 65.96s/it, 4311.72/10800 seconds][I 2025-06-11 15:38:45,117] Trial 58 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [1:12:27<36:49, 53.90s/it, 4337.47/10800 seconds][I 2025-06-11 15:38:57,152] Trial 59 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [1:12:29<27:33, 41.34s/it, 4349.51/10800 seconds]T60 | FÎ±2=0.756 P=0.521 R=0.978 lstm_conv win=60\n",
    "Best trial: 43. Best value: -0.763154:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [1:16:00<27:33, 41.34s/it, 4349.51/10800 seconds][I 2025-06-11 15:42:29,909] Trial 60 finished with value: -0.7564387917329093 and parameters: {'arch': 'lstm_conv', 'window': 60, 'filters': 48, 'kernel': 2, 'units': 128, 'conv_drop': 0.1786845807647062, 'dense': 64, 'dropout': 0.359900026827392, 'l2': 3.3000499138330928e-06, 'lr': 0.00193590456968632, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [1:16:16<1:00:15, 92.71s/it, 4562.09/10800 seconds][I 2025-06-11 15:42:45,953] Trial 61 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [1:16:29<44:11, 69.78s/it, 4578.37/10800 seconds]  [I 2025-06-11 15:42:58,972] Trial 62 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [1:16:40<32:31, 52.76s/it, 4591.40/10800 seconds][I 2025-06-11 15:43:10,621] Trial 63 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [1:17:09<24:15, 40.43s/it, 4603.07/10800 seconds][I 2025-06-11 15:43:38,816] Trial 64 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [1:17:11<21:26, 36.76s/it, 4631.27/10800 seconds]T65 | FÎ±2=0.738 P=0.521 R=0.932 lstm_conv win=72\n",
    "Best trial: 43. Best value: -0.763154:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [1:23:29<21:26, 36.76s/it, 4631.27/10800 seconds][I 2025-06-11 15:49:59,618] Trial 65 finished with value: -0.7381146871426237 and parameters: {'arch': 'lstm_conv', 'window': 72, 'filters': 48, 'kernel': 3, 'units': 128, 'conv_drop': 0.2616057646267626, 'dense': 64, 'dropout': 0.2801565061718615, 'l2': 2.814305011848096e-06, 'lr': 0.0009199291392622619, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [1:24:20<1:19:17, 139.92s/it, 5011.88/10800 seconds][I 2025-06-11 15:50:50,310] Trial 66 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [1:25:02<1:02:14, 113.17s/it, 5062.66/10800 seconds][I 2025-06-11 15:51:32,298] Trial 67 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [1:25:33<48:57, 91.79s/it, 5104.55/10800 seconds]   [I 2025-06-11 15:52:03,261] Trial 68 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [1:26:22<37:59, 73.54s/it, 5135.51/10800 seconds][I 2025-06-11 15:52:52,527] Trial 69 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [1:26:24<33:07, 66.27s/it, 5184.80/10800 seconds]T70 | FÎ±2=0.605 P=0.524 R=0.655 conv_lstm win=72\n",
    "Best trial: 43. Best value: -0.763154:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [1:28:54<33:07, 66.27s/it, 5184.80/10800 seconds][I 2025-06-11 15:55:23,937] Trial 70 finished with value: -0.604912414318355 and parameters: {'arch': 'conv_lstm', 'window': 72, 'filters': 48, 'kernel': 4, 'units': 64, 'conv_drop': 0.2676245859513692, 'dense': 32, 'dropout': 0.23038576915445977, 'l2': 2.0192902389379724e-06, 'lr': 0.0004206138733297379, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 43 with value: -0.7631537367672618.\n",
    "Best trial: 43. Best value: -0.763154:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [1:29:22<44:22, 91.80s/it, 5336.19/10800 seconds][I 2025-06-11 15:55:52,158] Trial 71 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [1:29:38<33:56, 72.74s/it, 5364.43/10800 seconds][I 2025-06-11 15:56:08,311] Trial 72 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [1:29:55<25:05, 55.76s/it, 5380.60/10800 seconds][I 2025-06-11 15:56:24,930] Trial 73 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [1:30:32<19:04, 44.02s/it, 5397.23/10800 seconds][I 2025-06-11 15:57:02,346] Trial 74 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [1:31:14<17:31, 42.04s/it, 5434.65/10800 seconds][I 2025-06-11 15:57:43,718] Trial 75 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [1:31:49<16:44, 41.84s/it, 5476.03/10800 seconds][I 2025-06-11 15:58:18,996] Trial 76 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [1:32:13<15:17, 39.87s/it, 5511.31/10800 seconds][I 2025-06-11 15:58:42,773] Trial 77 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [1:32:49<12:51, 35.05s/it, 5535.09/10800 seconds][I 2025-06-11 15:59:19,229] Trial 78 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [1:33:26<12:24, 35.47s/it, 5571.56/10800 seconds][I 2025-06-11 15:59:56,466] Trial 79 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 43. Best value: -0.763154:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [1:33:28<12:00, 36.00s/it, 5608.80/10800 seconds]T80 | FÎ±2=0.766 P=0.522 R=1.000 conv_lstm win=48\n",
    "Best trial: 43. Best value: -0.763154:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [1:38:31<12:00, 36.00s/it, 5608.80/10800 seconds][I 2025-06-11 16:05:01,249] Trial 80 finished with value: -0.765867418899859 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 96, 'kernel': 4, 'units': 128, 'conv_drop': 0.25353715468406557, 'dense': 128, 'dropout': 0.2400109393045205, 'l2': 1.2330234316739454e-06, 'lr': 0.0027549451673990243, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n",
    "Best trial: 80. Best value: -0.765867:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [1:38:33<36:56, 116.64s/it, 5913.58/10800 seconds]T81 | FÎ±2=0.754 P=0.522 R=0.970 conv_lstm win=54\n",
    "Best trial: 80. Best value: -0.765867:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [1:40:18<36:56, 116.64s/it, 5913.58/10800 seconds][I 2025-06-11 16:06:48,279] Trial 81 finished with value: -0.7538265306122449 and parameters: {'arch': 'conv_lstm', 'window': 54, 'filters': 32, 'kernel': 6, 'units': 32, 'conv_drop': 0.24780265564248816, 'dense': 128, 'dropout': 0.15203249635957786, 'l2': 1.4605959616680251e-06, 'lr': 0.0014485959437495195, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n",
    "Best trial: 80. Best value: -0.765867:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [1:40:20<34:07, 113.75s/it, 6020.61/10800 seconds]T82 | FÎ±2=0.745 P=0.525 R=0.943 conv_lstm win=60\n",
    "Best trial: 80. Best value: -0.765867:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [1:48:08<34:07, 113.75s/it, 6020.61/10800 seconds][I 2025-06-11 16:14:37,891] Trial 82 finished with value: -0.7454929348708786 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 32, 'kernel': 5, 'units': 192, 'conv_drop': 0.28405085280328335, 'dense': 128, 'dropout': 0.07610006940561165, 'l2': 2.0952605078380426e-06, 'lr': 0.002044950575846103, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n",
    "Best trial: 80. Best value: -0.765867:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [1:48:10<1:02:28, 220.52s/it, 6490.24/10800 seconds]T83 | FÎ±2=0.758 P=0.522 R=0.978 conv_lstm win=48\n",
    "Best trial: 80. Best value: -0.765867:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [1:49:33<1:02:28, 220.52s/it, 6490.24/10800 seconds][I 2025-06-11 16:16:03,581] Trial 83 finished with value: -0.7576093849080532 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 32, 'kernel': 5, 'units': 32, 'conv_drop': 0.2021224153022909, 'dense': 128, 'dropout': 0.13554867341584192, 'l2': 3.0130866982184018e-06, 'lr': 0.0008022301262628734, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n",
    "Best trial: 80. Best value: -0.765867:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [1:49:35<48:01, 180.07s/it, 6575.93/10800 seconds]  T84 | FÎ±2=0.686 P=0.525 R=0.811 conv_lstm win=48\n",
    "Best trial: 80. Best value: -0.765867:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [1:51:35<48:01, 180.07s/it, 6575.93/10800 seconds][I 2025-06-11 16:18:04,923] Trial 84 finished with value: -0.686471505283215 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 32, 'kernel': 6, 'units': 64, 'conv_drop': 0.16140691771117316, 'dense': 128, 'dropout': 0.12281630692639198, 'l2': 2.706615571231907e-06, 'lr': 0.0011431980447298225, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 80 with value: -0.765867418899859.\n",
    "Best trial: 80. Best value: -0.765867:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [1:52:25<40:36, 162.45s/it, 6697.28/10800 seconds][I 2025-06-11 16:18:55,338] Trial 85 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [1:52:36<30:05, 128.94s/it, 6748.03/10800 seconds][I 2025-06-11 16:19:06,526] Trial 86 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [1:52:50<20:16, 93.60s/it, 6759.16/10800 seconds] [I 2025-06-11 16:19:19,794] Trial 87 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [1:53:04<13:53, 69.49s/it, 6772.39/10800 seconds][I 2025-06-11 16:19:34,223] Trial 88 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [1:53:28<09:42, 52.97s/it, 6786.82/10800 seconds][I 2025-06-11 16:19:58,066] Trial 89 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [1:53:40<07:22, 44.24s/it, 6810.68/10800 seconds][I 2025-06-11 16:20:09,873] Trial 90 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [1:54:04<05:10, 34.51s/it, 6822.49/10800 seconds][I 2025-06-11 16:20:33,917] Trial 91 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [1:54:16<04:11, 31.38s/it, 6846.55/10800 seconds][I 2025-06-11 16:20:46,310] Trial 92 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [1:54:38<02:59, 25.69s/it, 6858.96/10800 seconds][I 2025-06-11 16:21:08,391] Trial 93 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 80. Best value: -0.765867:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [1:54:41<02:27, 24.60s/it, 6881.04/10800 seconds]T94 | FÎ±2=0.766 P=0.522 R=1.000 lstm_conv win=66\n",
    "Best trial: 80. Best value: -0.765867:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [2:00:14<02:27, 24.60s/it, 6881.04/10800 seconds][I 2025-06-11 16:26:44,439] Trial 94 finished with value: -0.7659574468085105 and parameters: {'arch': 'lstm_conv', 'window': 66, 'filters': 32, 'kernel': 4, 'units': 128, 'conv_drop': 0.17402498680568057, 'dense': 64, 'dropout': 0.38389775614113647, 'l2': 2.3729148086507226e-06, 'lr': 0.0028800779177212033, 'batch': 64, 'act': 'relu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 94 with value: -0.7659574468085105.\n",
    "Best trial: 94. Best value: -0.765957:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [2:00:41<09:49, 117.98s/it, 7216.89/10800 seconds][I 2025-06-11 16:27:10,685] Trial 95 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 94. Best value: -0.765957:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [2:01:13<06:01, 90.46s/it, 7243.16/10800 seconds] [I 2025-06-11 16:27:43,134] Trial 96 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 94. Best value: -0.765957:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [2:01:53<03:39, 73.06s/it, 7275.62/10800 seconds][I 2025-06-11 16:28:23,150] Trial 97 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 94. Best value: -0.765957:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [2:02:09<02:06, 63.15s/it, 7315.63/10800 seconds][I 2025-06-11 16:28:39,182] Trial 98 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 94. Best value: -0.765957:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [2:02:42<00:49, 49.01s/it, 7331.66/10800 seconds][I 2025-06-11 16:29:11,961] Trial 99 pruned. Trial was pruned at epoch 0.\n",
    "Best trial: 94. Best value: -0.765957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [2:02:44<00:00, 73.64s/it, 7364.45/10800 seconds]{\n",
    "  \"arch\": \"lstm_conv\",\n",
    "  \"window\": 66,\n",
    "  \"filters\": 32,\n",
    "  \"kernel\": 4,\n",
    "  \"units\": 128,\n",
    "  \"conv_drop\": 0.17402498680568057,\n",
    "  \"dense\": 64,\n",
    "  \"dropout\": 0.38389775614113647,\n",
    "  \"l2\": 2.3729148086507226e-06,\n",
    "  \"lr\": 0.0028800779177212033,\n",
    "  \"batch\": 64,\n",
    "  \"act\": \"relu\",\n",
    "  \"pool\": \"gap\",\n",
    "  \"conv_blocks\": 2,\n",
    "  \"optim\": \"nadam\",\n",
    "  \"F\\u03b12\": 0.7659574468085105,\n",
    "  \"P\": 0.5217391304347826,\n",
    "  \"R\": 1.0\n",
    "}\n",
    "ðŸ“œ Done â€“ scaler saved â†’ cnn_lstm_scaler.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e77ea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 141)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:141\u001b[1;36m\u001b[0m\n\u001b[1;33m    else layers.GlobalAveragePooling1D()(x))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, gc, warnings, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, roc_curve, auc,\n",
    "                             precision_recall_curve)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping,\n",
    "                                        ReduceLROnPlateau)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# â”€â”€â”€ fixed params from Optuna â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BEST_PARAMS = dict(\n",
    "    arch=\"lstm_conv\", window=24, filters=64, kernel=5, units=32,\n",
    "    conv_drop=0.220, lstm_drop=0.285, dense=32, dropout=0.378,\n",
    "    l2=1.449e-6, lr=0.00135, batch=32, act=\"relu\", pool=\"gmp\"\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ paths & config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_21', 'SMA_20',\n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower',\n",
    "    'resistance_level', 'support_level', 'high_low', 'high_close', 'low_close',\n",
    "    'true_range', 'volume_mean_20', 'MACD_line', 'MACD_signal',\n",
    "    'volatility_regime', 'trending_market', 'above_sma50', 'ema7_above_ema21',\n",
    "    'rsi_overbought', 'stoch_oversold', 'cci_oversold',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'break_upper_band', 'break_lower_band', 'rsi_oversold',\n",
    "    'above_sma20', 'macd_positive', 'volume_breakout', 'volume_breakdown',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6','ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal'\n",
    "]]\n",
    "\n",
    "SEED, VAL_FRAC, TEST_FRAC, ALPHA = 42, .15, .15, 2.0\n",
    "EPOCHS, PATIENCE = 200, 20\n",
    "\n",
    "OUT_DIR = Path(\"model_outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# GPU setup with feedback\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for g in gpus:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    print(f\"âœ… Found {len(gpus)} GPU(s)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU found, using CPU\")\n",
    "\n",
    "# â”€â”€â”€ data load & split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Loading data...\")\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors='ignore')\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "# Store feature names before dropping target\n",
    "feat_names = [col for col in df.columns if col != \"target\"]\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(np.int8).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "n = len(df)\n",
    "val_start = int(n * (1 - VAL_FRAC - TEST_FRAC))\n",
    "test_start = int(n * (1 - TEST_FRAC))\n",
    "train_idx = np.arange(0, val_start)\n",
    "val_idx = np.arange(val_start, test_start)\n",
    "test_idx = np.arange(test_start, n)\n",
    "\n",
    "scaler = StandardScaler().fit(X_raw[train_idx])\n",
    "X_train = scaler.transform(X_raw[train_idx])\n",
    "y_train = y_raw[train_idx]\n",
    "X_val = scaler.transform(X_raw[val_idx])\n",
    "y_val = y_raw[val_idx]\n",
    "X_test = scaler.transform(X_raw[test_idx])\n",
    "y_test = y_raw[test_idx]\n",
    "\n",
    "joblib.dump(scaler, OUT_DIR / f\"scaler_{STAMP}.pkl\")\n",
    "\n",
    "pos = y_train.mean()\n",
    "W0, W1 = 1.0, (1-pos)/pos if pos else 1.0\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "print(f\"Positive rate: {pos:.3f}, Weights: W0={W0:.2f}, W1={W1:.2f}\")\n",
    "\n",
    "# â”€â”€â”€ window helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def windows(X, y, w):\n",
    "    xs, ys = [], []\n",
    "    for i in range(w, len(X)):\n",
    "        xs.append(X[i-w:i])\n",
    "        ys.append(y[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "win = BEST_PARAMS[\"window\"]\n",
    "X_tr_w, y_tr_w = windows(X_train, y_train, win)\n",
    "X_va_w, y_va_w = windows(X_val, y_val, win)\n",
    "X_te_w, y_te_w = windows(X_test, y_test, win)\n",
    "\n",
    "print(f\"Window size: {win}, Train windows: {len(X_tr_w)}\")\n",
    "\n",
    "# â”€â”€â”€ weighted F1 score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def wf1(y, proba, alpha=ALPHA):\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    pr = precision_score(y, preds, zero_division=0)\n",
    "    rc = recall_score(y, preds, zero_division=0)\n",
    "    return 0 if pr + rc == 0 else (1 + alpha) * pr * rc / (alpha * pr + rc)\n",
    "\n",
    "# â”€â”€â”€ model builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "\n",
    "    # LSTM â†’ Conv1D branch\n",
    "    x = layers.LSTM(cfg[\"units\"], dropout=cfg[\"lstm_drop\"],\n",
    "                    return_sequences=True,\n",
    "                    kernel_regularizer=l2)(inp)\n",
    "    x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"same\",\n",
    "                      activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "    x = (layers.GlobalMaxPooling1D()(x) if cfg[\"pool\"]==\"gmp\"\n",
    "         else layers.GlobalAveragePooling1D()(x))\n",
    "    x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                     kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    w0 = tf.constant(W0, dtype=tf.float32)\n",
    "    w1 = tf.constant(W1, dtype=tf.float32)\n",
    "    \n",
    "    def wbce(y_t, y_p):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        w = tf.where(tf.equal(y_t, 1), w1, w0)\n",
    "        return tf.reduce_mean(w * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    model.compile(keras.optimizers.Adam(cfg[\"lr\"]), loss=wbce,\n",
    "                  metrics=['accuracy',\n",
    "                           keras.metrics.Precision(name='precision'),\n",
    "                           keras.metrics.Recall(name='recall'),\n",
    "                           keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "print(\"\\nðŸ—ï¸ Building model...\")\n",
    "model = build(BEST_PARAMS)\n",
    "model.summary()\n",
    "\n",
    "# â”€â”€â”€ callbacks & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "ckpt_path = OUT_DIR / f\"best_model_{STAMP}.keras\"\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr_w, y_tr_w,\n",
    "    validation_data=(X_va_w, y_va_w),\n",
    "    epochs=EPOCHS, batch_size=BEST_PARAMS[\"batch\"],\n",
    "    class_weight={0: W0, 1: W1},\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(patience=10, factor=.5, min_lr=1e-6, verbose=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(OUT_DIR / f\"final_model_{STAMP}.keras\")\n",
    "\n",
    "# â”€â”€â”€ evaluation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(X, y, name):\n",
    "    proba = model.predict(X, verbose=0).ravel()\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    pr = precision_score(y, preds, zero_division=0)\n",
    "    rc = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "    fÎ± = wf1(y, proba)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    print(f\"{name:<11} P={pr:.3f} R={rc:.3f} F1={f1:.3f} FÎ±2={fÎ±:.3f} Acc={acc:.3f}\")\n",
    "    return dict(p=pr, r=rc, f1=f1, fÎ±=fÎ±, acc=acc, proba=proba, preds=preds, y=y)\n",
    "\n",
    "print(\"\\nðŸ“Š Metrics (0.5 threshold)\")\n",
    "train_res = evaluate(X_tr_w, y_tr_w, \"Train\")\n",
    "val_res = evaluate(X_va_w, y_va_w, \"Val\")\n",
    "test_res = evaluate(X_te_w, y_te_w, \"Test\")\n",
    "\n",
    "# â”€â”€â”€ visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“ˆ Creating visualizations...\")\n",
    "\n",
    "# 1. Training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes[0,0].plot(history.history['loss'], label='Train')\n",
    "axes[0,0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0,0].set_title('Loss'); axes[0,0].legend()\n",
    "\n",
    "axes[0,1].plot(history.history['accuracy'], label='Train')\n",
    "axes[0,1].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[0,1].set_title('Accuracy'); axes[0,1].legend()\n",
    "\n",
    "axes[1,0].plot(history.history['precision'], label='Train')\n",
    "axes[1,0].plot(history.history['val_precision'], label='Val')\n",
    "axes[1,0].set_title('Precision'); axes[1,0].legend()\n",
    "\n",
    "axes[1,1].plot(history.history['recall'], label='Train')\n",
    "axes[1,1].plot(history.history['val_recall'], label='Val')\n",
    "axes[1,1].set_title('Recall'); axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"training_history_{STAMP}.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 2. Confusion Matrix (Test Set)\n",
    "cm = confusion_matrix(test_res['y'], test_res['preds'])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig(OUT_DIR / f\"confusion_matrix_{STAMP}.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# â”€â”€â”€ save JSON summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "json.dump(\n",
    "    {\"params\": BEST_PARAMS,\n",
    "     \"train\": {k: v for k, v in train_res.items() if not isinstance(v, np.ndarray)},\n",
    "     \"val\": {k: v for k, v in val_res.items() if not isinstance(v, np.ndarray)},\n",
    "     \"test\": {k: v for k, v in test_res.items() if not isinstance(v, np.ndarray)}},\n",
    "    open(OUT_DIR / f\"results_{STAMP}.json\", \"w\"), indent=2\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… All artifacts saved in {OUT_DIR} ({STAMP})\")\n",
    "print(f\"   - Model: final_model_{STAMP}.keras\")\n",
    "print(f\"   - Scaler: scaler_{STAMP}.pkl\")\n",
    "print(f\"   - Results: results_{STAMP}.json\")\n",
    "print(f\"   - Plots: *_{STAMP}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"arch\": \"conv_lstm\",\n",
    "  \"window\": 66,\n",
    "  \"filters\": 32,\n",
    "  \"kernel\": 5,\n",
    "  \"units\": 32,\n",
    "  \"conv_drop\": 0.1990,\n",
    "  \"lstm_drop\": 0.0056,\n",
    "  \"dense\": 32,\n",
    "  \"dropout\": 0.3674,\n",
    "  \"l2\": 1.73e-06,\n",
    "  \"lr\": 5.455e-05,\n",
    "  \"batch\": 32,\n",
    "  \"act\": \"relu\",\n",
    "  \"pool\": \"gap\",\n",
    "  \"conv_blocks\": 2,\n",
    "  \"optim\": \"nadam\",\n",
    "  \"precision\": 0.554,\n",
    "  \"recall\": 0.501,\n",
    "  \"f_alpha\": 0.518\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"arch\": \"conv_lstm\",\n",
    "  \"window\": 72,\n",
    "  \"filters\": 96,\n",
    "  \"kernel\": 4,\n",
    "  \"units\": 32,\n",
    "  \"conv_drop\": 0.2349,\n",
    "  \"lstm_drop\": 0.0741,\n",
    "  \"dense\": 128,\n",
    "  \"dropout\": 0.3562,\n",
    "  \"l2\": 1.93e-06,\n",
    "  \"lr\": 8.557e-05,\n",
    "  \"batch\": 128,\n",
    "  \"act\": \"relu\",\n",
    "  \"pool\": \"gmp\",\n",
    "  \"conv_blocks\": 2,\n",
    "  \"optim\": \"nadam\",\n",
    "  \"precision\": 0.547,\n",
    "  \"recall\": 0.371,\n",
    "  \"f_alpha\": 0.415\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"arch\": \"lstm_conv\",\n",
    "  \"window\": 66,\n",
    "  \"filters\": 64,\n",
    "  \"kernel\": 3,\n",
    "  \"units\": 64,\n",
    "  \"conv_drop\": 0.2411,\n",
    "  \"lstm_drop\": 0.0560,\n",
    "  \"dense\": 32,\n",
    "  \"dropout\": 0.3584,\n",
    "  \"l2\": 8.995e-06,\n",
    "  \"lr\": 7.846e-05,\n",
    "  \"batch\": 128,\n",
    "  \"act\": \"elu\",\n",
    "  \"pool\": \"gmp\",\n",
    "  \"conv_blocks\": 1,\n",
    "  \"optim\": \"adamw\",\n",
    "  \"precision\": 0.533,\n",
    "  \"recall\": 0.722,\n",
    "  \"f_alpha\": 0.646\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a740ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:56:49,064] A new study created in memory with name: no-name-fe98c006-a9b0-4cfd-b9cf-8be936417ff4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting 100-trial search optimizing F0.5 (precision-focused) with threshold=0.5\n",
      "   F0.5 gives 2x weight to precision vs recall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "T00 | F0.5=0.469 P=0.512 R=0.351 F1=0.416 | lstm_conv win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.468878:   1%|          | 1/100 [00:20<33:02, 20.03s/it, 20.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:57:09,005] Trial 0 finished with value: -0.4688783051404172 and parameters: {'arch': 'lstm_conv', 'window': 48, 'filters': 96, 'kernel': 2, 'units': 32, 'conv_drop': 0.15742692948967135, 'dense': 128, 'dropout': 0.11685785941408727, 'l2': 1.2562773503807034e-05, 'lr': 0.00032354255966394373, 'batch': 64, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 0 with value: -0.4688783051404172.\n",
      "T01 | F0.5=0.237 P=0.536 R=0.073 F1=0.129 | conv_lstm win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -0.468878:   2%|â–         | 2/100 [00:36<29:40, 18.17s/it, 36.90/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:57:25,866] Trial 1 finished with value: -0.2372479240806643 and parameters: {'arch': 'conv_lstm', 'window': 42, 'filters': 48, 'kernel': 5, 'units': 64, 'conv_drop': 0.27656227050693505, 'dense': 256, 'dropout': 0.15547091587579281, 'l2': 6.516990611177181e-06, 'lr': 0.0014879578963571908, 'batch': 256, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adam'}. Best is trial 0 with value: -0.4688783051404172.\n",
      "T02 | F0.5=0.541 P=0.523 R=0.626 F1=0.570 | conv_lstm win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   3%|â–Ž         | 3/100 [00:45<22:13, 13.75s/it, 45.39/10800 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:57:34,340] Trial 2 finished with value: -0.5406397482957525 and parameters: {'arch': 'conv_lstm', 'window': 18, 'filters': 32, 'kernel': 6, 'units': 64, 'conv_drop': 0.16838315927084888, 'dense': 32, 'dropout': 0.010167650697638076, 'l2': 2.107047280657824e-06, 'lr': 5.6866415006646447e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 2 with value: -0.5406397482957525.\n",
      "WARNING:tensorflow:5 out of the last 21 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001EE08D81F30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "T03 | F0.5=0.528 P=0.527 R=0.530 F1=0.529 | lstm_conv win=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   4%|â–         | 4/100 [01:17<33:23, 20.87s/it, 77.17/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:06,116] Trial 3 finished with value: -0.527750848279205 and parameters: {'arch': 'lstm_conv', 'window': 24, 'filters': 96, 'kernel': 3, 'units': 96, 'conv_drop': 0.06663234314121907, 'dense': 128, 'dropout': 0.20751624869734644, 'l2': 0.00012854549964879015, 'lr': 0.00022159422423074867, 'batch': 64, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 2 with value: -0.5406397482957525.\n",
      "T04 | F0.5=0.541 P=0.525 R=0.614 F1=0.566 | bidirectional_lstm win=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   5%|â–Œ         | 5/100 [01:32<29:51, 18.85s/it, 92.45/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:21,386] Trial 4 finished with value: -0.5406129447063867 and parameters: {'arch': 'bidirectional_lstm', 'window': 24, 'filters': 48, 'kernel': 5, 'units': 96, 'conv_drop': 0.17726788295647253, 'dense': 32, 'dropout': 0.25806911616378, 'l2': 3.3350264835699586e-06, 'lr': 0.0008463730130999536, 'batch': 128, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   6%|â–Œ         | 6/100 [01:36<21:39, 13.82s/it, 96.50/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:25,413] Trial 5 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   7%|â–‹         | 7/100 [01:40<16:18, 10.53s/it, 100.24/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:29,147] Trial 6 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   8%|â–Š         | 8/100 [01:44<13:03,  8.52s/it, 104.47/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:33,369] Trial 7 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   9%|â–‰         | 9/100 [01:46<09:58,  6.58s/it, 106.79/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:35,685] Trial 8 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:   9%|â–‰         | 9/100 [01:56<09:58,  6.58s/it, 106.79/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:45,342] Trial 9 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  10%|â–ˆ         | 10/100 [01:56<11:18,  7.53s/it, 116.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T10 | F0.5=0.501 P=0.527 R=0.420 F1=0.467 | bidirectional_lstm win=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  10%|â–ˆ         | 10/100 [02:03<11:18,  7.53s/it, 116.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:52,530] Trial 10 finished with value: -0.5012277914199046 and parameters: {'arch': 'bidirectional_lstm', 'window': 12, 'filters': 32, 'kernel': 7, 'units': 96, 'conv_drop': 0.27501186580524273, 'dense': 32, 'dropout': 0.2412445803523247, 'l2': 1.863384361699765e-06, 'lr': 0.00021910547420266003, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  11%|â–ˆ         | 11/100 [02:09<11:01,  7.43s/it, 123.65/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:58:58,264] Trial 11 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  12%|â–ˆâ–        | 12/100 [02:09<10:08,  6.92s/it, 129.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 33 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001EE8F6A1360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "T12 | F0.5=0.524 P=0.544 R=0.458 F1=0.497 | conv_lstm win=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  12%|â–ˆâ–        | 12/100 [02:25<10:08,  6.92s/it, 129.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:59:14,224] Trial 12 finished with value: -0.5241090146750524 and parameters: {'arch': 'conv_lstm', 'window': 30, 'filters': 32, 'kernel': 6, 'units': 64, 'conv_drop': 0.20708374390244955, 'dense': 32, 'dropout': 0.015972422580406324, 'l2': 1.936668127903784e-06, 'lr': 5.575388992764191e-05, 'batch': 128, 'act': 'relu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  13%|â–ˆâ–Ž        | 13/100 [02:25<14:00,  9.66s/it, 145.35/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T13 | F0.5=0.537 P=0.524 R=0.597 F1=0.558 | conv_lstm win=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  13%|â–ˆâ–Ž        | 13/100 [02:47<14:00,  9.66s/it, 145.35/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:59:36,739] Trial 13 finished with value: -0.5367542028348532 and parameters: {'arch': 'conv_lstm', 'window': 36, 'filters': 192, 'kernel': 6, 'units': 64, 'conv_drop': 0.04031411177790156, 'dense': 32, 'dropout': 0.02713039077779361, 'l2': 5.864951245185346e-06, 'lr': 0.00010289757628832589, 'batch': 512, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  14%|â–ˆâ–        | 14/100 [02:51<19:24, 13.54s/it, 167.88/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 22:59:41,004] Trial 14 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  15%|â–ˆâ–Œ        | 15/100 [02:52<15:14, 10.76s/it, 172.18/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T15 | F0.5=0.506 P=0.546 R=0.392 F1=0.456 | bidirectional_lstm win=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  15%|â–ˆâ–Œ        | 15/100 [03:17<15:14, 10.76s/it, 172.18/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:06,860] Trial 15 finished with value: -0.5063890203502129 and parameters: {'arch': 'bidirectional_lstm', 'window': 30, 'filters': 48, 'kernel': 4, 'units': 96, 'conv_drop': 0.17554583981246016, 'dense': 32, 'dropout': 0.3903087926872786, 'l2': 2.1888684590758556e-06, 'lr': 0.0021516355196331003, 'batch': 64, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adam'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  16%|â–ˆâ–Œ        | 16/100 [03:18<21:25, 15.30s/it, 198.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T16 | F0.5=0.514 P=0.522 R=0.481 F1=0.501 | conv_lstm win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  16%|â–ˆâ–Œ        | 16/100 [03:30<21:25, 15.30s/it, 198.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:19,088] Trial 16 finished with value: -0.5135520684736091 and parameters: {'arch': 'conv_lstm', 'window': 18, 'filters': 128, 'kernel': 5, 'units': 64, 'conv_drop': 0.1450459293759074, 'dense': 256, 'dropout': 0.06537685999469826, 'l2': 4.759397026593071e-06, 'lr': 5.949110925268813e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adam'}. Best is trial 2 with value: -0.5406397482957525.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  17%|â–ˆâ–‹        | 17/100 [03:34<19:53, 14.38s/it, 210.29/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:23,207] Trial 17 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  18%|â–ˆâ–Š        | 18/100 [03:39<15:26, 11.30s/it, 214.40/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:28,141] Trial 18 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  19%|â–ˆâ–‰        | 19/100 [03:42<12:40,  9.39s/it, 219.35/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:32,009] Trial 19 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  20%|â–ˆâ–ˆ        | 20/100 [03:46<10:18,  7.73s/it, 223.23/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:35,987] Trial 20 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  21%|â–ˆâ–ˆ        | 21/100 [03:50<08:42,  6.61s/it, 227.23/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:39,591] Trial 21 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  22%|â–ˆâ–ˆâ–       | 22/100 [03:54<07:25,  5.71s/it, 230.83/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:43,523] Trial 22 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [04:04<06:38,  5.18s/it, 234.78/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:53,641] Trial 23 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  24%|â–ˆâ–ˆâ–       | 24/100 [04:08<08:26,  6.66s/it, 244.89/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:00:57,075] Trial 24 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [04:12<07:07,  5.70s/it, 248.33/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:01,576] Trial 25 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [04:15<06:35,  5.34s/it, 252.84/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:04,627] Trial 26 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:15<05:39,  4.66s/it, 255.91/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T27 | F0.5=0.543 P=0.521 R=0.652 F1=0.579 | lstm_conv win=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: -0.54064:  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:26<05:39,  4.66s/it, 255.91/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:15,099] Trial 27 finished with value: -0.5431164901664144 and parameters: {'arch': 'lstm_conv', 'window': 12, 'filters': 32, 'kernel': 5, 'units': 64, 'conv_drop': 0.20823234640471214, 'dense': 32, 'dropout': 0.11405544999534918, 'l2': 1.3624691952835903e-05, 'lr': 7.233564075664256e-05, 'batch': 256, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 27 with value: -0.5431164901664144.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: -0.543116:  28%|â–ˆâ–ˆâ–Š       | 28/100 [04:28<07:40,  6.40s/it, 266.36/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:17,761] Trial 28 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: -0.543116:  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:29<06:14,  5.28s/it, 269.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T29 | F0.5=0.545 P=0.522 R=0.658 F1=0.582 | lstm_conv win=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: -0.543116:  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:40<06:14,  5.28s/it, 269.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:29,148] Trial 29 finished with value: -0.5448588709677419 and parameters: {'arch': 'lstm_conv', 'window': 24, 'filters': 32, 'kernel': 6, 'units': 32, 'conv_drop': 0.23448642255557725, 'dense': 64, 'dropout': 0.1509254459860942, 'l2': 9.988560787806539e-06, 'lr': 0.00014920551350652237, 'batch': 256, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:44<08:18,  7.11s/it, 280.43/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:33,569] Trial 30 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:50<07:15,  6.31s/it, 284.87/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:39,306] Trial 31 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [04:54<06:57,  6.14s/it, 290.61/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:43,116] Trial 32 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [04:58<06:04,  5.44s/it, 294.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:47,693] Trial 33 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [05:06<05:42,  5.19s/it, 299.01/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:01:55,414] Trial 34 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:15<06:26,  5.95s/it, 306.74/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:04,544] Trial 35 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:20<07:22,  6.91s/it, 315.88/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:10,023] Trial 36 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:21<06:48,  6.48s/it, 321.37/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T37 | F0.5=0.512 P=0.553 R=0.393 F1=0.460 | conv_lstm win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:37<06:48,  6.48s/it, 321.37/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:26,257] Trial 37 finished with value: -0.511624229005219 and parameters: {'arch': 'conv_lstm', 'window': 18, 'filters': 32, 'kernel': 6, 'units': 32, 'conv_drop': 0.13178513909462597, 'dense': 64, 'dropout': 0.11432307897644911, 'l2': 6.771470860271624e-06, 'lr': 9.438629992235078e-05, 'batch': 256, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:43<09:43,  9.41s/it, 337.60/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:32,921] Trial 38 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [05:52<08:44,  8.60s/it, 344.32/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:41,765] Trial 39 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [06:00<08:39,  8.67s/it, 353.14/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:02:49,141] Trial 40 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [06:00<08:08,  8.28s/it, 360.53/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T41 | F0.5=0.544 P=0.528 R=0.617 F1=0.569 | lstm_conv win=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [06:14<08:08,  8.28s/it, 360.53/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:03:03,531] Trial 41 finished with value: -0.5438276896699529 and parameters: {'arch': 'lstm_conv', 'window': 24, 'filters': 32, 'kernel': 7, 'units': 32, 'conv_drop': 0.24146282971335578, 'dense': 256, 'dropout': 0.17332105839007236, 'l2': 7.206562744166613e-06, 'lr': 6.6292540136249e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [06:19<09:46, 10.11s/it, 374.91/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:03:08,464] Trial 42 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [06:19<08:08,  8.57s/it, 379.87/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T43 | F0.5=0.534 P=0.518 R=0.606 F1=0.559 | lstm_conv win=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [06:47<08:08,  8.57s/it, 379.87/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:03:36,643] Trial 43 finished with value: -0.5336988068365043 and parameters: {'arch': 'lstm_conv', 'window': 30, 'filters': 32, 'kernel': 7, 'units': 32, 'conv_drop': 0.15061818437801633, 'dense': 64, 'dropout': 0.1771118701958524, 'l2': 3.1914081716375907e-06, 'lr': 7.344815489854465e-05, 'batch': 64, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'nadam'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [06:56<13:28, 14.45s/it, 408.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:03:45,528] Trial 44 pruned. Trial was pruned at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [07:02<11:43, 12.79s/it, 416.95/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:03:51,175] Trial 45 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [07:02<09:34, 10.64s/it, 422.59/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T46 | F0.5=0.529 P=0.518 R=0.578 F1=0.546 | lstm_conv win=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [07:41<09:34, 10.64s/it, 422.59/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:04:30,397] Trial 46 finished with value: -0.5292276740806974 and parameters: {'arch': 'lstm_conv', 'window': 30, 'filters': 96, 'kernel': 7, 'units': 64, 'conv_drop': 0.06904478591006713, 'dense': 32, 'dropout': 0.13179667738739392, 'l2': 1.7034920750070704e-06, 'lr': 6.539886422089319e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [07:49<16:58, 19.22s/it, 461.81/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:04:38,113] Trial 47 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [07:49<13:40, 15.77s/it, 469.56/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T48 | F0.5=0.390 P=0.543 R=0.183 F1=0.274 | lstm_conv win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [08:05<13:40, 15.77s/it, 469.56/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:04:54,143] Trial 48 finished with value: -0.3895935801190784 and parameters: {'arch': 'lstm_conv', 'window': 18, 'filters': 32, 'kernel': 7, 'units': 32, 'conv_drop': 0.2524780518069582, 'dense': 256, 'dropout': 0.29844535583217446, 'l2': 2.991362917063724e-06, 'lr': 0.00023818924215108342, 'batch': 128, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [08:44<13:28, 15.85s/it, 485.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:05:33,580] Trial 49 pruned. Trial was pruned at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [08:54<19:06, 22.93s/it, 525.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:05:43,783] Trial 50 pruned. Trial was pruned at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [08:55<15:36, 19.11s/it, 535.24/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T51 | F0.5=0.540 P=0.528 R=0.596 F1=0.560 | conv_lstm win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [09:58<15:36, 19.11s/it, 535.24/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:06:47,352] Trial 51 finished with value: -0.5400909393368082 and parameters: {'arch': 'conv_lstm', 'window': 42, 'filters': 128, 'kernel': 7, 'units': 64, 'conv_drop': 0.056601638709444305, 'dense': 32, 'dropout': 0.04411841913701223, 'l2': 1.0025762757824672e-06, 'lr': 0.00012136238969338824, 'batch': 128, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [10:05<25:57, 32.45s/it, 598.83/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:06:54,668] Trial 52 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [10:06<19:31, 24.92s/it, 606.16/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T53 | F0.5=0.531 P=0.526 R=0.551 F1=0.538 | conv_lstm win=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [11:09<19:31, 24.92s/it, 606.16/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:07:58,713] Trial 53 finished with value: -0.5310255504533146 and parameters: {'arch': 'conv_lstm', 'window': 36, 'filters': 128, 'kernel': 7, 'units': 96, 'conv_drop': 0.04921406795362219, 'dense': 128, 'dropout': 0.042012053560857715, 'l2': 2.4289940250238154e-06, 'lr': 6.169877883697654e-05, 'batch': 128, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [11:15<28:06, 36.65s/it, 670.20/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:08:04,513] Trial 54 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [11:23<20:33, 27.41s/it, 676.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:08:12,670] Trial 55 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [11:31<15:51, 21.63s/it, 684.19/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:08:21,064] Trial 56 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [11:32<12:39, 17.66s/it, 692.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T57 | F0.5=0.455 P=0.532 R=0.287 F1=0.373 | conv_lstm win=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [14:59<12:39, 17.66s/it, 692.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:11:48,561] Trial 57 finished with value: -0.45454545454545464 and parameters: {'arch': 'conv_lstm', 'window': 60, 'filters': 48, 'kernel': 5, 'units': 192, 'conv_drop': 0.05149825568985398, 'dense': 32, 'dropout': 0.021832785725483795, 'l2': 1.8124813906409173e-06, 'lr': 0.00011260587736901104, 'batch': 128, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [15:00<52:13, 74.60s/it, 900.06/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T58 | F0.5=0.501 P=0.536 R=0.396 F1=0.455 | bidirectional_lstm win=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [16:59<52:13, 74.60s/it, 900.06/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:13:48,337] Trial 58 finished with value: -0.50078125 and parameters: {'arch': 'bidirectional_lstm', 'window': 66, 'filters': 128, 'kernel': 7, 'units': 128, 'conv_drop': 0.06576562821307408, 'dense': 32, 'dropout': 0.07618164987811468, 'l2': 6.16871648250459e-06, 'lr': 0.00010226834124857485, 'batch': 128, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [16:59<1:00:14, 88.16s/it, 1019.86/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T59 | F0.5=0.539 P=0.525 R=0.604 F1=0.562 | lstm_conv win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [17:35<1:00:14, 88.16s/it, 1019.86/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:14:24,648] Trial 59 finished with value: -0.5394032134659525 and parameters: {'arch': 'lstm_conv', 'window': 42, 'filters': 32, 'kernel': 7, 'units': 64, 'conv_drop': 0.22393613877379492, 'dense': 256, 'dropout': 0.26592686540367555, 'l2': 1.4779974800873645e-06, 'lr': 0.00015114778588915835, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 29 with value: -0.5448588709677419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [17:46<48:24, 72.61s/it, 1056.18/10800 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:14:35,182] Trial 60 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [17:54<35:05, 53.99s/it, 1066.72/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:14:43,684] Trial 61 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [18:04<25:33, 40.35s/it, 1075.25/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:14:53,520] Trial 62 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [18:12<19:14, 31.19s/it, 1085.08/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:15:01,453] Trial 63 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [18:13<14:32, 24.22s/it, 1093.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T64 | F0.5=0.568 P=0.522 R=0.872 F1=0.653 | bidirectional_lstm win=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: -0.544859:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [19:02<14:32, 24.22s/it, 1093.03/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:15:51,892] Trial 64 finished with value: -0.5676060797973401 and parameters: {'arch': 'bidirectional_lstm', 'window': 24, 'filters': 48, 'kernel': 5, 'units': 192, 'conv_drop': 0.24262300642708146, 'dense': 32, 'dropout': 0.27627883899227457, 'l2': 4.930090508405868e-06, 'lr': 0.0006775279014836466, 'batch': 128, 'act': 'elu', 'pool': 'gap', 'conv_blocks': 2, 'optim': 'adam'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [19:11<18:42, 32.08s/it, 1143.45/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:16:00,195] Trial 65 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [19:18<14:08, 24.96s/it, 1151.79/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:16:07,837] Trial 66 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [19:19<10:52, 19.77s/it, 1159.45/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T67 | F0.5=0.440 P=0.520 R=0.273 F1=0.358 | lstm_conv win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [20:15<10:52, 19.77s/it, 1159.45/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:04,559] Trial 67 finished with value: -0.4399841803440775 and parameters: {'arch': 'lstm_conv', 'window': 42, 'filters': 128, 'kernel': 4, 'units': 64, 'conv_drop': 0.016670704068756495, 'dense': 32, 'dropout': 0.0005506049463467541, 'l2': 1.7596680605459591e-06, 'lr': 0.0001433831470240445, 'batch': 128, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [20:25<16:27, 30.85s/it, 1216.15/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:14,944] Trial 68 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [20:35<12:46, 24.72s/it, 1226.57/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:25,024] Trial 69 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [20:41<10:10, 20.34s/it, 1236.68/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:30,721] Trial 70 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [21:03<07:42, 15.94s/it, 1242.35/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:52,154] Trial 71 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [21:10<08:13, 17.61s/it, 1263.88/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:17:59,154] Trial 72 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [21:28<06:29, 14.42s/it, 1270.85/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:17,912] Trial 73 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [21:38<06:48, 15.71s/it, 1289.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:27,436] Trial 74 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [21:46<05:46, 13.86s/it, 1299.12/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:35,689] Trial 75 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [21:52<04:52, 12.19s/it, 1307.39/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:41,630] Trial 76 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [22:01<03:57, 10.31s/it, 1313.33/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:50,790] Trial 77 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [22:06<03:39,  9.98s/it, 1322.53/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:18:55,369] Trial 78 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [22:14<02:55,  8.36s/it, 1327.12/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:19:03,757] Trial 79 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [22:21<02:47,  8.37s/it, 1335.49/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:19:10,196] Trial 80 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [22:30<02:28,  7.81s/it, 1342.01/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:19:19,631] Trial 81 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [22:42<02:28,  8.28s/it, 1351.37/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:19:31,822] Trial 82 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [22:43<02:40,  9.45s/it, 1363.57/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T83 | F0.5=0.512 P=0.527 R=0.461 F1=0.492 | conv_lstm win=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [23:05<02:40,  9.45s/it, 1363.57/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:19:54,743] Trial 83 finished with value: -0.5123531794248684 and parameters: {'arch': 'conv_lstm', 'window': 18, 'filters': 32, 'kernel': 5, 'units': 64, 'conv_drop': 0.20011091401589934, 'dense': 32, 'dropout': 0.05399864646673275, 'l2': 1.614662233481081e-06, 'lr': 7.505921442087517e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'nadam'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [23:06<03:36, 13.52s/it, 1386.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T84 | F0.5=0.506 P=0.537 R=0.412 F1=0.466 | conv_lstm win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [24:17<03:36, 13.52s/it, 1386.58/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:21:06,492] Trial 84 finished with value: -0.5060911415250413 and parameters: {'arch': 'conv_lstm', 'window': 42, 'filters': 192, 'kernel': 6, 'units': 64, 'conv_drop': 0.06117997676799174, 'dense': 64, 'dropout': 0.10916595030556037, 'l2': 2.828958090051302e-06, 'lr': 5.890950356716916e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [24:22<07:44, 30.98s/it, 1458.29/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:21:11,401] Trial 85 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [24:23<05:24, 23.16s/it, 1463.20/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T86 | F0.5=0.534 P=0.528 R=0.561 F1=0.544 | conv_lstm win=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [25:19<05:24, 23.16s/it, 1463.20/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:22:08,935] Trial 86 finished with value: -0.534065677223326 and parameters: {'arch': 'conv_lstm', 'window': 48, 'filters': 96, 'kernel': 6, 'units': 64, 'conv_drop': 0.002788930584955225, 'dense': 32, 'dropout': 0.005418228418254296, 'l2': 8.361407567051372e-06, 'lr': 0.00011360815094424328, 'batch': 512, 'act': 'gelu', 'pool': 'gap', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [25:31<07:15, 33.46s/it, 1520.71/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:22:20,545] Trial 87 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [25:38<05:22, 26.91s/it, 1532.33/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:22:27,944] Trial 88 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [25:46<03:51, 21.06s/it, 1539.75/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:22:35,870] Trial 89 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [25:47<02:51, 17.12s/it, 1547.68/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T90 | F0.5=0.566 P=0.521 R=0.868 F1=0.651 | lstm_conv win=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [26:28<02:51, 17.12s/it, 1547.68/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:23:17,635] Trial 90 finished with value: -0.5662111404139695 and parameters: {'arch': 'lstm_conv', 'window': 42, 'filters': 32, 'kernel': 6, 'units': 64, 'conv_drop': 0.15602720615053245, 'dense': 256, 'dropout': 0.3116265057523749, 'l2': 1.1199992631369125e-06, 'lr': 0.00012030451467114272, 'batch': 512, 'act': 'elu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [26:29<03:40, 24.51s/it, 1589.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T91 | F0.5=0.533 P=0.527 R=0.561 F1=0.544 | conv_lstm win=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [27:15<03:40, 24.51s/it, 1589.42/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:24:04,205] Trial 91 finished with value: -0.5334958783234645 and parameters: {'arch': 'conv_lstm', 'window': 36, 'filters': 96, 'kernel': 6, 'units': 64, 'conv_drop': 0.14631258988521773, 'dense': 256, 'dropout': 0.36006986474479524, 'l2': 4.166047209084094e-06, 'lr': 6.629224967054153e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 1, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [27:19<04:09, 31.14s/it, 1636.04/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:24:09,042] Trial 92 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [27:20<02:42, 23.25s/it, 1640.86/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T93 | F0.5=0.554 P=0.524 R=0.720 F1=0.606 | bidirectional_lstm win=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [27:42<02:42, 23.25s/it, 1640.86/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:24:31,669] Trial 93 finished with value: -0.553727554761681 and parameters: {'arch': 'bidirectional_lstm', 'window': 36, 'filters': 32, 'kernel': 6, 'units': 32, 'conv_drop': 0.17637983863830226, 'dense': 32, 'dropout': 0.04433292899717302, 'l2': 1.6993327077996331e-06, 'lr': 6.120127325823707e-05, 'batch': 512, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [27:48<02:18, 23.07s/it, 1663.51/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:24:37,445] Trial 94 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [27:53<01:29, 17.88s/it, 1669.30/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:24:42,646] Trial 95 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [27:54<00:56, 14.08s/it, 1674.50/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T96 | F0.5=0.535 P=0.526 R=0.571 F1=0.548 | conv_lstm win=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [28:12<00:56, 14.08s/it, 1674.50/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:25:01,591] Trial 96 finished with value: -0.5345516847515706 and parameters: {'arch': 'conv_lstm', 'window': 30, 'filters': 32, 'kernel': 5, 'units': 32, 'conv_drop': 0.18419612461224424, 'dense': 32, 'dropout': 0.09639709027776724, 'l2': 1.0699276944600666e-06, 'lr': 0.00015595963894025205, 'batch': 128, 'act': 'gelu', 'pool': 'gmp', 'conv_blocks': 2, 'optim': 'adamw'}. Best is trial 64 with value: -0.5676060797973401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [28:18<00:46, 15.54s/it, 1693.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:25:07,575] Trial 97 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [28:25<00:25, 12.68s/it, 1699.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:25:14,552] Trial 98 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [28:44<00:10, 10.98s/it, 1706.46/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 23:25:33,105] Trial 99 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 64. Best value: -0.567606: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [28:45<00:00, 17.25s/it, 1725.01/10800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ BEST RESULTS (F0.5 - Precision Focused)\n",
      "============================================================\n",
      "{\n",
      "  \"arch\": \"bidirectional_lstm\",\n",
      "  \"window\": 24,\n",
      "  \"filters\": 48,\n",
      "  \"kernel\": 5,\n",
      "  \"units\": 192,\n",
      "  \"conv_drop\": 0.24262300642708146,\n",
      "  \"dense\": 32,\n",
      "  \"dropout\": 0.27627883899227457,\n",
      "  \"l2\": 4.930090508405868e-06,\n",
      "  \"lr\": 0.0006775279014836466,\n",
      "  \"batch\": 128,\n",
      "  \"act\": \"elu\",\n",
      "  \"pool\": \"gap\",\n",
      "  \"conv_blocks\": 2,\n",
      "  \"optim\": \"adam\",\n",
      "  \"threshold\": 0.5,\n",
      "  \"beta\": 0.5,\n",
      "  \"f05_score\": 0.5676060797973401,\n",
      "  \"precision\": 0.5220240262104113,\n",
      "  \"recall\": 0.8722627737226277,\n",
      "  \"f1_score\": 0.6531541790025052,\n",
      "  \"accuracy\": 0.5160470289164284\n",
      "}\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Best F0.5 Score: 0.5676\n",
      "   Precision: 0.5220\n",
      "   Recall: 0.8723\n",
      "   F1 Score: 0.6532\n",
      "   Fixed threshold: 0.5\n",
      "ðŸ“œ Scaler saved â†’ cnn_lstm_scaler.pkl\n",
      "\n",
      "Verification:\n",
      "Our F0.5: 1.0000\n",
      "Sklearn F0.5: 1.0000\n",
      "Match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  CNNâ€“LSTM Optuna Trainer - CORRECTED F0.5 Implementation\n",
    "# =============================================================\n",
    "import os, json, gc, warnings, optuna\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score, fbeta_score)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from optuna_integration.tfkeras import TFKerasPruningCallback\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Runtime setup â”€â”€â”€â”€â”€\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.config.optimizer.set_jit(True)            # XLA on\n",
    "\n",
    "SEED, VAL_FRAC = 42, 0.20\n",
    "BETA = 0.5           # F0.5 - gives 2x weight to PRECISION\n",
    "THRESHOLD = 0.5      # Fixed threshold as requested\n",
    "N_TRIALS, TIMEOUT = 100, 3 * 60 * 60    # 3h wall limit\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "SCALER_PKL = \"cnn_lstm_scaler.pkl\"\n",
    "\n",
    "DROP_COLS = [  # Your existing drop columns\n",
    "    \"open\",\"high\",\"low\",\"typical_price\",\"EMA_21\",\"SMA_20\",\"close_4h\",\n",
    "    \"bollinger_upper\",\"bollinger_lower\",\"resistance_level\",\"support_level\",\n",
    "    \"high_low\",\"high_close\",\"low_close\",\"true_range\",\"volume_mean_20\",\n",
    "    \"MACD_line\",\"MACD_signal\",\"volatility_regime\",\"trending_market\",\n",
    "    \"above_sma50\",\"ema7_above_ema21\",\"rsi_overbought\",\"stoch_oversold\",\n",
    "    \"cci_oversold\",\"near_upper_band\",\"near_lower_band\",\"break_upper_band\",\n",
    "    \"break_lower_band\",\"rsi_oversold\",\"above_sma20\",\"macd_positive\",\n",
    "    \"volume_breakout\",\"volume_breakdown\",\"bullish_scenario_1\",\"bullish_scenario_2\",\n",
    "    \"bullish_scenario_3\",\"bullish_scenario_4\",\"bullish_scenario_5\",\"bullish_scenario_6\",\n",
    "    \"bearish_scenario_1\",\"bearish_scenario_2\",\"bearish_scenario_3\",\"bearish_scenario_4\",\n",
    "    \"bearish_scenario_6\",\"ema_cross_up\",\"macd_cross_up\",\n",
    "    \"oversold_reversal\",\"overbought_reversal\",\"close\"\n",
    "]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True).loc[\"2018-01-01\":]\n",
    "\n",
    "# Drop columns after df is defined\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "split = int(len(df) * (1 - VAL_FRAC))\n",
    "scaler = StandardScaler().fit(X_raw[:split])\n",
    "joblib.dump(scaler, SCALER_PKL)\n",
    "\n",
    "X_tr_raw = scaler.transform(X_raw[:split]);  y_tr_raw = y_raw[:split]\n",
    "X_va_raw = scaler.transform(X_raw[split:]);   y_va_raw = y_raw[split:]\n",
    "\n",
    "pos = y_tr_raw.mean()\n",
    "W0, W1 = 1.0, np.float32((1 - pos) / pos) if pos else np.float32(1.0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Window cache (same as before) â”€â”€â”€â”€â”€\n",
    "_CACHE: Dict[Tuple[int,int,int], Tuple[np.ndarray,np.ndarray]] = {}\n",
    "def make_windows(arr, lab, win):\n",
    "    k = (len(arr), win, arr.shape[1])\n",
    "    if k in _CACHE: return _CACHE[k]\n",
    "    xs, ys = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        xs.append(arr[i-win:i]); ys.append(lab[i])\n",
    "    Xw, yw = np.asarray(xs,np.float32), np.asarray(ys,np.int8)\n",
    "    if Xw.nbytes + yw.nbytes < 1e9:\n",
    "        _CACHE[k] = (Xw, yw)\n",
    "    return Xw, yw\n",
    "\n",
    "# â”€â”€â”€â”€â”€ CORRECTED F0.5 Score Function â”€â”€â”€â”€â”€\n",
    "def f_beta_score(y_true, y_pred_proba, beta=BETA, threshold=THRESHOLD):\n",
    "    \"\"\"\n",
    "    Compute F-beta score with fixed threshold.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels\n",
    "        y_pred_proba: Predicted probabilities\n",
    "        beta: F-beta parameter (0.5 for F0.5, 2.0 for F2)\n",
    "        threshold: Classification threshold (fixed at 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        F-beta score\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # F-beta formula: (1 + betaÂ²) * (precision * recall) / ((betaÂ² * precision) + recall)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return f_beta\n",
    "\n",
    "# Alternative using sklearn's fbeta_score (recommended)\n",
    "def f05_score_sklearn(y_true, y_pred_proba, threshold=THRESHOLD):\n",
    "    \"\"\"Using sklearn's fbeta_score for verification\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta=0.5, zero_division=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Model building (enhanced version) â”€â”€â”€â”€â”€\n",
    "def build(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "\n",
    "    if cfg[\"arch\"] == \"conv_lstm\":\n",
    "        x = inp\n",
    "        for _ in range(cfg[\"conv_blocks\"]):\n",
    "            x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                              activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "            x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "        x = layers.LSTM(cfg[\"units\"], dropout=0.0)(x)  # cuDNN fast\n",
    "        \n",
    "    elif cfg[\"arch\"] == \"bidirectional_lstm\":\n",
    "        x = inp\n",
    "        for _ in range(cfg[\"conv_blocks\"]):\n",
    "            x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                              activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "            x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(cfg[\"units\"]//2, dropout=0.0))(x)\n",
    "        \n",
    "    else:  # lstm_conv\n",
    "        x = layers.LSTM(cfg[\"units\"], dropout=0.0, return_sequences=True)(inp)\n",
    "        x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"same\",\n",
    "                          activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "        x = (layers.GlobalMaxPooling1D() if cfg[\"pool\"]==\"gmp\"\n",
    "             else layers.GlobalAveragePooling1D())(x)\n",
    "\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    # Weighted loss function\n",
    "    w0 = tf.constant(W0, dtype=tf.float32)\n",
    "    w1 = tf.constant(W1, dtype=tf.float32)\n",
    "    def loss(y_t, y_p):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        weights = tf.where(tf.equal(y_t, 1), w1, w0)\n",
    "        return tf.reduce_mean(weights * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    opt = (keras.optimizers.AdamW(cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "           if cfg[\"optim\"] == \"adamw\"\n",
    "           else keras.optimizers.Nadam(cfg[\"lr\"]) if cfg[\"optim\"] == \"nadam\"\n",
    "           else keras.optimizers.Adam(cfg[\"lr\"]))\n",
    "    model.compile(opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Enhanced Optuna objective â”€â”€â”€â”€â”€\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    cfg = dict(\n",
    "        arch        = trial.suggest_categorical(\"arch\", [\"conv_lstm\", \"lstm_conv\", \"bidirectional_lstm\"]),\n",
    "        window      = trial.suggest_int(\"window\", 12, 72, step=6),\n",
    "        filters     = trial.suggest_categorical(\"filters\", [32, 48, 64, 96, 128, 192]),\n",
    "        kernel      = trial.suggest_int(\"kernel\", 2, 7),\n",
    "        units       = trial.suggest_categorical(\"units\", [32, 64, 96, 128, 192, 256]),\n",
    "        conv_drop   = trial.suggest_float(\"conv_drop\", 0, .3),\n",
    "        dense       = trial.suggest_categorical(\"dense\", [32, 64, 128, 256]),\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0, .4),\n",
    "        l2          = trial.suggest_float(\"l2\", 1e-6, 1e-3, log=True),\n",
    "        lr          = trial.suggest_float(\"lr\", 5e-5, 3e-3, log=True),\n",
    "        batch       = trial.suggest_categorical(\"batch\", [64, 128, 256, 512]),\n",
    "        act         = trial.suggest_categorical(\"act\", [\"relu\", \"elu\", \"gelu\"]),\n",
    "        pool        = trial.suggest_categorical(\"pool\", [\"gmp\", \"gap\"]),\n",
    "        conv_blocks = trial.suggest_int(\"conv_blocks\", 1, 2),\n",
    "        optim       = trial.suggest_categorical(\"optim\", [\"adam\", \"adamw\", \"nadam\"]),\n",
    "    )\n",
    "    \n",
    "    Xtr, ytr = make_windows(X_tr_raw, y_tr_raw, cfg[\"window\"])\n",
    "    Xva, yva = make_windows(X_va_raw, y_va_raw, cfg[\"window\"])\n",
    "    if len(Xtr) < cfg[\"batch\"] * 4:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    ds_tr = (tf.data.Dataset.from_tensor_slices((Xtr, ytr))\n",
    "             .shuffle(10_000).batch(cfg[\"batch\"]).prefetch(tf.data.AUTOTUNE))\n",
    "    ds_va = (tf.data.Dataset.from_tensor_slices((Xva, yva))\n",
    "             .batch(cfg[\"batch\"]).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    model = build(cfg)\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0),\n",
    "          keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, min_lr=1e-6, verbose=0),\n",
    "          TFKerasPruningCallback(trial, \"val_loss\")]\n",
    "    \n",
    "    model.fit(ds_tr, validation_data=ds_va, epochs=100, callbacks=cb, verbose=0)\n",
    "\n",
    "    # Get predictions\n",
    "    prob = model.predict(ds_va, verbose=0).ravel()\n",
    "    y_hat = (prob >= THRESHOLD).astype(int)\n",
    "    \n",
    "    # Calculate F0.5 score (emphasizes precision)\n",
    "    f05_score = f_beta_score(yva, prob, beta=0.5, threshold=THRESHOLD)\n",
    "    \n",
    "    # Also calculate individual metrics for logging\n",
    "    precision = precision_score(yva, y_hat, zero_division=0)\n",
    "    recall = recall_score(yva, y_hat, zero_division=0)\n",
    "    f1 = f1_score(yva, y_hat, zero_division=0)\n",
    "    accuracy = accuracy_score(yva, y_hat)\n",
    "    \n",
    "    # Store metrics as user attributes\n",
    "    trial.set_user_attr(\"f05_score\", f05_score)\n",
    "    trial.set_user_attr(\"precision\", precision)\n",
    "    trial.set_user_attr(\"recall\", recall)\n",
    "    trial.set_user_attr(\"f1_score\", f1)\n",
    "    trial.set_user_attr(\"accuracy\", accuracy)\n",
    "\n",
    "    print(f\"T{trial.number:02d} | F0.5={f05_score:.3f} P={precision:.3f} R={recall:.3f} \"\n",
    "          f\"F1={f1:.3f} | {cfg['arch']} win={cfg['window']}\")\n",
    "\n",
    "    del model; tf.keras.backend.clear_session(); gc.collect()\n",
    "    \n",
    "    # Return negative F0.5 score (since Optuna minimizes)\n",
    "    return -f05_score\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Search execution â”€â”€â”€â”€â”€\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED, multivariate=True),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5))\n",
    "\n",
    "print(f\"ðŸš€ Starting {N_TRIALS}-trial search optimizing F0.5 (precision-focused) with threshold={THRESHOLD}\")\n",
    "print(f\"   F0.5 gives 2x weight to precision vs recall\")\n",
    "study.optimize(objective, n_trials=N_TRIALS,\n",
    "               timeout=TIMEOUT, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Results â”€â”€â”€â”€â”€\n",
    "best = study.best_trial\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "results = {\n",
    "    **best.params,\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"beta\": BETA,\n",
    "    \"f05_score\": best.user_attrs[\"f05_score\"],\n",
    "    \"precision\": best.user_attrs[\"precision\"],\n",
    "    \"recall\": best.user_attrs[\"recall\"],\n",
    "    \"f1_score\": best.user_attrs[\"f1_score\"],\n",
    "    \"accuracy\": best.user_attrs[\"accuracy\"]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ BEST RESULTS (F0.5 - Precision Focused)\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(f\"best_params_f05_cnn_lstm_{ts}.json\", \"w\"), indent=2)\n",
    "study.trials_dataframe().to_csv(f\"trials_f05_cnn_lstm_{ts}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   Best F0.5 Score: {best.user_attrs['f05_score']:.4f}\")\n",
    "print(f\"   Precision: {best.user_attrs['precision']:.4f}\")\n",
    "print(f\"   Recall: {best.user_attrs['recall']:.4f}\")\n",
    "print(f\"   F1 Score: {best.user_attrs['f1_score']:.4f}\")\n",
    "print(f\"   Fixed threshold: {THRESHOLD}\")\n",
    "print(f\"ðŸ“œ Scaler saved â†’ {SCALER_PKL}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Verification function â”€â”€â”€â”€â”€\n",
    "def verify_f05_calculation():\n",
    "    \"\"\"Verify our F0.5 calculation is correct\"\"\"\n",
    "    # Test with known values\n",
    "    y_true = np.array([1, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "    y_prob = np.array([0.8, 0.6, 0.3, 0.7, 0.2, 0.9, 0.1, 0.4, 0.65, 0.85])\n",
    "    \n",
    "    # Our implementation\n",
    "    f05_ours = f_beta_score(y_true, y_prob, beta=0.5, threshold=0.5)\n",
    "    \n",
    "    # Sklearn implementation\n",
    "    f05_sklearn = f05_score_sklearn(y_true, y_prob, threshold=0.5)\n",
    "    \n",
    "    print(f\"\\nVerification:\")\n",
    "    print(f\"Our F0.5: {f05_ours:.4f}\")\n",
    "    print(f\"Sklearn F0.5: {f05_sklearn:.4f}\")\n",
    "    print(f\"Match: {abs(f05_ours - f05_sklearn) < 1e-6}\")\n",
    "\n",
    "# Run verification\n",
    "verify_f05_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9851bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No GPU found, using CPU\n",
      "ðŸ“Š Loading data...\n",
      "ðŸ“‹ Using 29 features: ['volume', 'EMA_7', 'SMA_50', 'RSI', 'MACD_histogram']... (+24 more)\n",
      "ðŸ“Š Data shape: (15855, 30)\n",
      "ðŸ”€ Train: 11098, Val: 2378, Test: 2379\n",
      "âš–ï¸ Positive rate: 0.512, Weights: W0=1.00, W1=0.95\n",
      "ðŸªŸ Window size: 66, Train windows: 11032\n",
      "\n",
      "ðŸ—ï¸ Building conv_lstm model...\n",
      "ðŸ”§ Architecture: conv_lstm\n",
      "ðŸªŸ Window: 66\n",
      "ðŸ”¢ Filters: 32, Units: 32\n",
      "ðŸŽ›ï¸ Optimizer: nadam, LR: 5.45e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,152</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m29\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚         \u001b[38;5;34m4,672\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚         \u001b[38;5;34m5,152\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m8,320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m1,056\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,233</span> (75.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,233\u001b[0m (75.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,233</span> (75.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,233\u001b[0m (75.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting training with batch size 32...\n",
      "Epoch 1/200\n",
      "\u001b[1m340/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4958 - auc: 0.4857 - loss: 0.6935 - precision: 0.5000 - recall: 0.8129\n",
      "Epoch 1: val_loss improved from inf to 0.67962, saving model to model_outputs\\best_model_20250613_233908.keras\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.4958 - auc: 0.4857 - loss: 0.6934 - precision: 0.5001 - recall: 0.8113 - val_accuracy: 0.4883 - val_auc: 0.5008 - val_loss: 0.6796 - val_precision: 0.4903 - val_recall: 0.9553 - learning_rate: 5.4550e-05\n",
      "Epoch 2/200\n",
      "\u001b[1m342/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5064 - auc: 0.5122 - loss: 0.6781 - precision: 0.5122 - recall: 0.6379\n",
      "Epoch 2: val_loss improved from 0.67962 to 0.67882, saving model to model_outputs\\best_model_20250613_233908.keras\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5063 - auc: 0.5121 - loss: 0.6781 - precision: 0.5122 - recall: 0.6377 - val_accuracy: 0.4918 - val_auc: 0.4988 - val_loss: 0.6788 - val_precision: 0.4917 - val_recall: 0.9070 - learning_rate: 5.4550e-05\n",
      "Epoch 3/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5071 - auc: 0.5035 - loss: 0.6782 - precision: 0.5214 - recall: 0.6536\n",
      "Epoch 3: val_loss improved from 0.67882 to 0.67821, saving model to model_outputs\\best_model_20250613_233908.keras\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5071 - auc: 0.5035 - loss: 0.6782 - precision: 0.5214 - recall: 0.6535 - val_accuracy: 0.4948 - val_auc: 0.4984 - val_loss: 0.6782 - val_precision: 0.4925 - val_recall: 0.8018 - learning_rate: 5.4550e-05\n",
      "Epoch 4/200\n",
      "\u001b[1m342/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5003 - auc: 0.4923 - loss: 0.6797 - precision: 0.5060 - recall: 0.5510\n",
      "Epoch 4: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5004 - auc: 0.4924 - loss: 0.6797 - precision: 0.5061 - recall: 0.5515 - val_accuracy: 0.4888 - val_auc: 0.4990 - val_loss: 0.6786 - val_precision: 0.4897 - val_recall: 0.8746 - learning_rate: 5.4550e-05\n",
      "Epoch 5/200\n",
      "\u001b[1m343/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5067 - auc: 0.5133 - loss: 0.6769 - precision: 0.5111 - recall: 0.6084\n",
      "Epoch 5: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5067 - auc: 0.5133 - loss: 0.6769 - precision: 0.5112 - recall: 0.6086 - val_accuracy: 0.4892 - val_auc: 0.4926 - val_loss: 0.6788 - val_precision: 0.4899 - val_recall: 0.8711 - learning_rate: 5.4550e-05\n",
      "Epoch 6/200\n",
      "\u001b[1m344/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5171 - auc: 0.5167 - loss: 0.6763 - precision: 0.5250 - recall: 0.6701\n",
      "Epoch 6: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5171 - auc: 0.5167 - loss: 0.6763 - precision: 0.5250 - recall: 0.6701 - val_accuracy: 0.4879 - val_auc: 0.4928 - val_loss: 0.6787 - val_precision: 0.4892 - val_recall: 0.8763 - learning_rate: 5.4550e-05\n",
      "Epoch 7/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5210 - auc: 0.5207 - loss: 0.6755 - precision: 0.5286 - recall: 0.6771\n",
      "Epoch 7: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5210 - auc: 0.5207 - loss: 0.6755 - precision: 0.5285 - recall: 0.6770 - val_accuracy: 0.4879 - val_auc: 0.4997 - val_loss: 0.6783 - val_precision: 0.4886 - val_recall: 0.8254 - learning_rate: 5.4550e-05\n",
      "Epoch 8/200\n",
      "\u001b[1m342/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5181 - auc: 0.5141 - loss: 0.6764 - precision: 0.5234 - recall: 0.6586\n",
      "Epoch 8: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5180 - auc: 0.5142 - loss: 0.6764 - precision: 0.5234 - recall: 0.6584 - val_accuracy: 0.4827 - val_auc: 0.4985 - val_loss: 0.6784 - val_precision: 0.4851 - val_recall: 0.7974 - learning_rate: 5.4550e-05\n",
      "Epoch 9/200\n",
      "\u001b[1m344/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5190 - auc: 0.5177 - loss: 0.6752 - precision: 0.5306 - recall: 0.6997\n",
      "Epoch 9: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5190 - auc: 0.5177 - loss: 0.6752 - precision: 0.5305 - recall: 0.6997 - val_accuracy: 0.4844 - val_auc: 0.4940 - val_loss: 0.6782 - val_precision: 0.4841 - val_recall: 0.6947 - learning_rate: 5.4550e-05\n",
      "Epoch 10/200\n",
      "\u001b[1m343/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5107 - auc: 0.5140 - loss: 0.6766 - precision: 0.5118 - recall: 0.5416\n",
      "Epoch 10: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5108 - auc: 0.5141 - loss: 0.6766 - precision: 0.5119 - recall: 0.5419 - val_accuracy: 0.4875 - val_auc: 0.4960 - val_loss: 0.6788 - val_precision: 0.4890 - val_recall: 0.8772 - learning_rate: 5.4550e-05\n",
      "Epoch 11/200\n",
      "\u001b[1m340/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5165 - auc: 0.5168 - loss: 0.6764 - precision: 0.5196 - recall: 0.6818\n",
      "Epoch 11: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5164 - auc: 0.5168 - loss: 0.6764 - precision: 0.5196 - recall: 0.6815 - val_accuracy: 0.4853 - val_auc: 0.4959 - val_loss: 0.6788 - val_precision: 0.4870 - val_recall: 0.8193 - learning_rate: 5.4550e-05\n",
      "Epoch 12/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5169 - auc: 0.5158 - loss: 0.6754 - precision: 0.5261 - recall: 0.6771\n",
      "Epoch 12: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5169 - auc: 0.5158 - loss: 0.6754 - precision: 0.5261 - recall: 0.6771 - val_accuracy: 0.4957 - val_auc: 0.4961 - val_loss: 0.6782 - val_precision: 0.4916 - val_recall: 0.6711 - learning_rate: 5.4550e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5230 - auc: 0.5284 - loss: 0.6749 - precision: 0.5338 - recall: 0.5986\n",
      "Epoch 13: val_loss did not improve from 0.67821\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.7275000320514664e-05.\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5230 - auc: 0.5283 - loss: 0.6749 - precision: 0.5338 - recall: 0.5986 - val_accuracy: 0.4935 - val_auc: 0.4973 - val_loss: 0.6785 - val_precision: 0.4904 - val_recall: 0.6947 - learning_rate: 5.4550e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m343/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5201 - auc: 0.5303 - loss: 0.6742 - precision: 0.5303 - recall: 0.6501\n",
      "Epoch 14: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5201 - auc: 0.5304 - loss: 0.6742 - precision: 0.5303 - recall: 0.6502 - val_accuracy: 0.4892 - val_auc: 0.4970 - val_loss: 0.6785 - val_precision: 0.4874 - val_recall: 0.6982 - learning_rate: 2.7275e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m344/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5315 - auc: 0.5371 - loss: 0.6740 - precision: 0.5397 - recall: 0.6731\n",
      "Epoch 15: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5315 - auc: 0.5371 - loss: 0.6740 - precision: 0.5396 - recall: 0.6731 - val_accuracy: 0.4913 - val_auc: 0.4989 - val_loss: 0.6784 - val_precision: 0.4883 - val_recall: 0.6614 - learning_rate: 2.7275e-05\n",
      "Epoch 16/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5212 - auc: 0.5257 - loss: 0.6747 - precision: 0.5293 - recall: 0.6379\n",
      "Epoch 16: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5212 - auc: 0.5257 - loss: 0.6747 - precision: 0.5293 - recall: 0.6379 - val_accuracy: 0.5009 - val_auc: 0.5002 - val_loss: 0.6784 - val_precision: 0.4956 - val_recall: 0.6974 - learning_rate: 2.7275e-05\n",
      "Epoch 17/200\n",
      "\u001b[1m344/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5294 - auc: 0.5389 - loss: 0.6737 - precision: 0.5321 - recall: 0.6567\n",
      "Epoch 17: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5294 - auc: 0.5389 - loss: 0.6737 - precision: 0.5321 - recall: 0.6567 - val_accuracy: 0.4987 - val_auc: 0.5014 - val_loss: 0.6783 - val_precision: 0.4942 - val_recall: 0.7149 - learning_rate: 2.7275e-05\n",
      "Epoch 18/200\n",
      "\u001b[1m344/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5197 - auc: 0.5263 - loss: 0.6749 - precision: 0.5257 - recall: 0.6652\n",
      "Epoch 18: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5197 - auc: 0.5263 - loss: 0.6749 - precision: 0.5257 - recall: 0.6653 - val_accuracy: 0.4978 - val_auc: 0.5027 - val_loss: 0.6784 - val_precision: 0.4937 - val_recall: 0.7184 - learning_rate: 2.7275e-05\n",
      "Epoch 19/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5276 - auc: 0.5362 - loss: 0.6746 - precision: 0.5290 - recall: 0.6486\n",
      "Epoch 19: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5277 - auc: 0.5362 - loss: 0.6746 - precision: 0.5290 - recall: 0.6486 - val_accuracy: 0.5013 - val_auc: 0.5006 - val_loss: 0.6784 - val_precision: 0.4959 - val_recall: 0.6851 - learning_rate: 2.7275e-05\n",
      "Epoch 20/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5254 - auc: 0.5343 - loss: 0.6743 - precision: 0.5334 - recall: 0.6534\n",
      "Epoch 20: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5254 - auc: 0.5343 - loss: 0.6743 - precision: 0.5334 - recall: 0.6534 - val_accuracy: 0.4970 - val_auc: 0.5035 - val_loss: 0.6782 - val_precision: 0.4927 - val_recall: 0.6763 - learning_rate: 2.7275e-05\n",
      "Epoch 21/200\n",
      "\u001b[1m343/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5240 - auc: 0.5349 - loss: 0.6749 - precision: 0.5231 - recall: 0.6498\n",
      "Epoch 21: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5241 - auc: 0.5350 - loss: 0.6749 - precision: 0.5232 - recall: 0.6500 - val_accuracy: 0.4913 - val_auc: 0.5041 - val_loss: 0.6784 - val_precision: 0.4889 - val_recall: 0.6939 - learning_rate: 2.7275e-05\n",
      "Epoch 22/200\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5356 - auc: 0.5414 - loss: 0.6731 - precision: 0.5411 - recall: 0.6741\n",
      "Epoch 22: val_loss did not improve from 0.67821\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5356 - auc: 0.5414 - loss: 0.6731 - precision: 0.5411 - recall: 0.6741 - val_accuracy: 0.4935 - val_auc: 0.5023 - val_loss: 0.6784 - val_precision: 0.4902 - val_recall: 0.6798 - learning_rate: 2.7275e-05\n",
      "Epoch 23/200\n",
      "\u001b[1m343/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5305 - auc: 0.5416 - loss: 0.6739 - precision: 0.5341 - recall: 0.6690\n",
      "Epoch 23: val_loss did not improve from 0.67821\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.3637500160257332e-05.\n",
      "\u001b[1m345/345\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5304 - auc: 0.5416 - loss: 0.6739 - precision: 0.5341 - recall: 0.6690 - val_accuracy: 0.4935 - val_auc: 0.5010 - val_loss: 0.6784 - val_precision: 0.4904 - val_recall: 0.6956 - learning_rate: 2.7275e-05\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "ðŸ“Š Evaluation Results (threshold=0.5)\n",
      "================================================================================\n",
      "Train       P=0.522 R=0.657 F1=0.582 F0.5=0.545 ACC=0.517 AUC=0.521\n",
      "Validation  P=0.492 R=0.802 F1=0.610 F0.5=0.534 ACC=0.495 AUC=0.499\n",
      "Test        P=0.531 R=0.287 F1=0.372 F0.5=0.453 ACC=0.497 AUC=0.502\n",
      "\n",
      "ðŸ“ˆ Creating enhanced visualizations...\n",
      "\n",
      "ðŸ“‹ Detailed Classification Report (Test Set)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Signal       0.48      0.72      0.58      1109\n",
      "      Signal       0.53      0.29      0.37      1204\n",
      "\n",
      "    accuracy                           0.50      2313\n",
      "   macro avg       0.51      0.51      0.48      2313\n",
      "weighted avg       0.51      0.50      0.47      2313\n",
      "\n",
      "\n",
      "ðŸ“„ Creating prediction CSV outputs...\n",
      "   ðŸ“Š Processing train set...\n",
      "   âœ… Saved train predictions: model_outputs\\predictions_train_20250613_233908.csv\n",
      "   ðŸ“Š Processing validation set...\n",
      "   âœ… Saved validation predictions: model_outputs\\predictions_validation_20250613_233908.csv\n",
      "   ðŸ“Š Processing test set...\n",
      "   âœ… Saved test predictions: model_outputs\\predictions_test_20250613_233908.csv\n",
      "\n",
      "ðŸ“ˆ Prediction Summary Statistics:\n",
      "============================================================\n",
      "Train       :  11032 samples | ACC=0.517 | P=0.522 | R=0.657\n",
      "Validation  :   2312 samples | ACC=0.495 | P=0.492 | R=0.802\n",
      "Test        :   2313 samples | ACC=0.497 | P=0.531 | R=0.287\n",
      "\n",
      "ðŸ“‹ Sample predictions (first 10 test samples):\n",
      "================================================================================\n",
      "timestamp        prob_up  prob_down winning_prob prediction actual\n",
      "----------------------------------------------------------------------\n",
      "2024-03-07 16:00:00 0.500344 0.499656 0.500344          1      1\n",
      "2024-03-07 20:00:00 0.494516 0.505484 0.505484          0      0\n",
      "2024-03-08 00:00:00 0.498301 0.501699 0.501699          0      0\n",
      "2024-03-08 04:00:00 0.498510 0.501490 0.501490          0      1\n",
      "2024-03-08 08:00:00 0.494966 0.505034 0.505034          0      1\n",
      "2024-03-08 12:00:00 0.493404 0.506596 0.506596          0      1\n",
      "2024-03-08 16:00:00 0.495583 0.504417 0.504417          0      1\n",
      "2024-03-08 20:00:00 0.496534 0.503466 0.503466          0      0\n",
      "2024-03-09 00:00:00 0.492700 0.507300 0.507300          0      1\n",
      "2024-03-09 04:00:00 0.488642 0.511358 0.511358          0      1\n",
      "\n",
      "âœ… Training completed successfully!\n",
      "ðŸŽ¯ Best Test F0.5 Score: 0.453\n",
      "ðŸŽ¯ Test Precision: 0.531\n",
      "ðŸŽ¯ Test Recall: 0.287\n",
      "\n",
      "ðŸ“ All artifacts saved in model_outputs with timestamp 20250613_233908:\n",
      "   ðŸ“¦ Model: final_model_20250613_233908.keras\n",
      "   ðŸ“¦ Best checkpoint: best_model_20250613_233908.keras\n",
      "   ðŸ“¦ Scaler: scaler_20250613_233908.pkl\n",
      "   ðŸ“Š Results: results_20250613_233908.json\n",
      "   ðŸ“ˆ Plots: *_20250613_233908.png\n",
      "   ðŸ“„ Predictions: predictions_*_20250613_233908.csv\n",
      "\n",
      "ðŸ”® Expected performance based on Optuna results:\n",
      "   Expected Precision: 0.554\n",
      "   Expected Recall: 0.501\n",
      "   Expected F-alpha: 0.518\n",
      "\n",
      "ðŸ“„ Prediction files created:\n",
      "   ðŸ”— Combined: model_outputs\\predictions_combined_20250613_233908.csv\n",
      "   ðŸ“Š Individual files for train/validation/test sets\n",
      "   ðŸ“ˆ Total predictions: 15,657\n",
      "\n",
      "ðŸ’¹ Trading Signal Analysis (Test Set):\n",
      "   ðŸ“ˆ High confidence signals (>60%): 0\n",
      "   ðŸ“ˆ High confidence BUY signals: 0\n",
      "   ðŸ“ˆ High confidence BUY accuracy: 0.000\n",
      "   ðŸš€ Very high confidence signals (>70%): 0\n",
      "   ðŸš€ Very high confidence BUY signals: 0\n",
      "   ðŸš€ Very high confidence BUY accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, gc, warnings, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, roc_curve, auc,\n",
    "                             precision_recall_curve, fbeta_score)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping,\n",
    "                                        ReduceLROnPlateau)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# â”€â”€â”€ BEST PARAMETERS FROM OPTUNA OPTIMIZATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BEST_PARAMS = {\n",
    "    \"arch\": \"conv_lstm\",\n",
    "    \"window\": 66,\n",
    "    \"filters\": 32,\n",
    "    \"kernel\": 5,\n",
    "    \"units\": 32,\n",
    "    \"conv_drop\": 0.1990,\n",
    "    \"lstm_drop\": 0.0056,\n",
    "    \"dense\": 32,\n",
    "    \"dropout\": 0.3674,\n",
    "    \"l2\": 1.73e-06,\n",
    "    \"lr\": 5.455e-05,\n",
    "    \"batch\": 32,\n",
    "    \"act\": \"relu\",\n",
    "    \"pool\": \"gap\",\n",
    "    \"conv_blocks\": 2,\n",
    "    \"optim\": \"nadam\",\n",
    "    \"precision\": 0.554,\n",
    "    \"recall\": 0.501,\n",
    "    \"f_alpha\": 0.518\n",
    "}\n",
    "\n",
    "# â”€â”€â”€ paths & config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "# FIXED: Removed extra bracket\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_21', 'SMA_20',\n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower',\n",
    "    'resistance_level', 'support_level', 'high_low', 'high_close', 'low_close',\n",
    "    'true_range', 'volume_mean_20', 'MACD_line', 'MACD_signal',\n",
    "    'volatility_regime', 'trending_market', 'above_sma50', 'ema7_above_ema21',\n",
    "    'rsi_overbought', 'stoch_oversold', 'cci_oversold',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'break_upper_band', 'break_lower_band', 'rsi_oversold',\n",
    "    'above_sma20', 'macd_positive', 'volume_breakout', 'volume_breakdown',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6', 'ema_cross_up', 'macd_cross_up', \n",
    "    'oversold_reversal', 'overbought_reversal', 'close'\n",
    "]\n",
    "\n",
    "SEED, VAL_FRAC, TEST_FRAC = 42, .15, .15\n",
    "BETA = 0.5  # F0.5 score (precision-focused)\n",
    "THRESHOLD = 0.5  # Fixed threshold\n",
    "EPOCHS, PATIENCE = 200, 20\n",
    "\n",
    "OUT_DIR = Path(\"model_outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# GPU setup with feedback\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for g in gpus:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    print(f\"âœ… Found {len(gpus)} GPU(s)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU found, using CPU\")\n",
    "\n",
    "# â”€â”€â”€ data load & split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Loading data...\")\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors='ignore')\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "# Store feature names before dropping target\n",
    "feat_names = [col for col in df.columns if col != \"target\"]\n",
    "print(f\"ðŸ“‹ Using {len(feat_names)} features: {feat_names[:5]}... (+{len(feat_names)-5} more)\")\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(np.int8).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "n = len(df)\n",
    "val_start = int(n * (1 - VAL_FRAC - TEST_FRAC))\n",
    "test_start = int(n * (1 - TEST_FRAC))\n",
    "train_idx = np.arange(0, val_start)\n",
    "val_idx = np.arange(val_start, test_start)\n",
    "test_idx = np.arange(test_start, n)\n",
    "\n",
    "scaler = StandardScaler().fit(X_raw[train_idx])\n",
    "X_train = scaler.transform(X_raw[train_idx])\n",
    "y_train = y_raw[train_idx]\n",
    "X_val = scaler.transform(X_raw[val_idx])\n",
    "y_val = y_raw[val_idx]\n",
    "X_test = scaler.transform(X_raw[test_idx])\n",
    "y_test = y_raw[test_idx]\n",
    "\n",
    "joblib.dump(scaler, OUT_DIR / f\"scaler_{STAMP}.pkl\")\n",
    "\n",
    "pos = y_train.mean()\n",
    "W0, W1 = 1.0, (1-pos)/pos if pos else 1.0\n",
    "\n",
    "print(f\"ðŸ“Š Data shape: {df.shape}\")\n",
    "print(f\"ðŸ”€ Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "print(f\"âš–ï¸ Positive rate: {pos:.3f}, Weights: W0={W0:.2f}, W1={W1:.2f}\")\n",
    "\n",
    "# â”€â”€â”€ window helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def windows(X, y, w):\n",
    "    xs, ys = [], []\n",
    "    for i in range(w, len(X)):\n",
    "        xs.append(X[i-w:i])\n",
    "        ys.append(y[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "win = BEST_PARAMS[\"window\"]\n",
    "X_tr_w, y_tr_w = windows(X_train, y_train, win)\n",
    "X_va_w, y_va_w = windows(X_val, y_val, win)\n",
    "X_te_w, y_te_w = windows(X_test, y_test, win)\n",
    "\n",
    "print(f\"ðŸªŸ Window size: {win}, Train windows: {len(X_tr_w)}\")\n",
    "\n",
    "# â”€â”€â”€ F0.5 score calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f05_score(y_true, y_pred_proba, threshold=THRESHOLD):\n",
    "    \"\"\"Calculate F0.5 score (precision-focused)\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta=0.5, zero_division=0)\n",
    "\n",
    "def f_beta_score_custom(y_true, y_pred_proba, beta=BETA, threshold=THRESHOLD):\n",
    "    \"\"\"Custom F-beta score calculation\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return f_beta\n",
    "\n",
    "# â”€â”€â”€ FIXED MODEL BUILDER - Supports all architectures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(cfg):\n",
    "    \"\"\"Build model based on architecture specified in config\"\"\"\n",
    "    l2_reg = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "    \n",
    "    if cfg[\"arch\"] == \"conv_lstm\":\n",
    "        # Conv1D layers first, then LSTM\n",
    "        x = inp\n",
    "        for _ in range(cfg[\"conv_blocks\"]):\n",
    "            x = layers.Conv1D(\n",
    "                cfg[\"filters\"], cfg[\"kernel\"], \n",
    "                padding=\"causal\", activation=cfg[\"act\"],\n",
    "                kernel_regularizer=l2_reg\n",
    "            )(x)\n",
    "            x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "        \n",
    "        # LSTM layer (no return_sequences for final output)\n",
    "        x = layers.LSTM(\n",
    "            cfg[\"units\"], \n",
    "            dropout=cfg[\"lstm_drop\"],\n",
    "            kernel_regularizer=l2_reg\n",
    "        )(x)\n",
    "        \n",
    "    elif cfg[\"arch\"] == \"lstm_conv\":\n",
    "        # LSTM first, then Conv1D\n",
    "        x = layers.LSTM(\n",
    "            cfg[\"units\"], \n",
    "            dropout=cfg[\"lstm_drop\"],\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2_reg\n",
    "        )(inp)\n",
    "        \n",
    "        x = layers.Conv1D(\n",
    "            cfg[\"filters\"], cfg[\"kernel\"], \n",
    "            padding=\"same\", activation=cfg[\"act\"],\n",
    "            kernel_regularizer=l2_reg\n",
    "        )(x)\n",
    "        \n",
    "        # Pooling layer\n",
    "        if cfg[\"pool\"] == \"gmp\":\n",
    "            x = layers.GlobalMaxPooling1D()(x)\n",
    "        else:  # gap\n",
    "            x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    elif cfg[\"arch\"] == \"bidirectional_lstm\":\n",
    "        # Conv1D layers first\n",
    "        x = inp\n",
    "        for _ in range(cfg[\"conv_blocks\"]):\n",
    "            x = layers.Conv1D(\n",
    "                cfg[\"filters\"], cfg[\"kernel\"], \n",
    "                padding=\"causal\", activation=cfg[\"act\"],\n",
    "                kernel_regularizer=l2_reg\n",
    "            )(x)\n",
    "            x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        x = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                cfg[\"units\"]//2,  # Divide by 2 for bidirectional\n",
    "                dropout=cfg[\"lstm_drop\"],\n",
    "                kernel_regularizer=l2_reg\n",
    "            )\n",
    "        )(x)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {cfg['arch']}\")\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(\n",
    "        cfg[\"dense\"], \n",
    "        activation=cfg[\"act\"],\n",
    "        kernel_regularizer=l2_reg\n",
    "    )(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    \n",
    "    # Output layer\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    \n",
    "    # Weighted loss function\n",
    "    w0 = tf.constant(W0, dtype=tf.float32)\n",
    "    w1 = tf.constant(W1, dtype=tf.float32)\n",
    "    \n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        weights = tf.where(tf.equal(y_true, 1), w1, w0)\n",
    "        return tf.reduce_mean(weights * keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    \n",
    "    # FIXED: Optimizer selection based on config\n",
    "    if cfg[\"optim\"] == \"adamw\":\n",
    "        optimizer = keras.optimizers.AdamW(cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "    elif cfg[\"optim\"] == \"nadam\":\n",
    "        optimizer = keras.optimizers.Nadam(cfg[\"lr\"])\n",
    "    elif cfg[\"optim\"] == \"rmsprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(cfg[\"lr\"])\n",
    "    else:  # adam\n",
    "        optimizer = keras.optimizers.Adam(cfg[\"lr\"])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=weighted_binary_crossentropy,\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(f\"\\nðŸ—ï¸ Building {BEST_PARAMS['arch']} model...\")\n",
    "print(f\"ðŸ”§ Architecture: {BEST_PARAMS['arch']}\")\n",
    "print(f\"ðŸªŸ Window: {BEST_PARAMS['window']}\")\n",
    "print(f\"ðŸ”¢ Filters: {BEST_PARAMS['filters']}, Units: {BEST_PARAMS['units']}\")\n",
    "print(f\"ðŸŽ›ï¸ Optimizer: {BEST_PARAMS['optim']}, LR: {BEST_PARAMS['lr']:.2e}\")\n",
    "\n",
    "model = build_model(BEST_PARAMS)\n",
    "model.summary()\n",
    "\n",
    "# â”€â”€â”€ callbacks & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nðŸš€ Starting training with batch size {BEST_PARAMS['batch']}...\")\n",
    "ckpt_path = OUT_DIR / f\"best_model_{STAMP}.keras\"\n",
    "\n",
    "# Create datasets for better performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_tr_w, y_tr_w))\n",
    "train_dataset = train_dataset.shuffle(10000).batch(BEST_PARAMS[\"batch\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_va_w, y_va_w))\n",
    "val_dataset = val_dataset.batch(BEST_PARAMS[\"batch\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(patience=10, factor=.5, min_lr=1e-7, verbose=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(OUT_DIR / f\"final_model_{STAMP}.keras\")\n",
    "\n",
    "# â”€â”€â”€ ENHANCED evaluation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_comprehensive(X, y, name):\n",
    "    \"\"\"Comprehensive evaluation with multiple metrics\"\"\"\n",
    "    proba = model.predict(X, verbose=0).ravel()\n",
    "    preds = (proba >= THRESHOLD).astype(int)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    precision = precision_score(y, preds, zero_division=0)\n",
    "    recall = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "    f05 = f05_score(y, proba, threshold=THRESHOLD)\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y, proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    print(f\"{name:<11} P={precision:.3f} R={recall:.3f} F1={f1:.3f} F0.5={f05:.3f} ACC={accuracy:.3f} AUC={roc_auc:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f05': f05,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'proba': proba,\n",
    "        'preds': preds,\n",
    "        'y': y\n",
    "    }\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluation Results (threshold={THRESHOLD})\")\n",
    "print(\"=\"*80)\n",
    "train_res = evaluate_comprehensive(X_tr_w, y_tr_w, \"Train\")\n",
    "val_res = evaluate_comprehensive(X_va_w, y_va_w, \"Validation\")\n",
    "test_res = evaluate_comprehensive(X_te_w, y_te_w, \"Test\")\n",
    "\n",
    "# â”€â”€â”€ ENHANCED visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“ˆ Creating enhanced visualizations...\")\n",
    "\n",
    "# 1. Training history (2x3 grid for more metrics)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Loss\n",
    "axes[0,0].plot(history.history['loss'], label='Train', color='blue')\n",
    "axes[0,0].plot(history.history['val_loss'], label='Validation', color='red')\n",
    "axes[0,0].set_title('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0,1].plot(history.history['accuracy'], label='Train', color='blue')\n",
    "axes[0,1].plot(history.history['val_accuracy'], label='Validation', color='red')\n",
    "axes[0,1].set_title('Accuracy')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[0,2].plot(history.history['precision'], label='Train', color='blue')\n",
    "axes[0,2].plot(history.history['val_precision'], label='Validation', color='red')\n",
    "axes[0,2].set_title('Precision')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1,0].plot(history.history['recall'], label='Train', color='blue')\n",
    "axes[1,0].plot(history.history['val_recall'], label='Validation', color='red')\n",
    "axes[1,0].set_title('Recall')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1,1].plot(history.history['auc'], label='Train', color='blue')\n",
    "axes[1,1].plot(history.history['val_auc'], label='Validation', color='red')\n",
    "axes[1,1].set_title('AUC')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate (if available)\n",
    "if 'lr' in history.history:\n",
    "    axes[1,2].plot(history.history['lr'], color='green')\n",
    "    axes[1,2].set_title('Learning Rate')\n",
    "    axes[1,2].set_yscale('log')\n",
    "else:\n",
    "    axes[1,2].text(0.5, 0.5, 'Learning Rate\\nNot Available', \n",
    "                   ha='center', va='center', transform=axes[1,2].transAxes)\n",
    "    axes[1,2].set_title('Learning Rate')\n",
    "\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"training_history_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Confusion Matrix for all sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (res, name) in enumerate([(train_res, 'Train'), (val_res, 'Validation'), (test_res, 'Test')]):\n",
    "    cm = confusion_matrix(res['y'], res['preds'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'{name} - Confusion Matrix')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"confusion_matrices_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for res, name, color in [(train_res, 'Train', 'blue'), (val_res, 'Val', 'red'), (test_res, 'Test', 'green')]:\n",
    "    if len(np.unique(res['y'])) > 1:  # Check if we have both classes\n",
    "        fpr, tpr, _ = roc_curve(res['y'], res['proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUT_DIR / f\"roc_curves_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â”€â”€â”€ ENHANCED JSON summary with all metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results_summary = {\n",
    "    \"model_info\": {\n",
    "        \"timestamp\": STAMP,\n",
    "        \"architecture\": BEST_PARAMS[\"arch\"],\n",
    "        \"window_size\": BEST_PARAMS[\"window\"],\n",
    "        \"total_parameters\": model.count_params(),\n",
    "        \"optimizer\": BEST_PARAMS[\"optim\"],\n",
    "        \"threshold\": THRESHOLD,\n",
    "        \"beta\": BETA\n",
    "    },\n",
    "    \"best_params\": BEST_PARAMS,\n",
    "    \"data_info\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"features\": len(feat_names),\n",
    "        \"train_samples\": len(X_tr_w),\n",
    "        \"val_samples\": len(X_va_w),\n",
    "        \"test_samples\": len(X_te_w),\n",
    "        \"positive_rate\": float(pos),\n",
    "        \"class_weights\": {\"W0\": float(W0), \"W1\": float(W1)}\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"epochs_trained\": len(history.history['loss']),\n",
    "        \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "        \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "        \"best_val_loss\": float(min(history.history['val_loss']))\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"train\": {k: float(v) for k, v in train_res.items() if not isinstance(v, np.ndarray)},\n",
    "        \"validation\": {k: float(v) for k, v in val_res.items() if not isinstance(v, np.ndarray)},\n",
    "        \"test\": {k: float(v) for k, v in test_res.items() if not isinstance(v, np.ndarray)}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / f\"results_{STAMP}.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# â”€â”€â”€ CLASSIFICATION REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nðŸ“‹ Detailed Classification Report (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(test_res['y'], test_res['preds'], \n",
    "                          target_names=['No Signal', 'Signal']))\n",
    "\n",
    "# â”€â”€â”€ PREDICTION CSV OUTPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nðŸ“„ Creating prediction CSV outputs...\")\n",
    "\n",
    "def create_prediction_csv(X_windows, y_true, original_indices, dataset_name):\n",
    "    \"\"\"Create prediction CSV with required format\"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    probabilities = model.predict(X_windows, verbose=0).ravel()\n",
    "    \n",
    "    # Calculate prob_up (probability of class 1) and prob_down (probability of class 0)\n",
    "    prob_up = probabilities\n",
    "    prob_down = 1 - probabilities\n",
    "    \n",
    "    # Winning probability is the higher of the two\n",
    "    winning_prob = np.maximum(prob_up, prob_down)\n",
    "    \n",
    "    # Predictions based on threshold\n",
    "    predictions = (probabilities >= THRESHOLD).astype(int)\n",
    "    \n",
    "    # Get timestamps for the windows\n",
    "    # We need to account for the window offset\n",
    "    window_size = BEST_PARAMS[\"window\"]\n",
    "    timestamps = df.index[original_indices[window_size:window_size + len(y_true)]]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    pred_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'prob_up': prob_up,\n",
    "        'prob_down': prob_down,\n",
    "        'winning_prob': winning_prob,\n",
    "        'prediction': predictions,\n",
    "        'actual': y_true\n",
    "    })\n",
    "    \n",
    "    # Format probabilities to 6 decimal places\n",
    "    for col in ['prob_up', 'prob_down', 'winning_prob']:\n",
    "        pred_df[col] = pred_df[col].round(6)\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "# Create prediction CSVs for all datasets\n",
    "datasets_info = [\n",
    "    (X_tr_w, y_tr_w, train_idx, \"train\"),\n",
    "    (X_va_w, y_va_w, val_idx, \"validation\"), \n",
    "    (X_te_w, y_te_w, test_idx, \"test\")\n",
    "]\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for X_windows, y_true, original_indices, name in datasets_info:\n",
    "    print(f\"   ðŸ“Š Processing {name} set...\")\n",
    "    pred_df = create_prediction_csv(X_windows, y_true, original_indices, name)\n",
    "    \n",
    "    # Save individual CSV\n",
    "    csv_path = OUT_DIR / f\"predictions_{name}_{STAMP}.csv\"\n",
    "    pred_df.to_csv(csv_path, index=False)\n",
    "    print(f\"   âœ… Saved {name} predictions: {csv_path}\")\n",
    "    \n",
    "    # Add dataset identifier and collect for combined CSV\n",
    "    pred_df['dataset'] = name\n",
    "    all_predictions.append(pred_df)\n",
    "\n",
    "# Create combined CSV with all predictions\n",
    "combined_df = pd.concat(all_predictions, ignore_index=True)\n",
    "combined_csv_path = OUT_DIR / f\"predictions_combined_{STAMP}.csv\"\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "# Create summary statistics\n",
    "print(f\"\\nðŸ“ˆ Prediction Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name in ['train', 'validation', 'test']:\n",
    "    subset = combined_df[combined_df['dataset'] == name]\n",
    "    accuracy = (subset['prediction'] == subset['actual']).mean()\n",
    "    precision = ((subset['prediction'] == 1) & (subset['actual'] == 1)).sum() / (subset['prediction'] == 1).sum() if (subset['prediction'] == 1).sum() > 0 else 0\n",
    "    recall = ((subset['prediction'] == 1) & (subset['actual'] == 1)).sum() / (subset['actual'] == 1).sum() if (subset['actual'] == 1).sum() > 0 else 0\n",
    "    \n",
    "    print(f\"{name.capitalize():<12}: {len(subset):>6} samples | ACC={accuracy:.3f} | P={precision:.3f} | R={recall:.3f}\")\n",
    "\n",
    "# Create a sample of the exact format you requested\n",
    "print(f\"\\nðŸ“‹ Sample predictions (first 10 test samples):\")\n",
    "print(\"=\"*80)\n",
    "sample_df = combined_df[combined_df['dataset'] == 'test'].head(10)\n",
    "\n",
    "# Print header\n",
    "print(\"timestamp        prob_up  prob_down winning_prob prediction actual\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Print sample rows in exact format\n",
    "for _, row in sample_df.iterrows():\n",
    "    timestamp_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(row['timestamp']) else '########'\n",
    "    print(f\"{timestamp_str} {row['prob_up']:.6f} {row['prob_down']:.6f} {row['winning_prob']:.6f} {row['prediction']:>10} {row['actual']:>6}\")\n",
    "\n",
    "# â”€â”€â”€ ENHANCED RESULTS WITH PREDICTION METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Update results summary to include prediction file info\n",
    "results_summary[\"prediction_files\"] = {\n",
    "    \"combined_csv\": f\"predictions_combined_{STAMP}.csv\",\n",
    "    \"train_csv\": f\"predictions_train_{STAMP}.csv\", \n",
    "    \"validation_csv\": f\"predictions_validation_{STAMP}.csv\",\n",
    "    \"test_csv\": f\"predictions_test_{STAMP}.csv\",\n",
    "    \"total_predictions\": len(combined_df),\n",
    "    \"format\": \"timestamp,prob_up,prob_down,winning_prob,prediction,actual,dataset\"\n",
    "}\n",
    "\n",
    "# Add winning probability statistics\n",
    "for name in ['train', 'validation', 'test']:\n",
    "    subset = combined_df[combined_df['dataset'] == name]\n",
    "    results_summary[\"performance\"][name][\"avg_winning_prob\"] = float(subset['winning_prob'].mean())\n",
    "    results_summary[\"performance\"][name][\"min_winning_prob\"] = float(subset['winning_prob'].min())\n",
    "    results_summary[\"performance\"][name][\"max_winning_prob\"] = float(subset['winning_prob'].max())\n",
    "\n",
    "# Re-save updated results\n",
    "with open(OUT_DIR / f\"results_{STAMP}.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸŽ¯ Best Test F0.5 Score: {test_res['f05']:.3f}\")\n",
    "print(f\"ðŸŽ¯ Test Precision: {test_res['precision']:.3f}\")\n",
    "print(f\"ðŸŽ¯ Test Recall: {test_res['recall']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All artifacts saved in {OUT_DIR} with timestamp {STAMP}:\")\n",
    "print(f\"   ðŸ“¦ Model: final_model_{STAMP}.keras\")\n",
    "print(f\"   ðŸ“¦ Best checkpoint: best_model_{STAMP}.keras\")\n",
    "print(f\"   ðŸ“¦ Scaler: scaler_{STAMP}.pkl\")\n",
    "print(f\"   ðŸ“Š Results: results_{STAMP}.json\")\n",
    "print(f\"   ðŸ“ˆ Plots: *_{STAMP}.png\")\n",
    "print(f\"   ðŸ“„ Predictions: predictions_*_{STAMP}.csv\")\n",
    "\n",
    "print(f\"\\nðŸ”® Expected performance based on Optuna results:\")\n",
    "print(f\"   Expected Precision: {BEST_PARAMS['precision']:.3f}\")\n",
    "print(f\"   Expected Recall: {BEST_PARAMS['recall']:.3f}\") \n",
    "print(f\"   Expected F-alpha: {BEST_PARAMS['f_alpha']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Prediction files created:\")\n",
    "print(f\"   ðŸ”— Combined: {combined_csv_path}\")\n",
    "print(f\"   ðŸ“Š Individual files for train/validation/test sets\")\n",
    "print(f\"   ðŸ“ˆ Total predictions: {len(combined_df):,}\")\n",
    "\n",
    "# â”€â”€â”€ CREATE TRADING SIGNAL ANALYSIS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nðŸ’¹ Trading Signal Analysis (Test Set):\")\n",
    "test_subset = combined_df[combined_df['dataset'] == 'test']\n",
    "\n",
    "# High confidence signals (winning_prob > 0.6)\n",
    "high_conf_signals = test_subset[test_subset['winning_prob'] > 0.6]\n",
    "high_conf_buy = high_conf_signals[high_conf_signals['prediction'] == 1]\n",
    "high_conf_accuracy = (high_conf_buy['prediction'] == high_conf_buy['actual']).mean() if len(high_conf_buy) > 0 else 0\n",
    "\n",
    "print(f\"   ðŸ“ˆ High confidence signals (>60%): {len(high_conf_signals)}\")\n",
    "print(f\"   ðŸ“ˆ High confidence BUY signals: {len(high_conf_buy)}\")\n",
    "print(f\"   ðŸ“ˆ High confidence BUY accuracy: {high_conf_accuracy:.3f}\")\n",
    "\n",
    "# Very high confidence signals (winning_prob > 0.7)\n",
    "very_high_conf_signals = test_subset[test_subset['winning_prob'] > 0.7]\n",
    "very_high_conf_buy = very_high_conf_signals[very_high_conf_signals['prediction'] == 1]\n",
    "very_high_conf_accuracy = (very_high_conf_buy['prediction'] == very_high_conf_buy['actual']).mean() if len(very_high_conf_buy) > 0 else 0\n",
    "\n",
    "print(f\"   ðŸš€ Very high confidence signals (>70%): {len(very_high_conf_signals)}\")\n",
    "print(f\"   ðŸš€ Very high confidence BUY signals: {len(very_high_conf_buy)}\")\n",
    "print(f\"   ðŸš€ Very high confidence BUY accuracy: {very_high_conf_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7af5a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving validation predictions â€¦\n",
      "ðŸ“Š Using predictions from: Current Model (fallback)\n",
      "âœ… CSV saved â†’ model_outputs\\cnn_lstm_val_preds_20250614_143207.csv  (0 rows)\n",
      "\n",
      "âœ… Training completed successfully!\n",
      "ðŸ“ All artifacts saved in: model_outputs\n",
      "ðŸ·ï¸  Timestamp: 20250614_143207\n",
      "ðŸŽ¯ Final model selection: Current Model\n",
      "\n",
      "ðŸ”§ IMPROVEMENTS IMPLEMENTED:\n",
      "   âœ… Increased epochs: 200 â†’ 500\n",
      "   âœ… Increased patience: 20 â†’ 50\n",
      "   âœ… More gradual LR reduction: factor 0.5 â†’ 0.7\n",
      "   âœ… Precision-focused class weighting (30% reduction in positive weight)\n",
      "   âœ… Smart model selection using precision-focused score: (3Ã—P + R)/4\n",
      "   âœ… Multiple model checkpoints (precision, F0.5, precision-focused)\n",
      "   âœ… Balanced precision-recall optimization (avoids useless high-P/low-R models)\n",
      "   âœ… Comprehensive model comparison and evaluation\n",
      "ðŸ“Š Loading data â€¦\n",
      "Data shape: (15855, 30), Features: 29\n",
      "Train size: 12684, Val size: 3171\n",
      "Positive rate: 0.508, Class weights: W0=1.000, W1=0.678 (precision-focused)\n",
      "Windowed train shape: (12618, 66, 29), val shape: (3105, 66, 29)\n",
      "ðŸ—ï¸  Building model â€¦\n",
      "Model built with 19,233 parameters\n",
      "ðŸš€ Starting training â€¦\n",
      "Epoch 1/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4933 - auc_3: 0.4942 - f05: 0.5214 - loss: 0.5871 - precision_3: 0.4988 - precision_focused: 0.5291 - recall_3: 0.5971\n",
      "Epoch 1: val_f05 improved from -inf to 0.45791, saving model to model_outputs\\best_f05_20250614_143306.keras\n",
      "\n",
      "Epoch 1: val_precision_focused improved from -inf to 0.46410, saving model to model_outputs\\best_precision_focused_20250614_143306.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.4934 - auc_3: 0.4943 - f05: 0.5214 - loss: 0.5870 - precision_3: 0.4989 - precision_focused: 0.5291 - recall_3: 0.5972 - val_accuracy: 0.5105 - val_auc_3: 0.5195 - val_f05: 0.4579 - val_loss: 0.5774 - val_precision_3: 0.5487 - val_precision_focused: 0.4641 - val_recall_3: 0.3475 - learning_rate: 5.4550e-05\n",
      "Epoch 2/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5062 - auc_3: 0.5063 - f05: 0.5345 - loss: 0.5790 - precision_3: 0.5196 - precision_focused: 0.5425 - recall_3: 0.6210\n",
      "Epoch 2: val_f05 did not improve from 0.45791\n",
      "\n",
      "Epoch 2: val_precision_focused did not improve from 0.46410\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5062 - auc_3: 0.5063 - f05: 0.5345 - loss: 0.5790 - precision_3: 0.5196 - precision_focused: 0.5425 - recall_3: 0.6211 - val_accuracy: 0.5089 - val_auc_3: 0.5144 - val_f05: 0.4277 - val_loss: 0.5779 - val_precision_3: 0.5548 - val_precision_focused: 0.4437 - val_recall_3: 0.2969 - learning_rate: 5.4550e-05\n",
      "Epoch 3/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5148 - auc_3: 0.5166 - f05: 0.5114 - loss: 0.5790 - precision_3: 0.5274 - precision_focused: 0.5121 - recall_3: 0.5332\n",
      "Epoch 3: val_f05 did not improve from 0.45791\n",
      "\n",
      "Epoch 3: val_precision_focused did not improve from 0.46410\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5148 - auc_3: 0.5166 - f05: 0.5114 - loss: 0.5790 - precision_3: 0.5273 - precision_focused: 0.5121 - recall_3: 0.5332 - val_accuracy: 0.5114 - val_auc_3: 0.5157 - val_f05: 0.4499 - val_loss: 0.5774 - val_precision_3: 0.5506 - val_precision_focused: 0.4564 - val_recall_3: 0.3457 - learning_rate: 5.4550e-05\n",
      "Epoch 4/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5048 - auc_3: 0.5055 - f05: 0.5059 - loss: 0.5810 - precision_3: 0.5128 - precision_focused: 0.5057 - recall_3: 0.5068\n",
      "Epoch 4: val_f05 improved from 0.45791 to 0.49548, saving model to model_outputs\\best_f05_20250614_143306.keras\n",
      "\n",
      "Epoch 4: val_precision_focused improved from 0.46410 to 0.49378, saving model to model_outputs\\best_precision_focused_20250614_143306.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5048 - auc_3: 0.5055 - f05: 0.5060 - loss: 0.5810 - precision_3: 0.5129 - precision_focused: 0.5058 - recall_3: 0.5070 - val_accuracy: 0.5050 - val_auc_3: 0.5164 - val_f05: 0.4955 - val_loss: 0.5768 - val_precision_3: 0.5287 - val_precision_focused: 0.4938 - val_recall_3: 0.4716 - learning_rate: 5.4550e-05\n",
      "Epoch 5/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5082 - auc_3: 0.5061 - f05: 0.5179 - loss: 0.5809 - precision_3: 0.5133 - precision_focused: 0.5226 - recall_3: 0.5814\n",
      "Epoch 5: val_f05 did not improve from 0.49548\n",
      "\n",
      "Epoch 5: val_precision_focused did not improve from 0.49378\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5082 - auc_3: 0.5062 - f05: 0.5179 - loss: 0.5808 - precision_3: 0.5134 - precision_focused: 0.5226 - recall_3: 0.5815 - val_accuracy: 0.5027 - val_auc_3: 0.5136 - val_f05: 0.4889 - val_loss: 0.5770 - val_precision_3: 0.5272 - val_precision_focused: 0.4871 - val_recall_3: 0.4543 - learning_rate: 5.4550e-05\n",
      "Epoch 6/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5133 - auc_3: 0.5128 - f05: 0.5241 - loss: 0.5793 - precision_3: 0.5210 - precision_focused: 0.5297 - recall_3: 0.6010\n",
      "Epoch 6: val_f05 did not improve from 0.49548\n",
      "\n",
      "Epoch 6: val_precision_focused did not improve from 0.49378\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5134 - auc_3: 0.5130 - f05: 0.5240 - loss: 0.5793 - precision_3: 0.5211 - precision_focused: 0.5297 - recall_3: 0.6009 - val_accuracy: 0.5060 - val_auc_3: 0.5131 - val_f05: 0.4861 - val_loss: 0.5771 - val_precision_3: 0.5313 - val_precision_focused: 0.4844 - val_recall_3: 0.4506 - learning_rate: 5.4550e-05\n",
      "Epoch 7/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5241 - auc_3: 0.5389 - f05: 0.5060 - loss: 0.5790 - precision_3: 0.5296 - precision_focused: 0.5058 - recall_3: 0.5237\n",
      "Epoch 7: val_f05 improved from 0.49548 to 0.51662, saving model to model_outputs\\best_f05_20250614_143306.keras\n",
      "\n",
      "Epoch 7: val_precision_focused improved from 0.49378 to 0.51945, saving model to model_outputs\\best_precision_focused_20250614_143306.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5241 - auc_3: 0.5388 - f05: 0.5060 - loss: 0.5790 - precision_3: 0.5296 - precision_focused: 0.5058 - recall_3: 0.5237 - val_accuracy: 0.5072 - val_auc_3: 0.5148 - val_f05: 0.5166 - val_loss: 0.5767 - val_precision_3: 0.5255 - val_precision_focused: 0.5195 - val_recall_3: 0.5722 - learning_rate: 5.4550e-05\n",
      "Epoch 8/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5176 - auc_3: 0.5238 - f05: 0.5425 - loss: 0.5770 - precision_3: 0.5258 - precision_focused: 0.5561 - recall_3: 0.6811\n",
      "Epoch 8: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 8: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5177 - auc_3: 0.5239 - f05: 0.5423 - loss: 0.5770 - precision_3: 0.5257 - precision_focused: 0.5559 - recall_3: 0.6806 - val_accuracy: 0.5024 - val_auc_3: 0.5150 - val_f05: 0.4553 - val_loss: 0.5778 - val_precision_3: 0.5347 - val_precision_focused: 0.4591 - val_recall_3: 0.3568 - learning_rate: 5.4550e-05\n",
      "Epoch 9/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5144 - auc_3: 0.5249 - f05: 0.5192 - loss: 0.5779 - precision_3: 0.5252 - precision_focused: 0.5214 - recall_3: 0.5616\n",
      "Epoch 9: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 9: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5145 - auc_3: 0.5249 - f05: 0.5191 - loss: 0.5779 - precision_3: 0.5253 - precision_focused: 0.5213 - recall_3: 0.5616 - val_accuracy: 0.5069 - val_auc_3: 0.5168 - val_f05: 0.4788 - val_loss: 0.5772 - val_precision_3: 0.5346 - val_precision_focused: 0.4777 - val_recall_3: 0.4241 - learning_rate: 5.4550e-05\n",
      "Epoch 10/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5297 - auc_3: 0.5390 - f05: 0.5134 - loss: 0.5795 - precision_3: 0.5285 - precision_focused: 0.5175 - recall_3: 0.5908\n",
      "Epoch 10: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 10: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5296 - auc_3: 0.5389 - f05: 0.5134 - loss: 0.5795 - precision_3: 0.5285 - precision_focused: 0.5175 - recall_3: 0.5906 - val_accuracy: 0.5134 - val_auc_3: 0.5170 - val_f05: 0.4494 - val_loss: 0.5779 - val_precision_3: 0.5531 - val_precision_focused: 0.4554 - val_recall_3: 0.3506 - learning_rate: 5.4550e-05\n",
      "Epoch 11/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5175 - auc_3: 0.5263 - f05: 0.5164 - loss: 0.5796 - precision_3: 0.5206 - precision_focused: 0.5209 - recall_3: 0.5870\n",
      "Epoch 11: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 11: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5175 - auc_3: 0.5263 - f05: 0.5164 - loss: 0.5796 - precision_3: 0.5206 - precision_focused: 0.5209 - recall_3: 0.5869 - val_accuracy: 0.5056 - val_auc_3: 0.5138 - val_f05: 0.4647 - val_loss: 0.5777 - val_precision_3: 0.5364 - val_precision_focused: 0.4658 - val_recall_3: 0.3864 - learning_rate: 5.4550e-05\n",
      "Epoch 12/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5114 - auc_3: 0.5161 - f05: 0.5181 - loss: 0.5800 - precision_3: 0.5180 - precision_focused: 0.5220 - recall_3: 0.5755\n",
      "Epoch 12: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 12: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5115 - auc_3: 0.5162 - f05: 0.5181 - loss: 0.5800 - precision_3: 0.5180 - precision_focused: 0.5220 - recall_3: 0.5754 - val_accuracy: 0.5079 - val_auc_3: 0.5128 - val_f05: 0.4614 - val_loss: 0.5780 - val_precision_3: 0.5403 - val_precision_focused: 0.4632 - val_recall_3: 0.3809 - learning_rate: 5.4550e-05\n",
      "Epoch 13/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5260 - auc_3: 0.5380 - f05: 0.5187 - loss: 0.5774 - precision_3: 0.5345 - precision_focused: 0.5206 - recall_3: 0.5672\n",
      "Epoch 13: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 13: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5259 - auc_3: 0.5379 - f05: 0.5186 - loss: 0.5774 - precision_3: 0.5344 - precision_focused: 0.5205 - recall_3: 0.5670 - val_accuracy: 0.5050 - val_auc_3: 0.5180 - val_f05: 0.4515 - val_loss: 0.5778 - val_precision_3: 0.5393 - val_precision_focused: 0.4562 - val_recall_3: 0.3519 - learning_rate: 5.4550e-05\n",
      "Epoch 14/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5232 - auc_3: 0.5271 - f05: 0.5156 - loss: 0.5793 - precision_3: 0.5283 - precision_focused: 0.5187 - recall_3: 0.5779\n",
      "Epoch 14: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 14: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5232 - auc_3: 0.5272 - f05: 0.5156 - loss: 0.5793 - precision_3: 0.5283 - precision_focused: 0.5187 - recall_3: 0.5777 - val_accuracy: 0.5063 - val_auc_3: 0.5192 - val_f05: 0.4730 - val_loss: 0.5770 - val_precision_3: 0.5358 - val_precision_focused: 0.4730 - val_recall_3: 0.4019 - learning_rate: 5.4550e-05\n",
      "Epoch 15/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5268 - auc_3: 0.5371 - f05: 0.5084 - loss: 0.5794 - precision_3: 0.5285 - precision_focused: 0.5103 - recall_3: 0.5595\n",
      "Epoch 15: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 15: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5268 - auc_3: 0.5371 - f05: 0.5084 - loss: 0.5794 - precision_3: 0.5285 - precision_focused: 0.5103 - recall_3: 0.5596 - val_accuracy: 0.5124 - val_auc_3: 0.5168 - val_f05: 0.5146 - val_loss: 0.5764 - val_precision_3: 0.5311 - val_precision_focused: 0.5161 - val_recall_3: 0.5586 - learning_rate: 5.4550e-05\n",
      "Epoch 16/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5249 - auc_3: 0.5403 - f05: 0.5214 - loss: 0.5770 - precision_3: 0.5312 - precision_focused: 0.5257 - recall_3: 0.5951\n",
      "Epoch 16: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 16: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5250 - auc_3: 0.5403 - f05: 0.5213 - loss: 0.5770 - precision_3: 0.5312 - precision_focused: 0.5256 - recall_3: 0.5948 - val_accuracy: 0.5143 - val_auc_3: 0.5160 - val_f05: 0.5102 - val_loss: 0.5766 - val_precision_3: 0.5340 - val_precision_focused: 0.5106 - val_recall_3: 0.5426 - learning_rate: 5.4550e-05\n",
      "Epoch 17/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5318 - auc_3: 0.5451 - f05: 0.5159 - loss: 0.5781 - precision_3: 0.5322 - precision_focused: 0.5203 - recall_3: 0.5985\n",
      "Epoch 17: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 17: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5318 - auc_3: 0.5450 - f05: 0.5159 - loss: 0.5781 - precision_3: 0.5322 - precision_focused: 0.5203 - recall_3: 0.5985 - val_accuracy: 0.5063 - val_auc_3: 0.5146 - val_f05: 0.5005 - val_loss: 0.5769 - val_precision_3: 0.5287 - val_precision_focused: 0.4992 - val_recall_3: 0.4951 - learning_rate: 5.4550e-05\n",
      "Epoch 18/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5280 - auc_3: 0.5390 - f05: 0.5167 - loss: 0.5783 - precision_3: 0.5328 - precision_focused: 0.5195 - recall_3: 0.5772\n",
      "Epoch 18: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 18: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5281 - auc_3: 0.5391 - f05: 0.5167 - loss: 0.5783 - precision_3: 0.5329 - precision_focused: 0.5194 - recall_3: 0.5770 - val_accuracy: 0.5008 - val_auc_3: 0.5151 - val_f05: 0.4853 - val_loss: 0.5776 - val_precision_3: 0.5259 - val_precision_focused: 0.4837 - val_recall_3: 0.4389 - learning_rate: 5.4550e-05\n",
      "Epoch 19/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5345 - auc_3: 0.5481 - f05: 0.5226 - loss: 0.5761 - precision_3: 0.5410 - precision_focused: 0.5260 - recall_3: 0.5931\n",
      "Epoch 19: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 19: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5345 - auc_3: 0.5481 - f05: 0.5226 - loss: 0.5761 - precision_3: 0.5410 - precision_focused: 0.5259 - recall_3: 0.5929 - val_accuracy: 0.5089 - val_auc_3: 0.5157 - val_f05: 0.4434 - val_loss: 0.5786 - val_precision_3: 0.5480 - val_precision_focused: 0.4513 - val_recall_3: 0.3346 - learning_rate: 5.4550e-05\n",
      "Epoch 20/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5325 - auc_3: 0.5436 - f05: 0.5196 - loss: 0.5758 - precision_3: 0.5460 - precision_focused: 0.5212 - recall_3: 0.5668\n",
      "Epoch 20: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 20: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5325 - auc_3: 0.5436 - f05: 0.5196 - loss: 0.5758 - precision_3: 0.5459 - precision_focused: 0.5212 - recall_3: 0.5668 - val_accuracy: 0.5047 - val_auc_3: 0.5174 - val_f05: 0.4074 - val_loss: 0.5794 - val_precision_3: 0.5526 - val_precision_focused: 0.4310 - val_recall_3: 0.2660 - learning_rate: 5.4550e-05\n",
      "Epoch 21/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - auc_3: 0.5463 - f05: 0.5135 - loss: 0.5759 - precision_3: 0.5496 - precision_focused: 0.5139 - recall_3: 0.5533\n",
      "Epoch 21: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 21: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5382 - auc_3: 0.5463 - f05: 0.5134 - loss: 0.5760 - precision_3: 0.5494 - precision_focused: 0.5138 - recall_3: 0.5534 - val_accuracy: 0.5031 - val_auc_3: 0.5155 - val_f05: 0.4449 - val_loss: 0.5784 - val_precision_3: 0.5378 - val_precision_focused: 0.4513 - val_recall_3: 0.3383 - learning_rate: 5.4550e-05\n",
      "Epoch 22/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5271 - auc_3: 0.5431 - f05: 0.4975 - loss: 0.5793 - precision_3: 0.5278 - precision_focused: 0.4974 - recall_3: 0.5211\n",
      "Epoch 22: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 22: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5272 - auc_3: 0.5431 - f05: 0.4975 - loss: 0.5793 - precision_3: 0.5279 - precision_focused: 0.4975 - recall_3: 0.5213 - val_accuracy: 0.4973 - val_auc_3: 0.5123 - val_f05: 0.4754 - val_loss: 0.5782 - val_precision_3: 0.5231 - val_precision_focused: 0.4745 - val_recall_3: 0.4117 - learning_rate: 5.4550e-05\n",
      "Epoch 23/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5322 - auc_3: 0.5449 - f05: 0.5361 - loss: 0.5746 - precision_3: 0.5405 - precision_focused: 0.5436 - recall_3: 0.6399\n",
      "Epoch 23: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 23: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5322 - auc_3: 0.5449 - f05: 0.5358 - loss: 0.5746 - precision_3: 0.5404 - precision_focused: 0.5432 - recall_3: 0.6390 - val_accuracy: 0.5024 - val_auc_3: 0.5131 - val_f05: 0.4586 - val_loss: 0.5785 - val_precision_3: 0.5335 - val_precision_focused: 0.4610 - val_recall_3: 0.3691 - learning_rate: 5.4550e-05\n",
      "Epoch 24/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5341 - auc_3: 0.5513 - f05: 0.4974 - loss: 0.5779 - precision_3: 0.5365 - precision_focused: 0.4968 - recall_3: 0.5201\n",
      "Epoch 24: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 24: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5341 - auc_3: 0.5513 - f05: 0.4975 - loss: 0.5779 - precision_3: 0.5365 - precision_focused: 0.4969 - recall_3: 0.5204 - val_accuracy: 0.5079 - val_auc_3: 0.5131 - val_f05: 0.4567 - val_loss: 0.5786 - val_precision_3: 0.5421 - val_precision_focused: 0.4601 - val_recall_3: 0.3654 - learning_rate: 5.4550e-05\n",
      "Epoch 25/500\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5332 - auc_3: 0.5532 - f05: 0.5261 - loss: 0.5748 - precision_3: 0.5404 - precision_focused: 0.5311 - recall_3: 0.6133\n",
      "Epoch 25: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 25: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5333 - auc_3: 0.5532 - f05: 0.5261 - loss: 0.5748 - precision_3: 0.5404 - precision_focused: 0.5310 - recall_3: 0.6133 - val_accuracy: 0.5063 - val_auc_3: 0.5122 - val_f05: 0.4311 - val_loss: 0.5793 - val_precision_3: 0.5475 - val_precision_focused: 0.4434 - val_recall_3: 0.3093 - learning_rate: 5.4550e-05\n",
      "Epoch 26/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5370 - auc_3: 0.5480 - f05: 0.4959 - loss: 0.5775 - precision_3: 0.5455 - precision_focused: 0.4944 - recall_3: 0.4940\n",
      "Epoch 26: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 26: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5369 - auc_3: 0.5480 - f05: 0.4961 - loss: 0.5775 - precision_3: 0.5454 - precision_focused: 0.4945 - recall_3: 0.4944 - val_accuracy: 0.5063 - val_auc_3: 0.5117 - val_f05: 0.4919 - val_loss: 0.5776 - val_precision_3: 0.5306 - val_precision_focused: 0.4901 - val_recall_3: 0.4654 - learning_rate: 5.4550e-05\n",
      "Epoch 27/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5407 - auc_3: 0.5494 - f05: 0.5272 - loss: 0.5750 - precision_3: 0.5478 - precision_focused: 0.5321 - recall_3: 0.6222\n",
      "Epoch 27: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 27: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5407 - auc_3: 0.5494 - f05: 0.5271 - loss: 0.5750 - precision_3: 0.5478 - precision_focused: 0.5320 - recall_3: 0.6220 - val_accuracy: 0.5034 - val_auc_3: 0.5119 - val_f05: 0.4495 - val_loss: 0.5784 - val_precision_3: 0.5376 - val_precision_focused: 0.4551 - val_recall_3: 0.3438 - learning_rate: 5.4550e-05\n",
      "Epoch 28/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5393 - auc_3: 0.5532 - f05: 0.5026 - loss: 0.5770 - precision_3: 0.5479 - precision_focused: 0.5017 - recall_3: 0.5223\n",
      "Epoch 28: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 28: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5394 - auc_3: 0.5533 - f05: 0.5026 - loss: 0.5770 - precision_3: 0.5479 - precision_focused: 0.5018 - recall_3: 0.5225 - val_accuracy: 0.5031 - val_auc_3: 0.5130 - val_f05: 0.4583 - val_loss: 0.5787 - val_precision_3: 0.5345 - val_precision_focused: 0.4608 - val_recall_3: 0.3679 - learning_rate: 5.4550e-05\n",
      "Epoch 29/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5469 - auc_3: 0.5637 - f05: 0.5197 - loss: 0.5736 - precision_3: 0.5562 - precision_focused: 0.5211 - recall_3: 0.5807\n",
      "Epoch 29: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 29: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5469 - auc_3: 0.5637 - f05: 0.5197 - loss: 0.5737 - precision_3: 0.5562 - precision_focused: 0.5210 - recall_3: 0.5806 - val_accuracy: 0.5050 - val_auc_3: 0.5107 - val_f05: 0.4651 - val_loss: 0.5798 - val_precision_3: 0.5348 - val_precision_focused: 0.4655 - val_recall_3: 0.3938 - learning_rate: 5.4550e-05\n",
      "Epoch 30/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5535 - auc_3: 0.5665 - f05: 0.5197 - loss: 0.5741 - precision_3: 0.5585 - precision_focused: 0.5221 - recall_3: 0.6000\n",
      "Epoch 30: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 30: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5535 - auc_3: 0.5665 - f05: 0.5197 - loss: 0.5741 - precision_3: 0.5584 - precision_focused: 0.5221 - recall_3: 0.5998 - val_accuracy: 0.4992 - val_auc_3: 0.5138 - val_f05: 0.4527 - val_loss: 0.5790 - val_precision_3: 0.5301 - val_precision_focused: 0.4564 - val_recall_3: 0.3537 - learning_rate: 5.4550e-05\n",
      "Epoch 31/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5426 - auc_3: 0.5576 - f05: 0.5166 - loss: 0.5755 - precision_3: 0.5502 - precision_focused: 0.5181 - recall_3: 0.5761\n",
      "Epoch 31: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 31: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5426 - auc_3: 0.5576 - f05: 0.5166 - loss: 0.5755 - precision_3: 0.5502 - precision_focused: 0.5181 - recall_3: 0.5761 - val_accuracy: 0.5056 - val_auc_3: 0.5119 - val_f05: 0.4725 - val_loss: 0.5790 - val_precision_3: 0.5340 - val_precision_focused: 0.4719 - val_recall_3: 0.4117 - learning_rate: 5.4550e-05\n",
      "Epoch 32/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5451 - auc_3: 0.5613 - f05: 0.5091 - loss: 0.5756 - precision_3: 0.5501 - precision_focused: 0.5099 - recall_3: 0.5653\n",
      "Epoch 32: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 32: val_precision_focused did not improve from 0.51945\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.818500044872053e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5451 - auc_3: 0.5613 - f05: 0.5091 - loss: 0.5756 - precision_3: 0.5501 - precision_focused: 0.5099 - recall_3: 0.5653 - val_accuracy: 0.5069 - val_auc_3: 0.5119 - val_f05: 0.4605 - val_loss: 0.5798 - val_precision_3: 0.5389 - val_precision_focused: 0.4622 - val_recall_3: 0.3802 - learning_rate: 5.4550e-05\n",
      "Epoch 33/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5419 - auc_3: 0.5571 - f05: 0.5105 - loss: 0.5772 - precision_3: 0.5442 - precision_focused: 0.5119 - recall_3: 0.5681\n",
      "Epoch 33: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 33: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5420 - auc_3: 0.5572 - f05: 0.5105 - loss: 0.5772 - precision_3: 0.5443 - precision_focused: 0.5118 - recall_3: 0.5680 - val_accuracy: 0.5063 - val_auc_3: 0.5116 - val_f05: 0.4540 - val_loss: 0.5800 - val_precision_3: 0.5400 - val_precision_focused: 0.4575 - val_recall_3: 0.3623 - learning_rate: 3.8185e-05\n",
      "Epoch 34/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5470 - auc_3: 0.5589 - f05: 0.5262 - loss: 0.5742 - precision_3: 0.5535 - precision_focused: 0.5300 - recall_3: 0.6136\n",
      "Epoch 34: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 34: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5470 - auc_3: 0.5590 - f05: 0.5260 - loss: 0.5742 - precision_3: 0.5534 - precision_focused: 0.5298 - recall_3: 0.6133 - val_accuracy: 0.5056 - val_auc_3: 0.5125 - val_f05: 0.4501 - val_loss: 0.5799 - val_precision_3: 0.5401 - val_precision_focused: 0.4547 - val_recall_3: 0.3531 - learning_rate: 3.8185e-05\n",
      "Epoch 35/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5484 - auc_3: 0.5599 - f05: 0.5250 - loss: 0.5729 - precision_3: 0.5590 - precision_focused: 0.5274 - recall_3: 0.5999\n",
      "Epoch 35: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 35: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5484 - auc_3: 0.5599 - f05: 0.5250 - loss: 0.5729 - precision_3: 0.5590 - precision_focused: 0.5273 - recall_3: 0.5998 - val_accuracy: 0.5037 - val_auc_3: 0.5124 - val_f05: 0.4491 - val_loss: 0.5799 - val_precision_3: 0.5375 - val_precision_focused: 0.4539 - val_recall_3: 0.3494 - learning_rate: 3.8185e-05\n",
      "Epoch 36/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5430 - auc_3: 0.5613 - f05: 0.5162 - loss: 0.5735 - precision_3: 0.5551 - precision_focused: 0.5162 - recall_3: 0.5543\n",
      "Epoch 36: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 36: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5430 - auc_3: 0.5613 - f05: 0.5161 - loss: 0.5735 - precision_3: 0.5551 - precision_focused: 0.5161 - recall_3: 0.5543 - val_accuracy: 0.5021 - val_auc_3: 0.5133 - val_f05: 0.4523 - val_loss: 0.5796 - val_precision_3: 0.5343 - val_precision_focused: 0.4560 - val_recall_3: 0.3562 - learning_rate: 3.8185e-05\n",
      "Epoch 37/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5449 - auc_3: 0.5676 - f05: 0.5101 - loss: 0.5744 - precision_3: 0.5505 - precision_focused: 0.5110 - recall_3: 0.5667\n",
      "Epoch 37: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 37: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5449 - auc_3: 0.5676 - f05: 0.5101 - loss: 0.5744 - precision_3: 0.5506 - precision_focused: 0.5110 - recall_3: 0.5666 - val_accuracy: 0.4998 - val_auc_3: 0.5157 - val_f05: 0.4391 - val_loss: 0.5798 - val_precision_3: 0.5341 - val_precision_focused: 0.4473 - val_recall_3: 0.3235 - learning_rate: 3.8185e-05\n",
      "Epoch 38/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5527 - auc_3: 0.5670 - f05: 0.5096 - loss: 0.5746 - precision_3: 0.5590 - precision_focused: 0.5097 - recall_3: 0.5605\n",
      "Epoch 38: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 38: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5527 - auc_3: 0.5670 - f05: 0.5096 - loss: 0.5746 - precision_3: 0.5590 - precision_focused: 0.5097 - recall_3: 0.5605 - val_accuracy: 0.5056 - val_auc_3: 0.5138 - val_f05: 0.4635 - val_loss: 0.5795 - val_precision_3: 0.5362 - val_precision_focused: 0.4644 - val_recall_3: 0.3889 - learning_rate: 3.8185e-05\n",
      "Epoch 39/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5470 - auc_3: 0.5664 - f05: 0.5126 - loss: 0.5754 - precision_3: 0.5494 - precision_focused: 0.5144 - recall_3: 0.5814\n",
      "Epoch 39: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 39: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5469 - auc_3: 0.5664 - f05: 0.5126 - loss: 0.5754 - precision_3: 0.5493 - precision_focused: 0.5144 - recall_3: 0.5813 - val_accuracy: 0.5037 - val_auc_3: 0.5143 - val_f05: 0.4521 - val_loss: 0.5795 - val_precision_3: 0.5367 - val_precision_focused: 0.4559 - val_recall_3: 0.3568 - learning_rate: 3.8185e-05\n",
      "Epoch 40/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5458 - auc_3: 0.5692 - f05: 0.5218 - loss: 0.5725 - precision_3: 0.5533 - precision_focused: 0.5243 - recall_3: 0.5963\n",
      "Epoch 40: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 40: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5459 - auc_3: 0.5692 - f05: 0.5217 - loss: 0.5725 - precision_3: 0.5533 - precision_focused: 0.5243 - recall_3: 0.5961 - val_accuracy: 0.4986 - val_auc_3: 0.5134 - val_f05: 0.4492 - val_loss: 0.5803 - val_precision_3: 0.5298 - val_precision_focused: 0.4539 - val_recall_3: 0.3457 - learning_rate: 3.8185e-05\n",
      "Epoch 41/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5579 - auc_3: 0.5775 - f05: 0.5112 - loss: 0.5733 - precision_3: 0.5599 - precision_focused: 0.5126 - recall_3: 0.5853\n",
      "Epoch 41: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 41: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5578 - auc_3: 0.5775 - f05: 0.5112 - loss: 0.5733 - precision_3: 0.5599 - precision_focused: 0.5126 - recall_3: 0.5851 - val_accuracy: 0.4966 - val_auc_3: 0.5155 - val_f05: 0.4455 - val_loss: 0.5809 - val_precision_3: 0.5276 - val_precision_focused: 0.4513 - val_recall_3: 0.3358 - learning_rate: 3.8185e-05\n",
      "Epoch 42/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5479 - auc_3: 0.5671 - f05: 0.5163 - loss: 0.5733 - precision_3: 0.5549 - precision_focused: 0.5175 - recall_3: 0.5767\n",
      "Epoch 42: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 42: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5480 - auc_3: 0.5671 - f05: 0.5163 - loss: 0.5733 - precision_3: 0.5549 - precision_focused: 0.5174 - recall_3: 0.5768 - val_accuracy: 0.4963 - val_auc_3: 0.5167 - val_f05: 0.4156 - val_loss: 0.5817 - val_precision_3: 0.5339 - val_precision_focused: 0.4348 - val_recall_3: 0.2722 - learning_rate: 3.8185e-05\n",
      "Epoch 43/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5498 - auc_3: 0.5661 - f05: 0.5123 - loss: 0.5741 - precision_3: 0.5595 - precision_focused: 0.5123 - recall_3: 0.5575\n",
      "Epoch 43: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 43: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5498 - auc_3: 0.5661 - f05: 0.5123 - loss: 0.5741 - precision_3: 0.5594 - precision_focused: 0.5123 - recall_3: 0.5575 - val_accuracy: 0.4944 - val_auc_3: 0.5154 - val_f05: 0.4330 - val_loss: 0.5813 - val_precision_3: 0.5267 - val_precision_focused: 0.4438 - val_recall_3: 0.3043 - learning_rate: 3.8185e-05\n",
      "Epoch 44/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5629 - auc_3: 0.5830 - f05: 0.5174 - loss: 0.5696 - precision_3: 0.5760 - precision_focused: 0.5179 - recall_3: 0.5835\n",
      "Epoch 44: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 44: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5628 - auc_3: 0.5829 - f05: 0.5172 - loss: 0.5696 - precision_3: 0.5758 - precision_focused: 0.5178 - recall_3: 0.5831 - val_accuracy: 0.4986 - val_auc_3: 0.5165 - val_f05: 0.4353 - val_loss: 0.5815 - val_precision_3: 0.5330 - val_precision_focused: 0.4450 - val_recall_3: 0.3142 - learning_rate: 3.8185e-05\n",
      "Epoch 45/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5541 - auc_3: 0.5758 - f05: 0.5094 - loss: 0.5731 - precision_3: 0.5600 - precision_focused: 0.5098 - recall_3: 0.5654\n",
      "Epoch 45: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 45: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5541 - auc_3: 0.5758 - f05: 0.5095 - loss: 0.5731 - precision_3: 0.5600 - precision_focused: 0.5098 - recall_3: 0.5654 - val_accuracy: 0.4986 - val_auc_3: 0.5153 - val_f05: 0.4519 - val_loss: 0.5807 - val_precision_3: 0.5290 - val_precision_focused: 0.4555 - val_recall_3: 0.3543 - learning_rate: 3.8185e-05\n",
      "Epoch 46/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5555 - auc_3: 0.5768 - f05: 0.5169 - loss: 0.5713 - precision_3: 0.5648 - precision_focused: 0.5176 - recall_3: 0.5774\n",
      "Epoch 46: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 46: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5555 - auc_3: 0.5767 - f05: 0.5169 - loss: 0.5713 - precision_3: 0.5648 - precision_focused: 0.5175 - recall_3: 0.5773 - val_accuracy: 0.5018 - val_auc_3: 0.5152 - val_f05: 0.4564 - val_loss: 0.5811 - val_precision_3: 0.5329 - val_precision_focused: 0.4591 - val_recall_3: 0.3654 - learning_rate: 3.8185e-05\n",
      "Epoch 47/500\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5553 - auc_3: 0.5795 - f05: 0.5060 - loss: 0.5730 - precision_3: 0.5595 - precision_focused: 0.5062 - recall_3: 0.5596\n",
      "Epoch 47: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 47: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5553 - auc_3: 0.5795 - f05: 0.5060 - loss: 0.5730 - precision_3: 0.5595 - precision_focused: 0.5062 - recall_3: 0.5596 - val_accuracy: 0.4982 - val_auc_3: 0.5144 - val_f05: 0.4471 - val_loss: 0.5815 - val_precision_3: 0.5296 - val_precision_focused: 0.4523 - val_recall_3: 0.3420 - learning_rate: 3.8185e-05\n",
      "Epoch 48/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5507 - auc_3: 0.5700 - f05: 0.5237 - loss: 0.5709 - precision_3: 0.5675 - precision_focused: 0.5240 - recall_3: 0.5741\n",
      "Epoch 48: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 48: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5508 - auc_3: 0.5701 - f05: 0.5236 - loss: 0.5709 - precision_3: 0.5675 - precision_focused: 0.5239 - recall_3: 0.5739 - val_accuracy: 0.4950 - val_auc_3: 0.5174 - val_f05: 0.4229 - val_loss: 0.5830 - val_precision_3: 0.5297 - val_precision_focused: 0.4380 - val_recall_3: 0.2864 - learning_rate: 3.8185e-05\n",
      "Epoch 49/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5484 - auc_3: 0.5703 - f05: 0.5103 - loss: 0.5726 - precision_3: 0.5627 - precision_focused: 0.5092 - recall_3: 0.5341\n",
      "Epoch 49: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 49: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5484 - auc_3: 0.5703 - f05: 0.5103 - loss: 0.5726 - precision_3: 0.5627 - precision_focused: 0.5092 - recall_3: 0.5341 - val_accuracy: 0.4960 - val_auc_3: 0.5145 - val_f05: 0.4346 - val_loss: 0.5815 - val_precision_3: 0.5289 - val_precision_focused: 0.4444 - val_recall_3: 0.3111 - learning_rate: 3.8185e-05\n",
      "Epoch 50/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5560 - auc_3: 0.5760 - f05: 0.5178 - loss: 0.5716 - precision_3: 0.5638 - precision_focused: 0.5192 - recall_3: 0.5893\n",
      "Epoch 50: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 50: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5560 - auc_3: 0.5761 - f05: 0.5176 - loss: 0.5716 - precision_3: 0.5637 - precision_focused: 0.5190 - recall_3: 0.5889 - val_accuracy: 0.4950 - val_auc_3: 0.5161 - val_f05: 0.4430 - val_loss: 0.5826 - val_precision_3: 0.5258 - val_precision_focused: 0.4500 - val_recall_3: 0.3265 - learning_rate: 3.8185e-05\n",
      "Epoch 51/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5574 - auc_3: 0.5806 - f05: 0.5209 - loss: 0.5709 - precision_3: 0.5613 - precision_focused: 0.5241 - recall_3: 0.6132\n",
      "Epoch 51: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 51: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5574 - auc_3: 0.5805 - f05: 0.5208 - loss: 0.5709 - precision_3: 0.5612 - precision_focused: 0.5240 - recall_3: 0.6129 - val_accuracy: 0.4989 - val_auc_3: 0.5144 - val_f05: 0.4589 - val_loss: 0.5823 - val_precision_3: 0.5282 - val_precision_focused: 0.4609 - val_recall_3: 0.3698 - learning_rate: 3.8185e-05\n",
      "Epoch 52/500\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5594 - auc_3: 0.5779 - f05: 0.5116 - loss: 0.5726 - precision_3: 0.5623 - precision_focused: 0.5129 - recall_3: 0.5847\n",
      "Epoch 52: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 52: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5594 - auc_3: 0.5780 - f05: 0.5116 - loss: 0.5725 - precision_3: 0.5623 - precision_focused: 0.5129 - recall_3: 0.5847 - val_accuracy: 0.4973 - val_auc_3: 0.5156 - val_f05: 0.4397 - val_loss: 0.5840 - val_precision_3: 0.5296 - val_precision_focused: 0.4472 - val_recall_3: 0.3253 - learning_rate: 3.8185e-05\n",
      "Epoch 53/500\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5539 - auc_3: 0.5781 - f05: 0.5148 - loss: 0.5709 - precision_3: 0.5628 - precision_focused: 0.5155 - recall_3: 0.5756\n",
      "Epoch 53: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 53: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5538 - auc_3: 0.5781 - f05: 0.5148 - loss: 0.5710 - precision_3: 0.5627 - precision_focused: 0.5155 - recall_3: 0.5756 - val_accuracy: 0.4969 - val_auc_3: 0.5150 - val_f05: 0.4439 - val_loss: 0.5835 - val_precision_3: 0.5282 - val_precision_focused: 0.4498 - val_recall_3: 0.3352 - learning_rate: 3.8185e-05\n",
      "Epoch 54/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5591 - auc_3: 0.5796 - f05: 0.4998 - loss: 0.5728 - precision_3: 0.5675 - precision_focused: 0.4987 - recall_3: 0.5338\n",
      "Epoch 54: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 54: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5591 - auc_3: 0.5795 - f05: 0.4999 - loss: 0.5728 - precision_3: 0.5674 - precision_focused: 0.4988 - recall_3: 0.5341 - val_accuracy: 0.4960 - val_auc_3: 0.5153 - val_f05: 0.4470 - val_loss: 0.5828 - val_precision_3: 0.5262 - val_precision_focused: 0.4519 - val_recall_3: 0.3414 - learning_rate: 3.8185e-05\n",
      "Epoch 55/500\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5553 - auc_3: 0.5669 - f05: 0.5086 - loss: 0.5752 - precision_3: 0.5602 - precision_focused: 0.5089 - recall_3: 0.5648\n",
      "Epoch 55: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 55: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5553 - auc_3: 0.5669 - f05: 0.5086 - loss: 0.5752 - precision_3: 0.5603 - precision_focused: 0.5089 - recall_3: 0.5648 - val_accuracy: 0.5053 - val_auc_3: 0.5149 - val_f05: 0.4724 - val_loss: 0.5816 - val_precision_3: 0.5335 - val_precision_focused: 0.4718 - val_recall_3: 0.4123 - learning_rate: 3.8185e-05\n",
      "Epoch 56/500\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5525 - auc_3: 0.5835 - f05: 0.5196 - loss: 0.5699 - precision_3: 0.5586 - precision_focused: 0.5212 - recall_3: 0.5854\n",
      "Epoch 56: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 56: val_precision_focused did not improve from 0.51945\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5525 - auc_3: 0.5834 - f05: 0.5195 - loss: 0.5700 - precision_3: 0.5587 - precision_focused: 0.5210 - recall_3: 0.5852 - val_accuracy: 0.5037 - val_auc_3: 0.5134 - val_f05: 0.4747 - val_loss: 0.5818 - val_precision_3: 0.5308 - val_precision_focused: 0.4737 - val_recall_3: 0.4198 - learning_rate: 3.8185e-05\n",
      "Epoch 57/500\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5600 - auc_3: 0.5819 - f05: 0.5072 - loss: 0.5719 - precision_3: 0.5630 - precision_focused: 0.5077 - recall_3: 0.5729\n",
      "Epoch 57: val_f05 did not improve from 0.51662\n",
      "\n",
      "Epoch 57: val_precision_focused did not improve from 0.51945\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 2.672950031410437e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5600 - auc_3: 0.5818 - f05: 0.5072 - loss: 0.5719 - precision_3: 0.5630 - precision_focused: 0.5078 - recall_3: 0.5729 - val_accuracy: 0.5066 - val_auc_3: 0.5130 - val_f05: 0.4851 - val_loss: 0.5811 - val_precision_3: 0.5318 - val_precision_focused: 0.4834 - val_recall_3: 0.4543 - learning_rate: 3.8185e-05\n",
      "Epoch 57: early stopping\n",
      "ðŸ” Evaluating Best Precision model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not locate function 'weighted_bce'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'weighted_bce', 'registered_name': 'function'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 403\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Evaluate all model variants\u001b[39;00m\n\u001b[0;32m    402\u001b[0m model_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 403\u001b[0m model_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_precision_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBest Precision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m model_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf05\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_model(best_f05_ckpt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest F0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    405\u001b[0m model_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision_focused\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_model(best_prec_focused_ckpt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Precision-Focused\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 358\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_path, name)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ” Evaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 358\u001b[0m temp_model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF05Metric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mF05Metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrecisionFocusedMetric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPrecisionFocusedMetric\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m    363\u001b[0m val_proba \u001b[38;5;241m=\u001b[39m temp_model\u001b[38;5;241m.\u001b[39mpredict(X_va_w, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:370\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[1;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    445\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 447\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    452\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:436\u001b[0m, in \u001b[0;36m_model_from_config\u001b[1;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 436\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:734\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m     compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compile_config:\n\u001b[1;32m--> 734\u001b[0m         \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m         instance\u001b[38;5;241m.\u001b[39mcompiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\trainers\\trainer.py:972\u001b[0m, in \u001b[0;36mTrainer.compile_from_config\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    961\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`compile()` was not called as part of model loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `compile()` method is custom. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    970\u001b[0m     )\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;66;03m# Create optimizer variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:594\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    595\u001b[0m         key: deserialize_keras_object(\n\u001b[0;32m    596\u001b[0m             value, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, safe_mode\u001b[38;5;241m=\u001b[39msafe_mode\n\u001b[0;32m    597\u001b[0m         )\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    599\u001b[0m     }\n\u001b[0;32m    601\u001b[0m class_name \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    602\u001b[0m inner_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:595\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m--> 595\u001b[0m         key: \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    599\u001b[0m     }\n\u001b[0;32m    601\u001b[0m class_name \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    602\u001b[0m inner_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:678\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    677\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m inner_config\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:810\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[1;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    805\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    806\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be imported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    808\u001b[0m             )\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not locate function 'weighted_bce'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'weighted_bce', 'registered_name': 'function'}"
     ]
    }
   ],
   "source": [
    "# â”€â”€ validation-prediction CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving validation predictions â€¦\")\n",
    "\n",
    "# Calculate the correct timestamp indices\n",
    "first_val_row = val_start + WIN\n",
    "end_val_row = first_val_row + len(y_va_w)\n",
    "\n",
    "# Ensure we don't go beyond the dataframe bounds\n",
    "end_val_row = min(end_val_row, len(df))\n",
    "timestamps = df.index.to_numpy()[first_val_row:end_val_row]\n",
    "\n",
    "# Use the best model's predictions if available\n",
    "if 'final_model_results' in locals() and final_model_results and 'proba' in final_model_results:\n",
    "    best_proba = final_model_results['proba']\n",
    "    best_pred = final_model_results['pred']\n",
    "    model_used = best_model_name\n",
    "    print(f\"ðŸ“Š Using predictions from: {best_model_name}\")\n",
    "else:\n",
    "    # Fallback: generate predictions from current model\n",
    "    print(\"ðŸ“Š Using predictions from: Current Model (fallback)\")\n",
    "    best_proba = model.predict(X_va_w, verbose=0).ravel()\n",
    "    best_pred = (best_proba >= THRESHOLD).astype(int)\n",
    "    model_used = \"Current Model\"\n",
    "\n",
    "# Ensure lengths match\n",
    "min_len = min(len(timestamps), len(best_proba), len(y_va_w))\n",
    "timestamps = timestamps[:min_len]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"prob_up\": best_proba[:min_len],\n",
    "    \"prob_down\": 1.0 - best_proba[:min_len],\n",
    "    \"winning_prob\": np.maximum(best_proba[:min_len], 1.0 - best_proba[:min_len]),\n",
    "    \"prediction\": best_pred[:min_len].astype(int),\n",
    "    \"actual\": y_va_w[:min_len],\n",
    "    \"model_used\": model_used\n",
    "})\n",
    "\n",
    "# Round probabilities\n",
    "val_df[[\"prob_up\", \"prob_down\", \"winning_prob\"]] = val_df[\n",
    "    [\"prob_up\", \"prob_down\", \"winning_prob\"]].round(6)\n",
    "\n",
    "csv_path = OUT_DIR / f\"cnn_lstm_val_preds_{STAMP}.csv\"\n",
    "val_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV saved â†’ {csv_path}  ({len(val_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved in: {OUT_DIR}\")\n",
    "print(f\"ðŸ·ï¸  Timestamp: {STAMP}\")\n",
    "print(f\"ðŸŽ¯ Final model selection: {model_used}\")\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    print(f\"ðŸŽ¯ Final precision-focused score: {final_score:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final precision: {final_model_results['precision']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final recall: {final_model_results['recall']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final F0.5: {final_model_results['f05']:.4f}\")\n",
    "\n",
    "# Summary of improvements made\n",
    "print(f\"\\nðŸ”§ IMPROVEMENTS IMPLEMENTED:\")\n",
    "print(f\"   âœ… Increased epochs: 200 â†’ 500\")\n",
    "print(f\"   âœ… Increased patience: 20 â†’ 50\") \n",
    "print(f\"   âœ… More gradual LR reduction: factor 0.5 â†’ 0.7\")\n",
    "print(f\"   âœ… Precision-focused class weighting (30% reduction in positive weight)\")\n",
    "print(f\"   âœ… Smart model selection using precision-focused score: (3Ã—P + R)/4\")\n",
    "print(f\"   âœ… Multiple model checkpoints (precision, F0.5, precision-focused)\")\n",
    "print(f\"   âœ… Balanced precision-recall optimization (avoids useless high-P/low-R models)\")\n",
    "print(f\"   âœ… Comprehensive model comparison and evaluation\")# =============================================================================\n",
    "#  CNNâ€“LSTM  (80-20 split)  â€“  Optimise F0Â·5  â€“  Export validation CSV\n",
    "# =============================================================================\n",
    "import os, json, warnings, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             fbeta_score, accuracy_score, roc_curve, auc)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping,\n",
    "                                        ReduceLROnPlateau)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# â”€â”€ Optuna-best hyper-params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BEST_PARAMS = {\n",
    "    \"arch\":        \"conv_lstm\",\n",
    "    \"window\":      66,\n",
    "    \"filters\":     32,\n",
    "    \"kernel\":      5,\n",
    "    \"units\":       32,\n",
    "    \"conv_drop\":   0.1990,\n",
    "    \"lstm_drop\":   0.0056,\n",
    "    \"dense\":       32,\n",
    "    \"dropout\":     0.3674,\n",
    "    \"l2\":          1.73e-06,\n",
    "    \"lr\":          5.455e-05,\n",
    "    \"batch\":       32,\n",
    "    \"act\":         \"relu\",\n",
    "    \"conv_blocks\": 2,\n",
    "    \"optim\":       \"nadam\",\n",
    "}\n",
    "\n",
    "# â”€â”€ paths & columns to drop (truncate if you like) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open','high','low','typical_price','EMA_21','SMA_20','vwap_24h','close_4h',\n",
    "    'bollinger_upper','bollinger_lower','resistance_level','support_level',\n",
    "    'high_low','high_close','low_close','true_range','volume_mean_20',\n",
    "    'MACD_line','MACD_signal','volatility_regime','trending_market',\n",
    "    'above_sma50','ema7_above_ema21','rsi_overbought','stoch_oversold',\n",
    "    'cci_oversold','vol_spike_1_5x','near_upper_band','near_lower_band',\n",
    "    'break_upper_band','break_lower_band','rsi_oversold','above_sma20',\n",
    "    'macd_positive','volume_breakout','volume_breakdown','bullish_scenario_1',\n",
    "    'bullish_scenario_2','bullish_scenario_3','bullish_scenario_4',\n",
    "    'bullish_scenario_5','bullish_scenario_6','bearish_scenario_1',\n",
    "    'bearish_scenario_2','bearish_scenario_3','bearish_scenario_4',\n",
    "    'bearish_scenario_6','ema_cross_up','macd_cross_up','oversold_reversal',\n",
    "    'overbought_reversal','close'\n",
    "]\n",
    "\n",
    "# â”€â”€ run parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED       = 42\n",
    "VAL_FRAC   = 0.20\n",
    "BETA       = 0.5\n",
    "THRESHOLD  = 0.5\n",
    "EPOCHS     = 500  # Increased epochs\n",
    "PATIENCE   = 50   # Much more patience\n",
    "LR_PATIENCE = 25  # More patience for LR reduction\n",
    "\n",
    "OUT_DIR = Path(\"model_outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "STAMP   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception: pass\n",
    "\n",
    "# â”€â”€ load & clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Loading data â€¦\")\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "\n",
    "# Filter DROP_COLS to only include columns that actually exist in df\n",
    "existing_drop_cols = [c for c in DROP_COLS if c in df.columns]\n",
    "df = df.drop(columns=existing_drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(np.int8).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "print(f\"Data shape: {df.shape}, Features: {n_feat}\")\n",
    "\n",
    "# â”€â”€ 80-20 chronological split (no test slice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_start = int(len(df) * (1 - VAL_FRAC))\n",
    "train_idx = np.arange(0, val_start)\n",
    "val_idx   = np.arange(val_start, len(df))\n",
    "\n",
    "scaler = StandardScaler().fit(X_raw[train_idx])\n",
    "joblib.dump(scaler, OUT_DIR / f\"scaler_{STAMP}.pkl\")\n",
    "\n",
    "X_train = scaler.transform(X_raw[train_idx])\n",
    "X_val   = scaler.transform(X_raw[val_idx])\n",
    "y_train, y_val = y_raw[train_idx], y_raw[val_idx]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "\n",
    "# â”€â”€ class weighting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pos_rate = y_train.mean()\n",
    "# Stronger emphasis on precision - reduce positive class weight\n",
    "W0, W1 = 1.0, ((1 - pos_rate) / pos_rate * 0.7) if pos_rate > 0 else 1.0  # Reduced by 30%\n",
    "print(f\"Positive rate: {pos_rate:.3f}, Class weights: W0={W0:.3f}, W1={W1:.3f} (precision-focused)\")\n",
    "\n",
    "# â”€â”€ window helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, w: int) -> Tuple[np.ndarray,np.ndarray]:\n",
    "    xs, ys = [], []\n",
    "    for i in range(w, len(X)):\n",
    "        xs.append(X[i-w:i]); ys.append(y[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "WIN = BEST_PARAMS[\"window\"]\n",
    "X_tr_w, y_tr_w = make_windows(X_train, y_train, WIN)\n",
    "X_va_w, y_va_w = make_windows(X_val,   y_val,   WIN)\n",
    "\n",
    "print(f\"Windowed train shape: {X_tr_w.shape}, val shape: {X_va_w.shape}\")\n",
    "\n",
    "# â”€â”€ offline F0Â·5 helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f05_np(y_true, proba):\n",
    "    pred = (proba >= THRESHOLD)\n",
    "    return fbeta_score(y_true, pred, beta=BETA, zero_division=0)\n",
    "\n",
    "# â”€â”€ Custom Precision-Focused Metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class PrecisionFocusedMetric(keras.metrics.Metric):\n",
    "    \"\"\"Custom metric that weighs precision 3x more than recall\"\"\"\n",
    "    def __init__(self, name=\"precision_focused\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        # Weight precision 3x more than recall: 3*P + R / 4\n",
    "        return (3 * prec + rec) / 4\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "# â”€â”€ graph-safe F0Â·5 metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class F05Metric(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f05\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.beta_sq = BETA ** 2\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        return (1 + self.beta_sq) * prec * rec / (self.beta_sq * prec + rec + 1e-9)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "\n",
    "# â”€â”€ model builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "    x   = inp\n",
    "    \n",
    "    for _ in range(cfg[\"conv_blocks\"]):\n",
    "        x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                          activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "        x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "    \n",
    "    x = layers.LSTM(cfg[\"units\"], dropout=cfg[\"lstm_drop\"],\n",
    "                    kernel_regularizer=l2)(x)\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                     kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    # weighted BCE (dtype-safe)\n",
    "    w0_tf = tf.constant(W0, tf.float32)\n",
    "    w1_tf = tf.constant(W1, tf.float32)\n",
    "    \n",
    "    def weighted_bce(y_t, y_p):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        w   = tf.where(tf.equal(y_t, 1.0), w1_tf, w0_tf)\n",
    "        return tf.reduce_mean(w * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    # Optimizer selection\n",
    "    opt_cls = {\n",
    "        \"nadam\": keras.optimizers.Nadam,\n",
    "        \"adamw\": keras.optimizers.AdamW,\n",
    "        \"rmsprop\": keras.optimizers.RMSprop\n",
    "    }.get(cfg[\"optim\"], keras.optimizers.Adam)\n",
    "    \n",
    "    if cfg[\"optim\"] == \"adamw\":\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "    else:\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=weighted_bce,\n",
    "        metrics=[PrecisionFocusedMetric(),\n",
    "                F05Metric(), \n",
    "                keras.metrics.Precision(),\n",
    "                keras.metrics.Recall(), \n",
    "                keras.metrics.AUC(), \n",
    "                \"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# â”€â”€ datasets & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ—ï¸  Building model â€¦\")\n",
    "model = build_model(BEST_PARAMS)\n",
    "print(f\"Model built with {model.count_params():,} parameters\")\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((X_tr_w, y_tr_w))\n",
    "            .shuffle(10_000, seed=SEED)\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds   = (tf.data.Dataset.from_tensor_slices((X_va_w, y_va_w))\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Create multiple checkpoints for different metrics\n",
    "best_precision_ckpt = OUT_DIR / f\"best_precision_{STAMP}.keras\"\n",
    "best_f05_ckpt = OUT_DIR / f\"best_f05_{STAMP}.keras\"\n",
    "best_prec_focused_ckpt = OUT_DIR / f\"best_precision_focused_{STAMP}.keras\"\n",
    "\n",
    "cbs  = [\n",
    "    # Save best models for different metrics\n",
    "    ModelCheckpoint(best_precision_ckpt, monitor=\"val_precision_1\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_f05_ckpt, monitor=\"val_f05\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_prec_focused_ckpt, monitor=\"val_precision_focused\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    \n",
    "    # Early stopping based on precision-focused metric (more lenient)\n",
    "    EarlyStopping(monitor=\"val_precision_focused\", mode=\"max\", patience=PATIENCE,\n",
    "                  restore_best_weights=False, verbose=1),  # Don't restore, we'll choose best manually\n",
    "    \n",
    "    # Reduce LR more gradually\n",
    "    ReduceLROnPlateau(monitor=\"val_precision_focused\", mode=\"max\", patience=LR_PATIENCE,\n",
    "                      factor=0.7, min_lr=1e-8, verbose=1)  # Less aggressive reduction\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Starting training â€¦\")\n",
    "hist = model.fit(train_ds, validation_data=val_ds,\n",
    "                 epochs=EPOCHS, callbacks=cbs, verbose=1)\n",
    "\n",
    "model.save(OUT_DIR / f\"final_model_{STAMP}.keras\")\n",
    "\n",
    "# â”€â”€ Evaluate all saved models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_model(model_path, name):\n",
    "    \"\"\"Evaluate a specific model and return results\"\"\"\n",
    "    if not model_path.exists():\n",
    "        print(f\"âŒ {name} model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ” Evaluating {name} model...\")\n",
    "    temp_model = keras.models.load_model(model_path, \n",
    "                                       custom_objects={'F05Metric': F05Metric, \n",
    "                                                     'PrecisionFocusedMetric': PrecisionFocusedMetric})\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_proba = temp_model.predict(X_va_w, verbose=0).ravel()\n",
    "    val_pred = (val_proba >= THRESHOLD)\n",
    "    \n",
    "    try:\n",
    "        precision = precision_score(y_va_w, val_pred, zero_division=0)\n",
    "        recall = recall_score(y_va_w, val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_va_w, val_pred, zero_division=0)\n",
    "        f05 = f05_np(y_va_w, val_proba)\n",
    "        acc = accuracy_score(y_va_w, val_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_va_w, val_proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        results = {\n",
    "            'name': name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'f05': f05,\n",
    "            'acc': acc,\n",
    "            'auc': auc_score,\n",
    "            'proba': val_proba,\n",
    "            'pred': val_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"   ðŸ“Š {name} Results:\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall:    {recall:.4f}\")\n",
    "        print(f\"      F1:        {f1:.4f}\")\n",
    "        print(f\"      F0.5:      {f05:.4f}\")\n",
    "        print(f\"      Accuracy:  {acc:.4f}\")\n",
    "        print(f\"      AUC:       {auc_score:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error evaluating {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate all model variants\n",
    "model_results = {}\n",
    "model_results['precision'] = evaluate_model(best_precision_ckpt, \"Best Precision\")\n",
    "model_results['f05'] = evaluate_model(best_f05_ckpt, \"Best F0.5\")\n",
    "model_results['precision_focused'] = evaluate_model(best_prec_focused_ckpt, \"Best Precision-Focused\")\n",
    "\n",
    "# Calculate precision-focused score for each model (3x precision + recall) / 4\n",
    "def calculate_precision_focused_score(results):\n",
    "    \"\"\"Calculate the precision-focused score: (3*precision + recall) / 4\"\"\"\n",
    "    if not results or results['precision'] == 0:\n",
    "        return 0\n",
    "    return (3 * results['precision'] + results['recall']) / 4\n",
    "\n",
    "# Find the best model based on precision-focused score\n",
    "valid_results = [r for r in model_results.values() if r and r['precision'] > 0]\n",
    "if valid_results:\n",
    "    best_precision_focused_model = max(valid_results, key=calculate_precision_focused_score)\n",
    "    \n",
    "    # Also find individual bests for comparison\n",
    "    best_precision_model = max(valid_results, key=lambda x: x['precision'])\n",
    "    best_f05_model = max(valid_results, key=lambda x: x['f05'])\n",
    "    best_recall_model = max(valid_results, key=lambda x: x['recall'])\n",
    "    \n",
    "    print(f\"\\nðŸ† MODELS COMPARISON:\")\n",
    "    print(f\"ðŸŽ¯ Highest Precision-Focused Score: {best_precision_focused_model['name']}\")\n",
    "    print(f\"   Score: {calculate_precision_focused_score(best_precision_focused_model):.4f}\")\n",
    "    print(f\"   Precision: {best_precision_focused_model['precision']:.4f}\")\n",
    "    print(f\"   Recall: {best_precision_focused_model['recall']:.4f}\")\n",
    "    print(f\"   F0.5: {best_precision_focused_model['f05']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š For comparison:\")\n",
    "    print(f\"   Highest Precision Only: {best_precision_model['name']} (P:{best_precision_model['precision']:.4f}, R:{best_precision_model['recall']:.4f})\")\n",
    "    print(f\"   Highest F0.5: {best_f05_model['name']} (P:{best_f05_model['precision']:.4f}, R:{best_f05_model['recall']:.4f})\")\n",
    "    print(f\"   Highest Recall: {best_recall_model['name']} (P:{best_recall_model['precision']:.4f}, R:{best_recall_model['recall']:.4f})\")\n",
    "    \n",
    "    # Use the precision-focused model as final choice\n",
    "    final_model_results = best_precision_focused_model\n",
    "    best_model_name = best_precision_focused_model['name']\n",
    "    final_score = calculate_precision_focused_score(best_precision_focused_model)\n",
    "    \n",
    "    print(f\"\\nâœ… SELECTED: {best_model_name}\")\n",
    "    print(f\"   Reason: Highest precision-focused score (3Ã—P + R)/4 = {final_score:.4f}\")\n",
    "    print(f\"   This balances high precision ({best_precision_focused_model['precision']:.4f}) with reasonable recall ({best_precision_focused_model['recall']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    # Fallback to current model\n",
    "    current_val_eval = evaluate(X_va_w, y_va_w, \"current model fallback\")\n",
    "    final_model_results = current_val_eval\n",
    "    best_model_name = \"Current Model (Fallback)\"\n",
    "    final_score = calculate_precision_focused_score(current_val_eval)\n",
    "    print(f\"\\nâš ï¸  Using fallback current model (precision-focused score: {final_score:.4f})\")\n",
    "\n",
    "# â”€â”€ evaluation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(X, y, name=\"\"):\n",
    "    print(f\"ðŸ” Evaluating {name}...\")\n",
    "    proba = model.predict(X, verbose=0).ravel()\n",
    "    pred  = (proba >= THRESHOLD)\n",
    "    \n",
    "    # Handle edge cases for metrics\n",
    "    try:\n",
    "        precision = precision_score(y, pred, zero_division=0)\n",
    "        recall = recall_score(y, pred, zero_division=0)\n",
    "        f1 = f1_score(y, pred, zero_division=0)\n",
    "        f05 = f05_np(y, proba)\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # AUC calculation with error handling\n",
    "        fpr, tpr, _ = roc_curve(y, proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in metric calculation: {e}\")\n",
    "        precision = recall = f1 = f05 = acc = auc_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'proba': proba, \n",
    "        'pred': pred, \n",
    "        'y': y,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f05': f05,\n",
    "        'acc': acc,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "\n",
    "# Evaluate current model first\n",
    "train_res = evaluate(X_tr_w, y_tr_w, \"training set\")\n",
    "current_val_res = evaluate(X_va_w, y_va_w, \"current validation set\")\n",
    "\n",
    "# Print current model results\n",
    "print(f\"\\nðŸ“ˆ Current Model Training Results:\")\n",
    "print(f\"   Precision: {train_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {train_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {train_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {train_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {train_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {train_res['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Model Validation Results:\")\n",
    "print(f\"   Precision: {current_val_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {current_val_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {current_val_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {current_val_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {current_val_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {current_val_res['auc']:.4f}\")\n",
    "\n",
    "# â”€â”€ plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“ˆ Creating training plots â€¦\")\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# F0.5 plot\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(hist.history[\"f05\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_f05\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"F0.5\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(hist.history[\"accuracy\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_accuracy\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision plot\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(hist.history[\"precision_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Recall plot\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(hist.history[\"recall_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_recall_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Recall\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision-focused metric plot\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(hist.history[\"precision_focused\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_focused\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision-Focused Metric\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"training_curves_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â”€â”€ JSON summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving results summary â€¦\")\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "train_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                for k, v in train_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "\n",
    "# Use the best model results for validation metrics\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in final_model_results.items() if k not in ['proba', 'pred', 'name']}\n",
    "    best_model_info = {\"best_model\": best_model_name, \"selection_criteria\": \"highest_precision\"}\n",
    "else:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in current_val_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "    best_model_info = {\"best_model\": \"current_model\", \"selection_criteria\": \"final_epoch\"}\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": STAMP,\n",
    "    \"best_params\": BEST_PARAMS,\n",
    "    \"training_config\": {\n",
    "        \"epochs_trained\": len(hist.history[\"loss\"]),\n",
    "        \"max_epochs\": EPOCHS,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"lr_patience\": LR_PATIENCE,\n",
    "        \"precision_focused\": True\n",
    "    },\n",
    "    \"model_selection\": {\n",
    "        \"best_model\": best_model_name,\n",
    "        \"selection_criteria\": \"precision_focused_score\",\n",
    "        \"precision_focused_score\": float(final_score),\n",
    "        \"formula\": \"(3 * precision + recall) / 4\"\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"features\": n_feat,\n",
    "        \"train_samples\": len(X_tr_w),\n",
    "        \"val_samples\": len(X_va_w)\n",
    "    },\n",
    "    \"class_weights\": {\n",
    "        \"W0\": float(W0),\n",
    "        \"W1\": float(W1),\n",
    "        \"positive_rate\": float(pos_rate),\n",
    "        \"strategy\": \"precision_focused\"\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"train\": train_metrics,\n",
    "        \"val\": val_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all model comparisons if available\n",
    "if 'model_results' in locals():\n",
    "    all_model_metrics = {}\n",
    "    for model_name, results in model_results.items():\n",
    "        if results:\n",
    "            all_model_metrics[model_name] = {k: float(v) if np.isscalar(v) else v \n",
    "                                           for k, v in results.items() \n",
    "                                           if k not in ['proba', 'pred', 'name']}\n",
    "    results_summary[\"all_models_comparison\"] = all_model_metrics\n",
    "\n",
    "with open(OUT_DIR / f\"results_{STAMP}.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# â”€â”€ validation-prediction CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving validation predictions â€¦\")\n",
    "\n",
    "# Calculate the correct timestamp indices\n",
    "first_val_row = val_start + WIN\n",
    "end_val_row = first_val_row + len(y_va_w)\n",
    "\n",
    "# Ensure we don't go beyond the dataframe bounds\n",
    "end_val_row = min(end_val_row, len(df))\n",
    "timestamps = df.index.to_numpy()[first_val_row:end_val_row]\n",
    "\n",
    "# Ensure lengths match\n",
    "min_len = min(len(timestamps), len(val_res[\"proba\"]), len(y_va_w))\n",
    "timestamps = timestamps[:min_len]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"prob_up\": val_res[\"proba\"][:min_len],\n",
    "    \"prob_down\": 1.0 - val_res[\"proba\"][:min_len],\n",
    "    \"winning_prob\": np.maximum(val_res[\"proba\"][:min_len], 1.0 - val_res[\"proba\"][:min_len]),\n",
    "    \"prediction\": val_res[\"pred\"][:min_len].astype(int),\n",
    "    \"actual\": y_va_w[:min_len],\n",
    "})\n",
    "\n",
    "# Round probabilities\n",
    "val_df[[\"prob_up\", \"prob_down\", \"winning_prob\"]] = val_df[\n",
    "    [\"prob_up\", \"prob_down\", \"winning_prob\"]].round(6)\n",
    "\n",
    "csv_path = OUT_DIR / f\"cnn_lstm_val_preds_{STAMP}.csv\"\n",
    "val_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV saved â†’ {csv_path}  ({len(val_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved in: {OUT_DIR}\")\n",
    "print(f\"ðŸ·ï¸  Timestamp: {STAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b684bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving validation predictions â€¦\n",
      "ðŸ“Š Using predictions from: Current Model (fallback)\n",
      "âœ… CSV saved â†’ model_outputs\\cnn_lstm_val_preds_20250614_143306.csv  (3,105 rows)\n",
      "\n",
      "âœ… Training completed successfully!\n",
      "ðŸ“ All artifacts saved in: model_outputs\n",
      "ðŸ·ï¸  Timestamp: 20250614_143306\n",
      "ðŸŽ¯ Final model selection: Current Model\n",
      "\n",
      "ðŸ”§ IMPROVEMENTS IMPLEMENTED:\n",
      "   âœ… Increased epochs: 200 â†’ 500\n",
      "   âœ… Increased patience: 20 â†’ 50\n",
      "   âœ… More gradual LR reduction: factor 0.5 â†’ 0.7\n",
      "   âœ… Precision-focused class weighting (30% reduction in positive weight)\n",
      "   âœ… Smart model selection using precision-focused score: (3Ã—P + R)/4\n",
      "   âœ… Multiple model checkpoints (precision, F0.5, precision-focused)\n",
      "   âœ… Balanced precision-recall optimization (avoids useless high-P/low-R models)\n",
      "   âœ… Comprehensive model comparison and evaluation\n",
      "ðŸ“Š Loading data â€¦\n",
      "Data shape: (15855, 30), Features: 29\n",
      "Train size: 12684, Val size: 3171\n",
      "Positive rate: 0.508, Class weights: W0=1.000, W1=0.678 (precision-focused)\n",
      "Windowed train shape: (12618, 66, 29), val shape: (3105, 66, 29)\n",
      "ðŸ—ï¸  Building model â€¦\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 311\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# â”€â”€ datasets & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ—ï¸  Building model â€¦\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBEST_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel built with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mcount_params()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    314\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_tr_w, y_tr_w))\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10_000\u001b[39m, seed\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;241m.\u001b[39mbatch(BEST_PARAMS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    317\u001b[0m             \u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE))\n",
      "Cell \u001b[1;32mIn[12], line 278\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m    275\u001b[0m w0_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(W0, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    276\u001b[0m w1_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(W1, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;129m@keras\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241m.\u001b[39mregister_keras_serializable()\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcustom_weighted_bce\u001b[39m(y_t, y_p):\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom weighted binary crossentropy with current class weights\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     y_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_t, tf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "# â”€â”€ validation-prediction CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving validation predictions â€¦\")\n",
    "\n",
    "# Calculate the correct timestamp indices\n",
    "first_val_row = val_start + WIN\n",
    "end_val_row = first_val_row + len(y_va_w)\n",
    "\n",
    "# Ensure we don't go beyond the dataframe bounds\n",
    "end_val_row = min(end_val_row, len(df))\n",
    "timestamps = df.index.to_numpy()[first_val_row:end_val_row]\n",
    "\n",
    "# Use the best model's predictions if available\n",
    "if 'final_model_results' in locals() and final_model_results and 'proba' in final_model_results:\n",
    "    best_proba = final_model_results['proba']\n",
    "    best_pred = final_model_results['pred']\n",
    "    model_used = best_model_name\n",
    "    print(f\"ðŸ“Š Using predictions from: {best_model_name}\")\n",
    "else:\n",
    "    # Fallback: generate predictions from current model\n",
    "    print(\"ðŸ“Š Using predictions from: Current Model (fallback)\")\n",
    "    best_proba = model.predict(X_va_w, verbose=0).ravel()\n",
    "    best_pred = (best_proba >= THRESHOLD).astype(int)\n",
    "    model_used = \"Current Model\"\n",
    "\n",
    "# Ensure lengths match\n",
    "min_len = min(len(timestamps), len(best_proba), len(y_va_w))\n",
    "timestamps = timestamps[:min_len]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"prob_up\": best_proba[:min_len],\n",
    "    \"prob_down\": 1.0 - best_proba[:min_len],\n",
    "    \"winning_prob\": np.maximum(best_proba[:min_len], 1.0 - best_proba[:min_len]),\n",
    "    \"prediction\": best_pred[:min_len].astype(int),\n",
    "    \"actual\": y_va_w[:min_len],\n",
    "    \"model_used\": model_used\n",
    "})\n",
    "\n",
    "# Round probabilities\n",
    "val_df[[\"prob_up\", \"prob_down\", \"winning_prob\"]] = val_df[\n",
    "    [\"prob_up\", \"prob_down\", \"winning_prob\"]].round(6)\n",
    "\n",
    "csv_path = OUT_DIR / f\"cnn_lstm_val_preds_{STAMP}.csv\"\n",
    "val_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV saved â†’ {csv_path}  ({len(val_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved in: {OUT_DIR}\")\n",
    "print(f\"ðŸ·ï¸  Timestamp: {STAMP}\")\n",
    "print(f\"ðŸŽ¯ Final model selection: {model_used}\")\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    print(f\"ðŸŽ¯ Final precision-focused score: {final_score:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final precision: {final_model_results['precision']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final recall: {final_model_results['recall']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final F0.5: {final_model_results['f05']:.4f}\")\n",
    "\n",
    "# Summary of improvements made\n",
    "print(f\"\\nðŸ”§ IMPROVEMENTS IMPLEMENTED:\")\n",
    "print(f\"   âœ… Increased epochs: 200 â†’ 500\")\n",
    "print(f\"   âœ… Increased patience: 20 â†’ 50\") \n",
    "print(f\"   âœ… More gradual LR reduction: factor 0.5 â†’ 0.7\")\n",
    "print(f\"   âœ… Precision-focused class weighting (30% reduction in positive weight)\")\n",
    "print(f\"   âœ… Smart model selection using precision-focused score: (3Ã—P + R)/4\")\n",
    "print(f\"   âœ… Multiple model checkpoints (precision, F0.5, precision-focused)\")\n",
    "print(f\"   âœ… Balanced precision-recall optimization (avoids useless high-P/low-R models)\")\n",
    "print(f\"   âœ… Comprehensive model comparison and evaluation\")# =============================================================================\n",
    "#  CNNâ€“LSTM  (80-20 split)  â€“  Optimise F0Â·5  â€“  Export validation CSV\n",
    "# =============================================================================\n",
    "import os, json, warnings, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             fbeta_score, accuracy_score, roc_curve, auc)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping,\n",
    "                                        ReduceLROnPlateau)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# â”€â”€ Optuna-best hyper-params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BEST_PARAMS = {\n",
    "    \"arch\":        \"conv_lstm\",\n",
    "    \"window\":      66,\n",
    "    \"filters\":     32,\n",
    "    \"kernel\":      5,\n",
    "    \"units\":       32,\n",
    "    \"conv_drop\":   0.1990,\n",
    "    \"lstm_drop\":   0.0056,\n",
    "    \"dense\":       32,\n",
    "    \"dropout\":     0.3674,\n",
    "    \"l2\":          1.73e-06,\n",
    "    \"lr\":          5.455e-05,\n",
    "    \"batch\":       32,\n",
    "    \"act\":         \"relu\",\n",
    "    \"conv_blocks\": 2,\n",
    "    \"optim\":       \"nadam\",\n",
    "}\n",
    "\n",
    "# â”€â”€ paths & columns to drop (truncate if you like) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open','high','low','typical_price','EMA_21','SMA_20','vwap_24h','close_4h',\n",
    "    'bollinger_upper','bollinger_lower','resistance_level','support_level',\n",
    "    'high_low','high_close','low_close','true_range','volume_mean_20',\n",
    "    'MACD_line','MACD_signal','volatility_regime','trending_market',\n",
    "    'above_sma50','ema7_above_ema21','rsi_overbought','stoch_oversold',\n",
    "    'cci_oversold','vol_spike_1_5x','near_upper_band','near_lower_band',\n",
    "    'break_upper_band','break_lower_band','rsi_oversold','above_sma20',\n",
    "    'macd_positive','volume_breakout','volume_breakdown','bullish_scenario_1',\n",
    "    'bullish_scenario_2','bullish_scenario_3','bullish_scenario_4',\n",
    "    'bullish_scenario_5','bullish_scenario_6','bearish_scenario_1',\n",
    "    'bearish_scenario_2','bearish_scenario_3','bearish_scenario_4',\n",
    "    'bearish_scenario_6','ema_cross_up','macd_cross_up','oversold_reversal',\n",
    "    'overbought_reversal','close'\n",
    "]\n",
    "\n",
    "# â”€â”€ run parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED       = 42\n",
    "VAL_FRAC   = 0.20\n",
    "BETA       = 0.5\n",
    "THRESHOLD  = 0.5\n",
    "EPOCHS     = 500  # Increased epochs\n",
    "PATIENCE   = 50   # Much more patience\n",
    "LR_PATIENCE = 25  # More patience for LR reduction\n",
    "\n",
    "OUT_DIR = Path(\"model_outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "STAMP   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception: pass\n",
    "\n",
    "# â”€â”€ load & clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Loading data â€¦\")\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "\n",
    "# Filter DROP_COLS to only include columns that actually exist in df\n",
    "existing_drop_cols = [c for c in DROP_COLS if c in df.columns]\n",
    "df = df.drop(columns=existing_drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(np.int8).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "print(f\"Data shape: {df.shape}, Features: {n_feat}\")\n",
    "\n",
    "# â”€â”€ 80-20 chronological split (no test slice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_start = int(len(df) * (1 - VAL_FRAC))\n",
    "train_idx = np.arange(0, val_start)\n",
    "val_idx   = np.arange(val_start, len(df))\n",
    "\n",
    "scaler = StandardScaler().fit(X_raw[train_idx])\n",
    "joblib.dump(scaler, OUT_DIR / f\"scaler_{STAMP}.pkl\")\n",
    "\n",
    "X_train = scaler.transform(X_raw[train_idx])\n",
    "X_val   = scaler.transform(X_raw[val_idx])\n",
    "y_train, y_val = y_raw[train_idx], y_raw[val_idx]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "\n",
    "# â”€â”€ class weighting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pos_rate = y_train.mean()\n",
    "# Stronger emphasis on precision - reduce positive class weight\n",
    "W0, W1 = 1.0, ((1 - pos_rate) / pos_rate * 0.7) if pos_rate > 0 else 1.0  # Reduced by 30%\n",
    "print(f\"Positive rate: {pos_rate:.3f}, Class weights: W0={W0:.3f}, W1={W1:.3f} (precision-focused)\")\n",
    "\n",
    "# â”€â”€ window helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, w: int) -> Tuple[np.ndarray,np.ndarray]:\n",
    "    xs, ys = [], []\n",
    "    for i in range(w, len(X)):\n",
    "        xs.append(X[i-w:i]); ys.append(y[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "WIN = BEST_PARAMS[\"window\"]\n",
    "X_tr_w, y_tr_w = make_windows(X_train, y_train, WIN)\n",
    "X_va_w, y_va_w = make_windows(X_val,   y_val,   WIN)\n",
    "\n",
    "print(f\"Windowed train shape: {X_tr_w.shape}, val shape: {X_va_w.shape}\")\n",
    "\n",
    "# â”€â”€ offline F0Â·5 helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f05_np(y_true, proba):\n",
    "    pred = (proba >= THRESHOLD)\n",
    "    return fbeta_score(y_true, pred, beta=BETA, zero_division=0)\n",
    "\n",
    "# â”€â”€ Custom Precision-Focused Metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class PrecisionFocusedMetric(keras.metrics.Metric):\n",
    "    \"\"\"Custom metric that weighs precision 3x more than recall\"\"\"\n",
    "    def __init__(self, name=\"precision_focused\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        # Weight precision 3x more than recall: 3*P + R / 4\n",
    "        return (3 * prec + rec) / 4\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "# â”€â”€ graph-safe F0Â·5 metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class F05Metric(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f05\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.beta_sq = BETA ** 2\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        return (1 + self.beta_sq) * prec * rec / (self.beta_sq * prec + rec + 1e-9)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "\n",
    "# â”€â”€ model builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "    x   = inp\n",
    "    \n",
    "    for _ in range(cfg[\"conv_blocks\"]):\n",
    "        x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                          activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "        x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "    \n",
    "    x = layers.LSTM(cfg[\"units\"], dropout=cfg[\"lstm_drop\"],\n",
    "                    kernel_regularizer=l2)(x)\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                     kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    # weighted BCE (dtype-safe) - create a proper closure with current W0, W1\n",
    "    w0_tf = tf.constant(W0, tf.float32)\n",
    "    w1_tf = tf.constant(W1, tf.float32)\n",
    "    \n",
    "    @keras.saving.register_keras_serializable()\n",
    "    def custom_weighted_bce(y_t, y_p):\n",
    "        \"\"\"Custom weighted binary crossentropy with current class weights\"\"\"\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        w = tf.where(tf.equal(y_t, 1.0), w1_tf, w0_tf)\n",
    "        return tf.reduce_mean(w * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    # Optimizer selection\n",
    "    opt_cls = {\n",
    "        \"nadam\": keras.optimizers.Nadam,\n",
    "        \"adamw\": keras.optimizers.AdamW,\n",
    "        \"rmsprop\": keras.optimizers.RMSprop\n",
    "    }.get(cfg[\"optim\"], keras.optimizers.Adam)\n",
    "    \n",
    "    if cfg[\"optim\"] == \"adamw\":\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "    else:\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=custom_weighted_bce,\n",
    "        metrics=[PrecisionFocusedMetric(),\n",
    "                F05Metric(), \n",
    "                keras.metrics.Precision(),\n",
    "                keras.metrics.Recall(), \n",
    "                keras.metrics.AUC(), \n",
    "                \"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# â”€â”€ datasets & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ—ï¸  Building model â€¦\")\n",
    "model = build_model(BEST_PARAMS)\n",
    "print(f\"Model built with {model.count_params():,} parameters\")\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((X_tr_w, y_tr_w))\n",
    "            .shuffle(10_000, seed=SEED)\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds   = (tf.data.Dataset.from_tensor_slices((X_va_w, y_va_w))\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Create multiple checkpoints for different metrics\n",
    "best_precision_ckpt = OUT_DIR / f\"best_precision_{STAMP}.keras\"\n",
    "best_f05_ckpt = OUT_DIR / f\"best_f05_{STAMP}.keras\"\n",
    "best_prec_focused_ckpt = OUT_DIR / f\"best_precision_focused_{STAMP}.keras\"\n",
    "\n",
    "cbs  = [\n",
    "    # Save best models for different metrics\n",
    "    ModelCheckpoint(best_precision_ckpt, monitor=\"val_precision_1\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_f05_ckpt, monitor=\"val_f05\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_prec_focused_ckpt, monitor=\"val_precision_focused\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    \n",
    "    # Early stopping based on precision-focused metric (more lenient)\n",
    "    EarlyStopping(monitor=\"val_precision_focused\", mode=\"max\", patience=PATIENCE,\n",
    "                  restore_best_weights=False, verbose=1),  # Don't restore, we'll choose best manually\n",
    "    \n",
    "    # Reduce LR more gradually\n",
    "    ReduceLROnPlateau(monitor=\"val_precision_focused\", mode=\"max\", patience=LR_PATIENCE,\n",
    "                      factor=0.7, min_lr=1e-8, verbose=1)  # Less aggressive reduction\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Starting training â€¦\")\n",
    "hist = model.fit(train_ds, validation_data=val_ds,\n",
    "                 epochs=EPOCHS, callbacks=cbs, verbose=1)\n",
    "\n",
    "model.save(OUT_DIR / f\"final_model_{STAMP}.keras\")\n",
    "\n",
    "# â”€â”€ Evaluate all saved models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_model(model_path, name):\n",
    "    \"\"\"Evaluate a specific model and return results\"\"\"\n",
    "    if not model_path.exists():\n",
    "        print(f\"âŒ {name} model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ” Evaluating {name} model...\")\n",
    "    temp_model = keras.models.load_model(model_path, \n",
    "                                       custom_objects={'F05Metric': F05Metric, \n",
    "                                                     'PrecisionFocusedMetric': PrecisionFocusedMetric})\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_proba = temp_model.predict(X_va_w, verbose=0).ravel()\n",
    "    val_pred = (val_proba >= THRESHOLD)\n",
    "    \n",
    "    try:\n",
    "        precision = precision_score(y_va_w, val_pred, zero_division=0)\n",
    "        recall = recall_score(y_va_w, val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_va_w, val_pred, zero_division=0)\n",
    "        f05 = f05_np(y_va_w, val_proba)\n",
    "        acc = accuracy_score(y_va_w, val_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_va_w, val_proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        results = {\n",
    "            'name': name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'f05': f05,\n",
    "            'acc': acc,\n",
    "            'auc': auc_score,\n",
    "            'proba': val_proba,\n",
    "            'pred': val_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"   ðŸ“Š {name} Results:\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall:    {recall:.4f}\")\n",
    "        print(f\"      F1:        {f1:.4f}\")\n",
    "        print(f\"      F0.5:      {f05:.4f}\")\n",
    "        print(f\"      Accuracy:  {acc:.4f}\")\n",
    "        print(f\"      AUC:       {auc_score:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error evaluating {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate all model variants\n",
    "model_results = {}\n",
    "model_results['precision'] = evaluate_model(best_precision_ckpt, \"Best Precision\")\n",
    "model_results['f05'] = evaluate_model(best_f05_ckpt, \"Best F0.5\")\n",
    "model_results['precision_focused'] = evaluate_model(best_prec_focused_ckpt, \"Best Precision-Focused\")\n",
    "\n",
    "# Calculate precision-focused score for each model (3x precision + recall) / 4\n",
    "def calculate_precision_focused_score(results):\n",
    "    \"\"\"Calculate the precision-focused score: (3*precision + recall) / 4\"\"\"\n",
    "    if not results or results['precision'] == 0:\n",
    "        return 0\n",
    "    return (3 * results['precision'] + results['recall']) / 4\n",
    "\n",
    "# Find the best model based on precision-focused score\n",
    "valid_results = [r for r in model_results.values() if r and r['precision'] > 0]\n",
    "if valid_results:\n",
    "    best_precision_focused_model = max(valid_results, key=calculate_precision_focused_score)\n",
    "    \n",
    "    # Also find individual bests for comparison\n",
    "    best_precision_model = max(valid_results, key=lambda x: x['precision'])\n",
    "    best_f05_model = max(valid_results, key=lambda x: x['f05'])\n",
    "    best_recall_model = max(valid_results, key=lambda x: x['recall'])\n",
    "    \n",
    "    print(f\"\\nðŸ† MODELS COMPARISON:\")\n",
    "    print(f\"ðŸŽ¯ Highest Precision-Focused Score: {best_precision_focused_model['name']}\")\n",
    "    print(f\"   Score: {calculate_precision_focused_score(best_precision_focused_model):.4f}\")\n",
    "    print(f\"   Precision: {best_precision_focused_model['precision']:.4f}\")\n",
    "    print(f\"   Recall: {best_precision_focused_model['recall']:.4f}\")\n",
    "    print(f\"   F0.5: {best_precision_focused_model['f05']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š For comparison:\")\n",
    "    print(f\"   Highest Precision Only: {best_precision_model['name']} (P:{best_precision_model['precision']:.4f}, R:{best_precision_model['recall']:.4f})\")\n",
    "    print(f\"   Highest F0.5: {best_f05_model['name']} (P:{best_f05_model['precision']:.4f}, R:{best_f05_model['recall']:.4f})\")\n",
    "    print(f\"   Highest Recall: {best_recall_model['name']} (P:{best_recall_model['precision']:.4f}, R:{best_recall_model['recall']:.4f})\")\n",
    "    \n",
    "    # Use the precision-focused model as final choice\n",
    "    final_model_results = best_precision_focused_model\n",
    "    best_model_name = best_precision_focused_model['name']\n",
    "    final_score = calculate_precision_focused_score(best_precision_focused_model)\n",
    "    \n",
    "    print(f\"\\nâœ… SELECTED: {best_model_name}\")\n",
    "    print(f\"   Reason: Highest precision-focused score (3Ã—P + R)/4 = {final_score:.4f}\")\n",
    "    print(f\"   This balances high precision ({best_precision_focused_model['precision']:.4f}) with reasonable recall ({best_precision_focused_model['recall']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    # Fallback to current model\n",
    "    current_val_eval = evaluate(X_va_w, y_va_w, \"current model fallback\")\n",
    "    final_model_results = current_val_eval\n",
    "    best_model_name = \"Current Model (Fallback)\"\n",
    "    final_score = calculate_precision_focused_score(current_val_eval)\n",
    "    print(f\"\\nâš ï¸  Using fallback current model (precision-focused score: {final_score:.4f})\")\n",
    "\n",
    "# â”€â”€ evaluation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(X, y, name=\"\"):\n",
    "    print(f\"ðŸ” Evaluating {name}...\")\n",
    "    proba = model.predict(X, verbose=0).ravel()\n",
    "    pred  = (proba >= THRESHOLD)\n",
    "    \n",
    "    # Handle edge cases for metrics\n",
    "    try:\n",
    "        precision = precision_score(y, pred, zero_division=0)\n",
    "        recall = recall_score(y, pred, zero_division=0)\n",
    "        f1 = f1_score(y, pred, zero_division=0)\n",
    "        f05 = f05_np(y, proba)\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # AUC calculation with error handling\n",
    "        fpr, tpr, _ = roc_curve(y, proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in metric calculation: {e}\")\n",
    "        precision = recall = f1 = f05 = acc = auc_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'proba': proba, \n",
    "        'pred': pred, \n",
    "        'y': y,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f05': f05,\n",
    "        'acc': acc,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "\n",
    "# Evaluate current model first\n",
    "train_res = evaluate(X_tr_w, y_tr_w, \"training set\")\n",
    "current_val_res = evaluate(X_va_w, y_va_w, \"current validation set\")\n",
    "\n",
    "# Print current model results\n",
    "print(f\"\\nðŸ“ˆ Current Model Training Results:\")\n",
    "print(f\"   Precision: {train_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {train_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {train_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {train_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {train_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {train_res['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Model Validation Results:\")\n",
    "print(f\"   Precision: {current_val_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {current_val_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {current_val_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {current_val_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {current_val_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {current_val_res['auc']:.4f}\")\n",
    "\n",
    "# â”€â”€ plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“ˆ Creating training plots â€¦\")\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# F0.5 plot\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(hist.history[\"f05\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_f05\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"F0.5\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(hist.history[\"accuracy\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_accuracy\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision plot\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(hist.history[\"precision_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Recall plot\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(hist.history[\"recall_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_recall_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Recall\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision-focused metric plot\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(hist.history[\"precision_focused\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_focused\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision-Focused Metric\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"training_curves_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â”€â”€ JSON summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving results summary â€¦\")\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "train_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                for k, v in train_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "\n",
    "# Use the best model results for validation metrics\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in final_model_results.items() if k not in ['proba', 'pred', 'name']}\n",
    "    best_model_info = {\"best_model\": best_model_name, \"selection_criteria\": \"highest_precision\"}\n",
    "else:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in current_val_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "    best_model_info = {\"best_model\": \"current_model\", \"selection_criteria\": \"final_epoch\"}\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": STAMP,\n",
    "    \"best_params\": BEST_PARAMS,\n",
    "    \"training_config\": {\n",
    "        \"epochs_trained\": len(hist.history[\"loss\"]),\n",
    "        \"max_epochs\": EPOCHS,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"lr_patience\": LR_PATIENCE,\n",
    "        \"precision_focused\": True\n",
    "    },\n",
    "    \"model_selection\": {\n",
    "        \"best_model\": best_model_name,\n",
    "        \"selection_criteria\": \"precision_focused_score\",\n",
    "        \"precision_focused_score\": float(final_score),\n",
    "        \"formula\": \"(3 * precision + recall) / 4\"\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"features\": n_feat,\n",
    "        \"train_samples\": len(X_tr_w),\n",
    "        \"val_samples\": len(X_va_w)\n",
    "    },\n",
    "    \"class_weights\": {\n",
    "        \"W0\": float(W0),\n",
    "        \"W1\": float(W1),\n",
    "        \"positive_rate\": float(pos_rate),\n",
    "        \"strategy\": \"precision_focused\"\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"train\": train_metrics,\n",
    "        \"val\": val_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all model comparisons if available\n",
    "if 'model_results' in locals():\n",
    "    all_model_metrics = {}\n",
    "    for model_name, results in model_results.items():\n",
    "        if results:\n",
    "            all_model_metrics[model_name] = {k: float(v) if np.isscalar(v) else v \n",
    "                                           for k, v in results.items() \n",
    "                                           if k not in ['proba', 'pred', 'name']}\n",
    "    results_summary[\"all_models_comparison\"] = all_model_metrics\n",
    "\n",
    "with open(OUT_DIR / f\"results_{STAMP}.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# â”€â”€ validation-prediction CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving validation predictions â€¦\")\n",
    "\n",
    "# Calculate the correct timestamp indices\n",
    "first_val_row = val_start + WIN\n",
    "end_val_row = first_val_row + len(y_va_w)\n",
    "\n",
    "# Ensure we don't go beyond the dataframe bounds\n",
    "end_val_row = min(end_val_row, len(df))\n",
    "timestamps = df.index.to_numpy()[first_val_row:end_val_row]\n",
    "\n",
    "# Ensure lengths match\n",
    "min_len = min(len(timestamps), len(val_res[\"proba\"]), len(y_va_w))\n",
    "timestamps = timestamps[:min_len]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"prob_up\": val_res[\"proba\"][:min_len],\n",
    "    \"prob_down\": 1.0 - val_res[\"proba\"][:min_len],\n",
    "    \"winning_prob\": np.maximum(val_res[\"proba\"][:min_len], 1.0 - val_res[\"proba\"][:min_len]),\n",
    "    \"prediction\": val_res[\"pred\"][:min_len].astype(int),\n",
    "    \"actual\": y_va_w[:min_len],\n",
    "})\n",
    "\n",
    "# Round probabilities\n",
    "val_df[[\"prob_up\", \"prob_down\", \"winning_prob\"]] = val_df[\n",
    "    [\"prob_up\", \"prob_down\", \"winning_prob\"]].round(6)\n",
    "\n",
    "csv_path = OUT_DIR / f\"cnn_lstm_val_preds_{STAMP}.csv\"\n",
    "val_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV saved â†’ {csv_path}  ({len(val_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved in: {OUT_DIR}\")\n",
    "print(f\"ðŸ·ï¸  Timestamp: {STAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "468eaf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation at threshold 0.5:\n",
      "Precision: 0.531\n",
      "Recall   : 0.607\n",
      "F1 Score : 0.566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the predictions CSV\n",
    "csv_path =r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Final_runs_csv\\cnn_lstm_val_preds_20250614_142329.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names are correct and lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Extract actual and predicted values\n",
    "y_true = df['actual']\n",
    "y_pred = df['prediction']  # prediction at threshold 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ“Š Evaluation at threshold 0.5:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1 Score : {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c58bce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading data â€¦\n",
      "Data shape: (15855, 30), Features: 29\n",
      "Train size: 12684, Val size: 3171\n",
      "Positive rate: 0.508, Class weights: W0=1.000, W1=0.678 (precision-focused)\n",
      "Windowed train shape: (12618, 66, 29), val shape: (3105, 66, 29)\n",
      "ðŸ—ï¸  Building model â€¦\n",
      "Model built with 19,233 parameters\n",
      "ðŸš€ Starting UNLIMITED training (1000 epochs, no early stopping)â€¦\n",
      "Epoch 1/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5032 - auc_5: 0.4987 - f05: 0.5010 - loss: 0.5867 - precision_5: 0.5083 - precision_focused: 0.5008 - recall_5: 0.5008\n",
      "Epoch 1: val_f05 improved from -inf to 0.46238, saving model to model_outputs\\best_f05_20250614_150426.keras\n",
      "\n",
      "Epoch 1: val_precision_focused improved from -inf to 0.46384, saving model to model_outputs\\best_precision_focused_20250614_150426.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.5032 - auc_5: 0.4987 - f05: 0.5012 - loss: 0.5867 - precision_5: 0.5084 - precision_focused: 0.5010 - recall_5: 0.5012 - val_accuracy: 0.5002 - val_auc_5: 0.5049 - val_f05: 0.4624 - val_loss: 0.5782 - val_precision_5: 0.5295 - val_precision_focused: 0.4638 - val_recall_5: 0.3765 - learning_rate: 5.4550e-05\n",
      "Epoch 2/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5041 - auc_5: 0.5048 - f05: 0.5301 - loss: 0.5797 - precision_5: 0.5186 - precision_focused: 0.5355 - recall_5: 0.5929\n",
      "Epoch 2: val_f05 did not improve from 0.46238\n",
      "\n",
      "Epoch 2: val_precision_focused did not improve from 0.46384\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5042 - auc_5: 0.5049 - f05: 0.5300 - loss: 0.5797 - precision_5: 0.5186 - precision_focused: 0.5355 - recall_5: 0.5931 - val_accuracy: 0.4934 - val_auc_5: 0.5043 - val_f05: 0.4420 - val_loss: 0.5784 - val_precision_5: 0.5238 - val_precision_focused: 0.4499 - val_recall_5: 0.3198 - learning_rate: 5.4550e-05\n",
      "Epoch 3/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4944 - auc_5: 0.5042 - f05: 0.5098 - loss: 0.5804 - precision_5: 0.5075 - precision_focused: 0.5096 - recall_5: 0.4956\n",
      "Epoch 3: val_f05 did not improve from 0.46238\n",
      "\n",
      "Epoch 3: val_precision_focused did not improve from 0.46384\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.4945 - auc_5: 0.5042 - f05: 0.5098 - loss: 0.5804 - precision_5: 0.5075 - precision_focused: 0.5096 - recall_5: 0.4957 - val_accuracy: 0.4982 - val_auc_5: 0.5016 - val_f05: 0.4508 - val_loss: 0.5783 - val_precision_5: 0.5291 - val_precision_focused: 0.4552 - val_recall_5: 0.3475 - learning_rate: 5.4550e-05\n",
      "Epoch 4/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5079 - auc_5: 0.5138 - f05: 0.5086 - loss: 0.5805 - precision_5: 0.5156 - precision_focused: 0.5087 - recall_5: 0.5161\n",
      "Epoch 4: val_f05 improved from 0.46238 to 0.47820, saving model to model_outputs\\best_f05_20250614_150426.keras\n",
      "\n",
      "Epoch 4: val_precision_focused improved from 0.46384 to 0.47670, saving model to model_outputs\\best_precision_focused_20250614_150426.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5079 - auc_5: 0.5138 - f05: 0.5086 - loss: 0.5805 - precision_5: 0.5156 - precision_focused: 0.5087 - recall_5: 0.5161 - val_accuracy: 0.4918 - val_auc_5: 0.4992 - val_f05: 0.4782 - val_loss: 0.5779 - val_precision_5: 0.5158 - val_precision_focused: 0.4767 - val_recall_5: 0.4241 - learning_rate: 5.4550e-05\n",
      "Epoch 5/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5110 - auc_5: 0.5184 - f05: 0.5179 - loss: 0.5799 - precision_5: 0.5160 - precision_focused: 0.5224 - recall_5: 0.5816\n",
      "Epoch 5: val_f05 did not improve from 0.47820\n",
      "\n",
      "Epoch 5: val_precision_focused did not improve from 0.47670\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5110 - auc_5: 0.5184 - f05: 0.5179 - loss: 0.5799 - precision_5: 0.5160 - precision_focused: 0.5224 - recall_5: 0.5816 - val_accuracy: 0.4918 - val_auc_5: 0.5019 - val_f05: 0.4725 - val_loss: 0.5780 - val_precision_5: 0.5164 - val_precision_focused: 0.4715 - val_recall_5: 0.4080 - learning_rate: 5.4550e-05\n",
      "Epoch 6/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5085 - auc_5: 0.5173 - f05: 0.5202 - loss: 0.5794 - precision_5: 0.5178 - precision_focused: 0.5239 - recall_5: 0.5730\n",
      "Epoch 6: val_f05 improved from 0.47820 to 0.49275, saving model to model_outputs\\best_f05_20250614_150426.keras\n",
      "\n",
      "Epoch 6: val_precision_focused improved from 0.47670 to 0.49125, saving model to model_outputs\\best_precision_focused_20250614_150426.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5085 - auc_5: 0.5173 - f05: 0.5202 - loss: 0.5794 - precision_5: 0.5178 - precision_focused: 0.5239 - recall_5: 0.5730 - val_accuracy: 0.4973 - val_auc_5: 0.5073 - val_f05: 0.4927 - val_loss: 0.5773 - val_precision_5: 0.5199 - val_precision_focused: 0.4913 - val_recall_5: 0.4765 - learning_rate: 5.4550e-05\n",
      "Epoch 7/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5251 - auc_5: 0.5251 - f05: 0.5147 - loss: 0.5801 - precision_5: 0.5277 - precision_focused: 0.5174 - recall_5: 0.5717\n",
      "Epoch 7: val_f05 improved from 0.49275 to 0.50772, saving model to model_outputs\\best_f05_20250614_150426.keras\n",
      "\n",
      "Epoch 7: val_precision_focused improved from 0.49125 to 0.50901, saving model to model_outputs\\best_precision_focused_20250614_150426.keras\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5251 - auc_5: 0.5251 - f05: 0.5146 - loss: 0.5801 - precision_5: 0.5277 - precision_focused: 0.5174 - recall_5: 0.5716 - val_accuracy: 0.5031 - val_auc_5: 0.5083 - val_f05: 0.5077 - val_loss: 0.5770 - val_precision_5: 0.5227 - val_precision_focused: 0.5090 - val_recall_5: 0.5463 - learning_rate: 5.4550e-05\n",
      "Epoch 8/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5156 - auc_5: 0.5206 - f05: 0.5366 - loss: 0.5774 - precision_5: 0.5259 - precision_focused: 0.5459 - recall_5: 0.6419\n",
      "Epoch 8: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 8: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5156 - auc_5: 0.5206 - f05: 0.5366 - loss: 0.5774 - precision_5: 0.5258 - precision_focused: 0.5458 - recall_5: 0.6418 - val_accuracy: 0.5105 - val_auc_5: 0.5126 - val_f05: 0.4684 - val_loss: 0.5774 - val_precision_5: 0.5414 - val_precision_focused: 0.4685 - val_recall_5: 0.4037 - learning_rate: 5.4550e-05\n",
      "Epoch 9/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5112 - auc_5: 0.5158 - f05: 0.5156 - loss: 0.5788 - precision_5: 0.5231 - precision_focused: 0.5166 - recall_5: 0.5400\n",
      "Epoch 9: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 9: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5112 - auc_5: 0.5158 - f05: 0.5155 - loss: 0.5788 - precision_5: 0.5231 - precision_focused: 0.5166 - recall_5: 0.5399 - val_accuracy: 0.5050 - val_auc_5: 0.5118 - val_f05: 0.4668 - val_loss: 0.5775 - val_precision_5: 0.5349 - val_precision_focused: 0.4674 - val_recall_5: 0.3926 - learning_rate: 5.4550e-05\n",
      "Epoch 10/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5199 - auc_5: 0.5279 - f05: 0.5033 - loss: 0.5804 - precision_5: 0.5221 - precision_focused: 0.5035 - recall_5: 0.5237\n",
      "Epoch 10: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 10: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5199 - auc_5: 0.5280 - f05: 0.5034 - loss: 0.5804 - precision_5: 0.5221 - precision_focused: 0.5036 - recall_5: 0.5238 - val_accuracy: 0.4995 - val_auc_5: 0.5098 - val_f05: 0.4682 - val_loss: 0.5774 - val_precision_5: 0.5273 - val_precision_focused: 0.4683 - val_recall_5: 0.3938 - learning_rate: 5.4550e-05\n",
      "Epoch 11/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5307 - auc_5: 0.5403 - f05: 0.5111 - loss: 0.5787 - precision_5: 0.5346 - precision_focused: 0.5125 - recall_5: 0.5595\n",
      "Epoch 11: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 11: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5306 - auc_5: 0.5402 - f05: 0.5111 - loss: 0.5787 - precision_5: 0.5345 - precision_focused: 0.5125 - recall_5: 0.5594 - val_accuracy: 0.4979 - val_auc_5: 0.5080 - val_f05: 0.4772 - val_loss: 0.5775 - val_precision_5: 0.5234 - val_precision_focused: 0.4760 - val_recall_5: 0.4210 - learning_rate: 5.4550e-05\n",
      "Epoch 12/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5202 - auc_5: 0.5330 - f05: 0.5144 - loss: 0.5787 - precision_5: 0.5269 - precision_focused: 0.5162 - recall_5: 0.5586\n",
      "Epoch 12: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 12: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5202 - auc_5: 0.5330 - f05: 0.5144 - loss: 0.5787 - precision_5: 0.5269 - precision_focused: 0.5162 - recall_5: 0.5586 - val_accuracy: 0.5002 - val_auc_5: 0.5078 - val_f05: 0.4784 - val_loss: 0.5775 - val_precision_5: 0.5261 - val_precision_focused: 0.4771 - val_recall_5: 0.4235 - learning_rate: 5.4550e-05\n",
      "Epoch 13/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5274 - auc_5: 0.5389 - f05: 0.5290 - loss: 0.5776 - precision_5: 0.5320 - precision_focused: 0.5366 - recall_5: 0.6330\n",
      "Epoch 13: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 13: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5274 - auc_5: 0.5389 - f05: 0.5290 - loss: 0.5776 - precision_5: 0.5319 - precision_focused: 0.5365 - recall_5: 0.6327 - val_accuracy: 0.5018 - val_auc_5: 0.5087 - val_f05: 0.4670 - val_loss: 0.5777 - val_precision_5: 0.5303 - val_precision_focused: 0.4672 - val_recall_5: 0.3944 - learning_rate: 5.4550e-05\n",
      "Epoch 14/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5243 - auc_5: 0.5313 - f05: 0.5197 - loss: 0.5788 - precision_5: 0.5282 - precision_focused: 0.5243 - recall_5: 0.5964\n",
      "Epoch 14: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 14: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5243 - auc_5: 0.5313 - f05: 0.5197 - loss: 0.5788 - precision_5: 0.5283 - precision_focused: 0.5243 - recall_5: 0.5963 - val_accuracy: 0.5002 - val_auc_5: 0.5129 - val_f05: 0.4655 - val_loss: 0.5776 - val_precision_5: 0.5283 - val_precision_focused: 0.4658 - val_recall_5: 0.3920 - learning_rate: 5.4550e-05\n",
      "Epoch 15/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5254 - auc_5: 0.5380 - f05: 0.5136 - loss: 0.5794 - precision_5: 0.5262 - precision_focused: 0.5178 - recall_5: 0.5871\n",
      "Epoch 15: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 15: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5254 - auc_5: 0.5379 - f05: 0.5136 - loss: 0.5794 - precision_5: 0.5262 - precision_focused: 0.5178 - recall_5: 0.5869 - val_accuracy: 0.4995 - val_auc_5: 0.5117 - val_f05: 0.4395 - val_loss: 0.5782 - val_precision_5: 0.5329 - val_precision_focused: 0.4467 - val_recall_5: 0.3296 - learning_rate: 5.4550e-05\n",
      "Epoch 16/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5238 - auc_5: 0.5357 - f05: 0.5175 - loss: 0.5774 - precision_5: 0.5315 - precision_focused: 0.5198 - recall_5: 0.5689\n",
      "Epoch 16: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 16: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5239 - auc_5: 0.5357 - f05: 0.5174 - loss: 0.5774 - precision_5: 0.5315 - precision_focused: 0.5197 - recall_5: 0.5688 - val_accuracy: 0.5047 - val_auc_5: 0.5140 - val_f05: 0.4560 - val_loss: 0.5777 - val_precision_5: 0.5367 - val_precision_focused: 0.4585 - val_recall_5: 0.3698 - learning_rate: 5.4550e-05\n",
      "Epoch 17/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5374 - auc_5: 0.5504 - f05: 0.5077 - loss: 0.5782 - precision_5: 0.5403 - precision_focused: 0.5089 - recall_5: 0.5573\n",
      "Epoch 17: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 17: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5374 - auc_5: 0.5503 - f05: 0.5077 - loss: 0.5782 - precision_5: 0.5403 - precision_focused: 0.5089 - recall_5: 0.5573 - val_accuracy: 0.5089 - val_auc_5: 0.5134 - val_f05: 0.4526 - val_loss: 0.5779 - val_precision_5: 0.5435 - val_precision_focused: 0.4560 - val_recall_5: 0.3660 - learning_rate: 5.4550e-05\n",
      "Epoch 18/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5288 - auc_5: 0.5332 - f05: 0.5142 - loss: 0.5790 - precision_5: 0.5343 - precision_focused: 0.5160 - recall_5: 0.5633\n",
      "Epoch 18: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 18: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.5288 - auc_5: 0.5333 - f05: 0.5141 - loss: 0.5790 - precision_5: 0.5343 - precision_focused: 0.5160 - recall_5: 0.5632 - val_accuracy: 0.5105 - val_auc_5: 0.5135 - val_f05: 0.4525 - val_loss: 0.5779 - val_precision_5: 0.5466 - val_precision_focused: 0.4565 - val_recall_5: 0.3623 - learning_rate: 5.4550e-05\n",
      "Epoch 19/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5386 - auc_5: 0.5528 - f05: 0.5235 - loss: 0.5761 - precision_5: 0.5427 - precision_focused: 0.5285 - recall_5: 0.6176\n",
      "Epoch 19: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 19: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5385 - auc_5: 0.5528 - f05: 0.5234 - loss: 0.5761 - precision_5: 0.5427 - precision_focused: 0.5284 - recall_5: 0.6174 - val_accuracy: 0.5011 - val_auc_5: 0.5171 - val_f05: 0.4200 - val_loss: 0.5786 - val_precision_5: 0.5418 - val_precision_focused: 0.4372 - val_recall_5: 0.2840 - learning_rate: 5.4550e-05\n",
      "Epoch 20/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5238 - auc_5: 0.5380 - f05: 0.5195 - loss: 0.5763 - precision_5: 0.5370 - precision_focused: 0.5209 - recall_5: 0.5602\n",
      "Epoch 20: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 20: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.5238 - auc_5: 0.5380 - f05: 0.5195 - loss: 0.5763 - precision_5: 0.5370 - precision_focused: 0.5209 - recall_5: 0.5602 - val_accuracy: 0.4998 - val_auc_5: 0.5168 - val_f05: 0.4139 - val_loss: 0.5788 - val_precision_5: 0.5410 - val_precision_focused: 0.4339 - val_recall_5: 0.2728 - learning_rate: 5.4550e-05\n",
      "Epoch 21/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5234 - auc_5: 0.5365 - f05: 0.5145 - loss: 0.5772 - precision_5: 0.5338 - precision_focused: 0.5151 - recall_5: 0.5414\n",
      "Epoch 21: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 21: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5234 - auc_5: 0.5366 - f05: 0.5145 - loss: 0.5772 - precision_5: 0.5338 - precision_focused: 0.5151 - recall_5: 0.5414 - val_accuracy: 0.5031 - val_auc_5: 0.5190 - val_f05: 0.4260 - val_loss: 0.5783 - val_precision_5: 0.5435 - val_precision_focused: 0.4404 - val_recall_5: 0.2969 - learning_rate: 5.4550e-05\n",
      "Epoch 22/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5296 - auc_5: 0.5412 - f05: 0.4939 - loss: 0.5798 - precision_5: 0.5315 - precision_focused: 0.4931 - recall_5: 0.5057\n",
      "Epoch 22: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 22: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5296 - auc_5: 0.5413 - f05: 0.4940 - loss: 0.5798 - precision_5: 0.5316 - precision_focused: 0.4932 - recall_5: 0.5057 - val_accuracy: 0.5037 - val_auc_5: 0.5139 - val_f05: 0.4627 - val_loss: 0.5779 - val_precision_5: 0.5338 - val_precision_focused: 0.4638 - val_recall_5: 0.3846 - learning_rate: 5.4550e-05\n",
      "Epoch 23/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5354 - auc_5: 0.5425 - f05: 0.5351 - loss: 0.5755 - precision_5: 0.5432 - precision_focused: 0.5423 - recall_5: 0.6416\n",
      "Epoch 23: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 23: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5354 - auc_5: 0.5425 - f05: 0.5350 - loss: 0.5755 - precision_5: 0.5431 - precision_focused: 0.5421 - recall_5: 0.6414 - val_accuracy: 0.5002 - val_auc_5: 0.5154 - val_f05: 0.4225 - val_loss: 0.5788 - val_precision_5: 0.5383 - val_precision_focused: 0.4366 - val_recall_5: 0.2951 - learning_rate: 5.4550e-05\n",
      "Epoch 24/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5309 - auc_5: 0.5467 - f05: 0.4923 - loss: 0.5784 - precision_5: 0.5352 - precision_focused: 0.4911 - recall_5: 0.4952\n",
      "Epoch 24: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 24: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5309 - auc_5: 0.5467 - f05: 0.4923 - loss: 0.5784 - precision_5: 0.5352 - precision_focused: 0.4912 - recall_5: 0.4953 - val_accuracy: 0.5031 - val_auc_5: 0.5158 - val_f05: 0.4362 - val_loss: 0.5785 - val_precision_5: 0.5394 - val_precision_focused: 0.4447 - val_recall_5: 0.3253 - learning_rate: 5.4550e-05\n",
      "Epoch 25/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5233 - auc_5: 0.5378 - f05: 0.5284 - loss: 0.5763 - precision_5: 0.5317 - precision_focused: 0.5336 - recall_5: 0.6074\n",
      "Epoch 25: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 25: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5234 - auc_5: 0.5379 - f05: 0.5283 - loss: 0.5763 - precision_5: 0.5317 - precision_focused: 0.5336 - recall_5: 0.6074 - val_accuracy: 0.5034 - val_auc_5: 0.5162 - val_f05: 0.4076 - val_loss: 0.5795 - val_precision_5: 0.5487 - val_precision_focused: 0.4290 - val_recall_5: 0.2710 - learning_rate: 5.4550e-05\n",
      "Epoch 26/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5327 - auc_5: 0.5541 - f05: 0.4975 - loss: 0.5774 - precision_5: 0.5402 - precision_focused: 0.4961 - recall_5: 0.4946\n",
      "Epoch 26: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 26: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5328 - auc_5: 0.5541 - f05: 0.4976 - loss: 0.5774 - precision_5: 0.5402 - precision_focused: 0.4961 - recall_5: 0.4948 - val_accuracy: 0.5076 - val_auc_5: 0.5164 - val_f05: 0.4509 - val_loss: 0.5785 - val_precision_5: 0.5421 - val_precision_focused: 0.4546 - val_recall_5: 0.3617 - learning_rate: 5.4550e-05\n",
      "Epoch 27/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - auc_5: 0.5497 - f05: 0.5268 - loss: 0.5751 - precision_5: 0.5470 - precision_focused: 0.5305 - recall_5: 0.6060\n",
      "Epoch 27: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 27: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5382 - auc_5: 0.5498 - f05: 0.5266 - loss: 0.5751 - precision_5: 0.5468 - precision_focused: 0.5303 - recall_5: 0.6057 - val_accuracy: 0.4995 - val_auc_5: 0.5151 - val_f05: 0.4163 - val_loss: 0.5796 - val_precision_5: 0.5388 - val_precision_focused: 0.4333 - val_recall_5: 0.2827 - learning_rate: 5.4550e-05\n",
      "Epoch 28/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5327 - auc_5: 0.5420 - f05: 0.4998 - loss: 0.5780 - precision_5: 0.5426 - precision_focused: 0.4984 - recall_5: 0.4978\n",
      "Epoch 28: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 28: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5327 - auc_5: 0.5421 - f05: 0.4999 - loss: 0.5780 - precision_5: 0.5426 - precision_focused: 0.4985 - recall_5: 0.4982 - val_accuracy: 0.5005 - val_auc_5: 0.5165 - val_f05: 0.4433 - val_loss: 0.5787 - val_precision_5: 0.5337 - val_precision_focused: 0.4494 - val_recall_5: 0.3377 - learning_rate: 5.4550e-05\n",
      "Epoch 29/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5449 - auc_5: 0.5543 - f05: 0.5251 - loss: 0.5752 - precision_5: 0.5512 - precision_focused: 0.5290 - recall_5: 0.6126\n",
      "Epoch 29: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 29: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5449 - auc_5: 0.5543 - f05: 0.5250 - loss: 0.5752 - precision_5: 0.5511 - precision_focused: 0.5289 - recall_5: 0.6122 - val_accuracy: 0.4986 - val_auc_5: 0.5174 - val_f05: 0.4264 - val_loss: 0.5789 - val_precision_5: 0.5345 - val_precision_focused: 0.4387 - val_recall_5: 0.3012 - learning_rate: 5.4550e-05\n",
      "Epoch 30/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5369 - auc_5: 0.5519 - f05: 0.5103 - loss: 0.5758 - precision_5: 0.5470 - precision_focused: 0.5102 - recall_5: 0.5419\n",
      "Epoch 30: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 30: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.5369 - auc_5: 0.5519 - f05: 0.5103 - loss: 0.5758 - precision_5: 0.5470 - precision_focused: 0.5102 - recall_5: 0.5420 - val_accuracy: 0.5002 - val_auc_5: 0.5175 - val_f05: 0.4482 - val_loss: 0.5783 - val_precision_5: 0.5321 - val_precision_focused: 0.4528 - val_recall_5: 0.3475 - learning_rate: 5.4550e-05\n",
      "Epoch 31/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5425 - auc_5: 0.5527 - f05: 0.5168 - loss: 0.5758 - precision_5: 0.5491 - precision_focused: 0.5189 - recall_5: 0.5850\n",
      "Epoch 31: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 31: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5425 - auc_5: 0.5527 - f05: 0.5168 - loss: 0.5758 - precision_5: 0.5491 - precision_focused: 0.5188 - recall_5: 0.5848 - val_accuracy: 0.4973 - val_auc_5: 0.5162 - val_f05: 0.4211 - val_loss: 0.5792 - val_precision_5: 0.5339 - val_precision_focused: 0.4366 - val_recall_5: 0.2864 - learning_rate: 5.4550e-05\n",
      "Epoch 32/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5356 - auc_5: 0.5494 - f05: 0.5098 - loss: 0.5770 - precision_5: 0.5415 - precision_focused: 0.5108 - recall_5: 0.5558\n",
      "Epoch 32: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 32: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.5357 - auc_5: 0.5494 - f05: 0.5098 - loss: 0.5770 - precision_5: 0.5416 - precision_focused: 0.5107 - recall_5: 0.5557 - val_accuracy: 0.5034 - val_auc_5: 0.5160 - val_f05: 0.4329 - val_loss: 0.5792 - val_precision_5: 0.5412 - val_precision_focused: 0.4431 - val_recall_5: 0.3160 - learning_rate: 5.4550e-05\n",
      "Epoch 33/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5293 - auc_5: 0.5437 - f05: 0.5073 - loss: 0.5786 - precision_5: 0.5331 - precision_focused: 0.5078 - recall_5: 0.5415\n",
      "Epoch 33: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 33: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5294 - auc_5: 0.5438 - f05: 0.5073 - loss: 0.5786 - precision_5: 0.5332 - precision_focused: 0.5078 - recall_5: 0.5415 - val_accuracy: 0.5005 - val_auc_5: 0.5170 - val_f05: 0.4483 - val_loss: 0.5789 - val_precision_5: 0.5327 - val_precision_focused: 0.4531 - val_recall_5: 0.3469 - learning_rate: 5.4550e-05\n",
      "Epoch 34/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5496 - auc_5: 0.5589 - f05: 0.5264 - loss: 0.5742 - precision_5: 0.5559 - precision_focused: 0.5304 - recall_5: 0.6175\n",
      "Epoch 34: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 34: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.5495 - auc_5: 0.5589 - f05: 0.5263 - loss: 0.5743 - precision_5: 0.5559 - precision_focused: 0.5303 - recall_5: 0.6172 - val_accuracy: 0.5011 - val_auc_5: 0.5168 - val_f05: 0.4256 - val_loss: 0.5795 - val_precision_5: 0.5393 - val_precision_focused: 0.4386 - val_recall_5: 0.3006 - learning_rate: 5.4550e-05\n",
      "Epoch 35/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5440 - auc_5: 0.5583 - f05: 0.5264 - loss: 0.5732 - precision_5: 0.5537 - precision_focused: 0.5297 - recall_5: 0.6088\n",
      "Epoch 35: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 35: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5439 - auc_5: 0.5583 - f05: 0.5263 - loss: 0.5732 - precision_5: 0.5536 - precision_focused: 0.5297 - recall_5: 0.6086 - val_accuracy: 0.4947 - val_auc_5: 0.5156 - val_f05: 0.4471 - val_loss: 0.5788 - val_precision_5: 0.5242 - val_precision_focused: 0.4518 - val_recall_5: 0.3414 - learning_rate: 5.4550e-05\n",
      "Epoch 36/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5449 - auc_5: 0.5577 - f05: 0.5208 - loss: 0.5743 - precision_5: 0.5542 - precision_focused: 0.5224 - recall_5: 0.5811\n",
      "Epoch 36: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 36: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5449 - auc_5: 0.5577 - f05: 0.5207 - loss: 0.5743 - precision_5: 0.5541 - precision_focused: 0.5223 - recall_5: 0.5808 - val_accuracy: 0.5050 - val_auc_5: 0.5156 - val_f05: 0.4254 - val_loss: 0.5796 - val_precision_5: 0.5461 - val_precision_focused: 0.4386 - val_recall_5: 0.3037 - learning_rate: 5.4550e-05\n",
      "Epoch 37/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5497 - auc_5: 0.5665 - f05: 0.5126 - loss: 0.5750 - precision_5: 0.5546 - precision_focused: 0.5138 - recall_5: 0.5765\n",
      "Epoch 37: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 37: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5497 - auc_5: 0.5665 - f05: 0.5126 - loss: 0.5750 - precision_5: 0.5546 - precision_focused: 0.5138 - recall_5: 0.5765 - val_accuracy: 0.5002 - val_auc_5: 0.5149 - val_f05: 0.4477 - val_loss: 0.5794 - val_precision_5: 0.5321 - val_precision_focused: 0.4523 - val_recall_5: 0.3475 - learning_rate: 5.4550e-05\n",
      "Epoch 38/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5413 - auc_5: 0.5596 - f05: 0.5140 - loss: 0.5755 - precision_5: 0.5446 - precision_focused: 0.5163 - recall_5: 0.5840\n",
      "Epoch 38: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 38: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5413 - auc_5: 0.5596 - f05: 0.5140 - loss: 0.5755 - precision_5: 0.5446 - precision_focused: 0.5163 - recall_5: 0.5839 - val_accuracy: 0.5076 - val_auc_5: 0.5175 - val_f05: 0.4547 - val_loss: 0.5789 - val_precision_5: 0.5409 - val_precision_focused: 0.4573 - val_recall_5: 0.3716 - learning_rate: 5.4550e-05\n",
      "Epoch 39/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5504 - auc_5: 0.5668 - f05: 0.5107 - loss: 0.5752 - precision_5: 0.5531 - precision_focused: 0.5122 - recall_5: 0.5798\n",
      "Epoch 39: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 39: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5503 - auc_5: 0.5668 - f05: 0.5107 - loss: 0.5752 - precision_5: 0.5530 - precision_focused: 0.5122 - recall_5: 0.5797 - val_accuracy: 0.5066 - val_auc_5: 0.5178 - val_f05: 0.4373 - val_loss: 0.5792 - val_precision_5: 0.5451 - val_precision_focused: 0.4458 - val_recall_5: 0.3284 - learning_rate: 5.4550e-05\n",
      "Epoch 40/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5450 - auc_5: 0.5634 - f05: 0.5186 - loss: 0.5735 - precision_5: 0.5538 - precision_focused: 0.5202 - recall_5: 0.5816\n",
      "Epoch 40: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 40: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5449 - auc_5: 0.5633 - f05: 0.5186 - loss: 0.5735 - precision_5: 0.5537 - precision_focused: 0.5201 - recall_5: 0.5814 - val_accuracy: 0.5021 - val_auc_5: 0.5157 - val_f05: 0.4641 - val_loss: 0.5787 - val_precision_5: 0.5307 - val_precision_focused: 0.4643 - val_recall_5: 0.3944 - learning_rate: 5.4550e-05\n",
      "Epoch 41/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5537 - auc_5: 0.5715 - f05: 0.5126 - loss: 0.5746 - precision_5: 0.5549 - precision_focused: 0.5148 - recall_5: 0.5925\n",
      "Epoch 41: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 41: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5537 - auc_5: 0.5715 - f05: 0.5126 - loss: 0.5746 - precision_5: 0.5549 - precision_focused: 0.5148 - recall_5: 0.5924 - val_accuracy: 0.5043 - val_auc_5: 0.5163 - val_f05: 0.4491 - val_loss: 0.5789 - val_precision_5: 0.5377 - val_precision_focused: 0.4532 - val_recall_5: 0.3562 - learning_rate: 5.4550e-05\n",
      "Epoch 42/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5448 - auc_5: 0.5680 - f05: 0.5213 - loss: 0.5736 - precision_5: 0.5488 - precision_focused: 0.5250 - recall_5: 0.6067\n",
      "Epoch 42: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 42: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5448 - auc_5: 0.5680 - f05: 0.5212 - loss: 0.5736 - precision_5: 0.5488 - precision_focused: 0.5249 - recall_5: 0.6066 - val_accuracy: 0.5027 - val_auc_5: 0.5156 - val_f05: 0.4116 - val_loss: 0.5804 - val_precision_5: 0.5469 - val_precision_focused: 0.4321 - val_recall_5: 0.2735 - learning_rate: 5.4550e-05\n",
      "Epoch 43/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5469 - auc_5: 0.5655 - f05: 0.5124 - loss: 0.5741 - precision_5: 0.5556 - precision_focused: 0.5126 - recall_5: 0.5570\n",
      "Epoch 43: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 43: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5469 - auc_5: 0.5655 - f05: 0.5124 - loss: 0.5741 - precision_5: 0.5556 - precision_focused: 0.5126 - recall_5: 0.5571 - val_accuracy: 0.5031 - val_auc_5: 0.5186 - val_f05: 0.4275 - val_loss: 0.5798 - val_precision_5: 0.5434 - val_precision_focused: 0.4418 - val_recall_5: 0.2975 - learning_rate: 5.4550e-05\n",
      "Epoch 44/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5537 - auc_5: 0.5750 - f05: 0.5271 - loss: 0.5707 - precision_5: 0.5620 - precision_focused: 0.5302 - recall_5: 0.6120\n",
      "Epoch 44: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 44: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5536 - auc_5: 0.5749 - f05: 0.5269 - loss: 0.5707 - precision_5: 0.5619 - precision_focused: 0.5300 - recall_5: 0.6115 - val_accuracy: 0.5043 - val_auc_5: 0.5182 - val_f05: 0.4585 - val_loss: 0.5793 - val_precision_5: 0.5361 - val_precision_focused: 0.4609 - val_recall_5: 0.3710 - learning_rate: 5.4550e-05\n",
      "Epoch 45/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5391 - auc_5: 0.5596 - f05: 0.5097 - loss: 0.5755 - precision_5: 0.5451 - precision_focused: 0.5109 - recall_5: 0.5619\n",
      "Epoch 45: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 45: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5391 - auc_5: 0.5596 - f05: 0.5097 - loss: 0.5755 - precision_5: 0.5452 - precision_focused: 0.5108 - recall_5: 0.5619 - val_accuracy: 0.5040 - val_auc_5: 0.5177 - val_f05: 0.4316 - val_loss: 0.5802 - val_precision_5: 0.5441 - val_precision_focused: 0.4444 - val_recall_5: 0.3049 - learning_rate: 5.4550e-05\n",
      "Epoch 46/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5481 - auc_5: 0.5681 - f05: 0.5221 - loss: 0.5729 - precision_5: 0.5551 - precision_focused: 0.5244 - recall_5: 0.5954\n",
      "Epoch 46: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 46: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5481 - auc_5: 0.5682 - f05: 0.5220 - loss: 0.5730 - precision_5: 0.5551 - precision_focused: 0.5243 - recall_5: 0.5951 - val_accuracy: 0.5063 - val_auc_5: 0.5178 - val_f05: 0.4317 - val_loss: 0.5804 - val_precision_5: 0.5479 - val_precision_focused: 0.4445 - val_recall_5: 0.3074 - learning_rate: 5.4550e-05\n",
      "Epoch 47/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5395 - auc_5: 0.5680 - f05: 0.5064 - loss: 0.5747 - precision_5: 0.5435 - precision_focused: 0.5068 - recall_5: 0.5486\n",
      "Epoch 47: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 47: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5395 - auc_5: 0.5679 - f05: 0.5064 - loss: 0.5747 - precision_5: 0.5436 - precision_focused: 0.5068 - recall_5: 0.5487 - val_accuracy: 0.5063 - val_auc_5: 0.5195 - val_f05: 0.4131 - val_loss: 0.5811 - val_precision_5: 0.5539 - val_precision_focused: 0.4341 - val_recall_5: 0.2759 - learning_rate: 5.4550e-05\n",
      "Epoch 48/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5557 - auc_5: 0.5729 - f05: 0.5304 - loss: 0.5706 - precision_5: 0.5681 - precision_focused: 0.5330 - recall_5: 0.6128\n",
      "Epoch 48: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 48: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5557 - auc_5: 0.5729 - f05: 0.5302 - loss: 0.5706 - precision_5: 0.5679 - precision_focused: 0.5328 - recall_5: 0.6126 - val_accuracy: 0.5101 - val_auc_5: 0.5172 - val_f05: 0.4130 - val_loss: 0.5815 - val_precision_5: 0.5615 - val_precision_focused: 0.4341 - val_recall_5: 0.2790 - learning_rate: 5.4550e-05\n",
      "Epoch 49/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5556 - auc_5: 0.5755 - f05: 0.5147 - loss: 0.5718 - precision_5: 0.5671 - precision_focused: 0.5146 - recall_5: 0.5631\n",
      "Epoch 49: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 49: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5556 - auc_5: 0.5755 - f05: 0.5147 - loss: 0.5718 - precision_5: 0.5670 - precision_focused: 0.5146 - recall_5: 0.5632 - val_accuracy: 0.5105 - val_auc_5: 0.5186 - val_f05: 0.4104 - val_loss: 0.5816 - val_precision_5: 0.5631 - val_precision_focused: 0.4328 - val_recall_5: 0.2753 - learning_rate: 5.4550e-05\n",
      "Epoch 50/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5536 - auc_5: 0.5703 - f05: 0.5210 - loss: 0.5730 - precision_5: 0.5611 - precision_focused: 0.5226 - recall_5: 0.5912\n",
      "Epoch 50: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 50: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5536 - auc_5: 0.5703 - f05: 0.5209 - loss: 0.5730 - precision_5: 0.5611 - precision_focused: 0.5226 - recall_5: 0.5911 - val_accuracy: 0.5085 - val_auc_5: 0.5169 - val_f05: 0.4753 - val_loss: 0.5793 - val_precision_5: 0.5372 - val_precision_focused: 0.4745 - val_recall_5: 0.4185 - learning_rate: 5.4550e-05\n",
      "Epoch 51/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5710 - auc_5: 0.5830 - f05: 0.5224 - loss: 0.5710 - precision_5: 0.5723 - precision_focused: 0.5261 - recall_5: 0.6325\n",
      "Epoch 51: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 51: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5709 - auc_5: 0.5829 - f05: 0.5223 - loss: 0.5710 - precision_5: 0.5722 - precision_focused: 0.5260 - recall_5: 0.6320 - val_accuracy: 0.5118 - val_auc_5: 0.5159 - val_f05: 0.4690 - val_loss: 0.5797 - val_precision_5: 0.5430 - val_precision_focused: 0.4691 - val_recall_5: 0.4056 - learning_rate: 5.4550e-05\n",
      "Epoch 52/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5526 - auc_5: 0.5735 - f05: 0.5184 - loss: 0.5732 - precision_5: 0.5521 - precision_focused: 0.5224 - recall_5: 0.6161\n",
      "Epoch 52: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 52: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5526 - auc_5: 0.5735 - f05: 0.5184 - loss: 0.5732 - precision_5: 0.5521 - precision_focused: 0.5224 - recall_5: 0.6161 - val_accuracy: 0.5098 - val_auc_5: 0.5183 - val_f05: 0.4643 - val_loss: 0.5800 - val_precision_5: 0.5419 - val_precision_focused: 0.4653 - val_recall_5: 0.3914 - learning_rate: 5.4550e-05\n",
      "Epoch 53/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5497 - auc_5: 0.5730 - f05: 0.5193 - loss: 0.5727 - precision_5: 0.5561 - precision_focused: 0.5216 - recall_5: 0.5961\n",
      "Epoch 53: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 53: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5497 - auc_5: 0.5730 - f05: 0.5193 - loss: 0.5727 - precision_5: 0.5561 - precision_focused: 0.5216 - recall_5: 0.5961 - val_accuracy: 0.5095 - val_auc_5: 0.5178 - val_f05: 0.4287 - val_loss: 0.5811 - val_precision_5: 0.5538 - val_precision_focused: 0.4420 - val_recall_5: 0.3080 - learning_rate: 5.4550e-05\n",
      "Epoch 54/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5569 - auc_5: 0.5774 - f05: 0.5042 - loss: 0.5731 - precision_5: 0.5619 - precision_focused: 0.5041 - recall_5: 0.5572\n",
      "Epoch 54: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 54: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5568 - auc_5: 0.5774 - f05: 0.5043 - loss: 0.5731 - precision_5: 0.5619 - precision_focused: 0.5041 - recall_5: 0.5573 - val_accuracy: 0.5153 - val_auc_5: 0.5180 - val_f05: 0.4453 - val_loss: 0.5805 - val_precision_5: 0.5568 - val_precision_focused: 0.4520 - val_recall_5: 0.3481 - learning_rate: 5.4550e-05\n",
      "Epoch 55/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5520 - auc_5: 0.5765 - f05: 0.5075 - loss: 0.5733 - precision_5: 0.5564 - precision_focused: 0.5081 - recall_5: 0.5672\n",
      "Epoch 55: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 55: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5521 - auc_5: 0.5765 - f05: 0.5075 - loss: 0.5733 - precision_5: 0.5564 - precision_focused: 0.5081 - recall_5: 0.5672 - val_accuracy: 0.5127 - val_auc_5: 0.5177 - val_f05: 0.4489 - val_loss: 0.5807 - val_precision_5: 0.5516 - val_precision_focused: 0.4544 - val_recall_5: 0.3531 - learning_rate: 5.4550e-05\n",
      "Epoch 56/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5618 - auc_5: 0.5783 - f05: 0.5180 - loss: 0.5720 - precision_5: 0.5668 - precision_focused: 0.5200 - recall_5: 0.5997\n",
      "Epoch 56: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 56: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5618 - auc_5: 0.5783 - f05: 0.5180 - loss: 0.5720 - precision_5: 0.5668 - precision_focused: 0.5200 - recall_5: 0.5996 - val_accuracy: 0.5105 - val_auc_5: 0.5168 - val_f05: 0.4702 - val_loss: 0.5803 - val_precision_5: 0.5405 - val_precision_focused: 0.4698 - val_recall_5: 0.4117 - learning_rate: 5.4550e-05\n",
      "Epoch 57/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5523 - auc_5: 0.5782 - f05: 0.5085 - loss: 0.5722 - precision_5: 0.5552 - precision_focused: 0.5092 - recall_5: 0.5681\n",
      "Epoch 57: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 57: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.364000051282346e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5523 - auc_5: 0.5782 - f05: 0.5085 - loss: 0.5722 - precision_5: 0.5552 - precision_focused: 0.5092 - recall_5: 0.5681 - val_accuracy: 0.5105 - val_auc_5: 0.5188 - val_f05: 0.4698 - val_loss: 0.5799 - val_precision_5: 0.5412 - val_precision_focused: 0.4698 - val_recall_5: 0.4056 - learning_rate: 5.4550e-05\n",
      "Epoch 58/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5575 - auc_5: 0.5865 - f05: 0.4978 - loss: 0.5734 - precision_5: 0.5516 - precision_focused: 0.4991 - recall_5: 0.5715\n",
      "Epoch 58: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 58: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5575 - auc_5: 0.5865 - f05: 0.4978 - loss: 0.5734 - precision_5: 0.5516 - precision_focused: 0.4991 - recall_5: 0.5715 - val_accuracy: 0.5050 - val_auc_5: 0.5185 - val_f05: 0.4683 - val_loss: 0.5802 - val_precision_5: 0.5343 - val_precision_focused: 0.4684 - val_recall_5: 0.3988 - learning_rate: 4.3640e-05\n",
      "Epoch 59/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5634 - auc_5: 0.5862 - f05: 0.5137 - loss: 0.5711 - precision_5: 0.5648 - precision_focused: 0.5157 - recall_5: 0.6017\n",
      "Epoch 59: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 59: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5634 - auc_5: 0.5862 - f05: 0.5137 - loss: 0.5711 - precision_5: 0.5648 - precision_focused: 0.5157 - recall_5: 0.6016 - val_accuracy: 0.5124 - val_auc_5: 0.5192 - val_f05: 0.4549 - val_loss: 0.5808 - val_precision_5: 0.5489 - val_precision_focused: 0.4585 - val_recall_5: 0.3673 - learning_rate: 4.3640e-05\n",
      "Epoch 60/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5519 - auc_5: 0.5822 - f05: 0.5100 - loss: 0.5717 - precision_5: 0.5544 - precision_focused: 0.5114 - recall_5: 0.5784\n",
      "Epoch 60: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 60: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5519 - auc_5: 0.5821 - f05: 0.5100 - loss: 0.5717 - precision_5: 0.5545 - precision_focused: 0.5114 - recall_5: 0.5784 - val_accuracy: 0.5118 - val_auc_5: 0.5203 - val_f05: 0.4563 - val_loss: 0.5805 - val_precision_5: 0.5476 - val_precision_focused: 0.4596 - val_recall_5: 0.3691 - learning_rate: 4.3640e-05\n",
      "Epoch 61/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5707 - auc_5: 0.5958 - f05: 0.5199 - loss: 0.5695 - precision_5: 0.5695 - precision_focused: 0.5238 - recall_5: 0.6321\n",
      "Epoch 61: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 61: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5706 - auc_5: 0.5957 - f05: 0.5199 - loss: 0.5695 - precision_5: 0.5695 - precision_focused: 0.5238 - recall_5: 0.6319 - val_accuracy: 0.5130 - val_auc_5: 0.5199 - val_f05: 0.4839 - val_loss: 0.5795 - val_precision_5: 0.5401 - val_precision_focused: 0.4823 - val_recall_5: 0.4488 - learning_rate: 4.3640e-05\n",
      "Epoch 62/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5622 - auc_5: 0.5854 - f05: 0.5278 - loss: 0.5692 - precision_5: 0.5668 - precision_focused: 0.5318 - recall_5: 0.6307\n",
      "Epoch 62: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 62: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5621 - auc_5: 0.5854 - f05: 0.5277 - loss: 0.5692 - precision_5: 0.5668 - precision_focused: 0.5316 - recall_5: 0.6304 - val_accuracy: 0.5118 - val_auc_5: 0.5184 - val_f05: 0.4485 - val_loss: 0.5819 - val_precision_5: 0.5502 - val_precision_focused: 0.4541 - val_recall_5: 0.3519 - learning_rate: 4.3640e-05\n",
      "Epoch 63/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5728 - auc_5: 0.5947 - f05: 0.5180 - loss: 0.5686 - precision_5: 0.5770 - precision_focused: 0.5196 - recall_5: 0.6072\n",
      "Epoch 63: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 63: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5727 - auc_5: 0.5946 - f05: 0.5180 - loss: 0.5686 - precision_5: 0.5769 - precision_focused: 0.5196 - recall_5: 0.6071 - val_accuracy: 0.5137 - val_auc_5: 0.5211 - val_f05: 0.4567 - val_loss: 0.5811 - val_precision_5: 0.5506 - val_precision_focused: 0.4603 - val_recall_5: 0.3691 - learning_rate: 4.3640e-05\n",
      "Epoch 64/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5640 - auc_5: 0.5879 - f05: 0.5098 - loss: 0.5715 - precision_5: 0.5654 - precision_focused: 0.5109 - recall_5: 0.5840\n",
      "Epoch 64: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 64: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5640 - auc_5: 0.5879 - f05: 0.5098 - loss: 0.5714 - precision_5: 0.5654 - precision_focused: 0.5109 - recall_5: 0.5840 - val_accuracy: 0.5130 - val_auc_5: 0.5201 - val_f05: 0.4677 - val_loss: 0.5805 - val_precision_5: 0.5455 - val_precision_focused: 0.4683 - val_recall_5: 0.4000 - learning_rate: 4.3640e-05\n",
      "Epoch 65/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5657 - auc_5: 0.5884 - f05: 0.5249 - loss: 0.5690 - precision_5: 0.5664 - precision_focused: 0.5294 - recall_5: 0.6368\n",
      "Epoch 65: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 65: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5657 - auc_5: 0.5884 - f05: 0.5249 - loss: 0.5690 - precision_5: 0.5664 - precision_focused: 0.5294 - recall_5: 0.6367 - val_accuracy: 0.5127 - val_auc_5: 0.5192 - val_f05: 0.4517 - val_loss: 0.5817 - val_precision_5: 0.5515 - val_precision_focused: 0.4572 - val_recall_5: 0.3537 - learning_rate: 4.3640e-05\n",
      "Epoch 66/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5522 - auc_5: 0.5794 - f05: 0.5242 - loss: 0.5694 - precision_5: 0.5636 - precision_focused: 0.5259 - recall_5: 0.5948\n",
      "Epoch 66: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 66: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5523 - auc_5: 0.5794 - f05: 0.5240 - loss: 0.5695 - precision_5: 0.5635 - precision_focused: 0.5257 - recall_5: 0.5946 - val_accuracy: 0.5137 - val_auc_5: 0.5199 - val_f05: 0.4467 - val_loss: 0.5819 - val_precision_5: 0.5547 - val_precision_focused: 0.4537 - val_recall_5: 0.3444 - learning_rate: 4.3640e-05\n",
      "Epoch 67/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5609 - auc_5: 0.5858 - f05: 0.5105 - loss: 0.5707 - precision_5: 0.5611 - precision_focused: 0.5117 - recall_5: 0.5849\n",
      "Epoch 67: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 67: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5609 - auc_5: 0.5858 - f05: 0.5105 - loss: 0.5707 - precision_5: 0.5611 - precision_focused: 0.5118 - recall_5: 0.5850 - val_accuracy: 0.5134 - val_auc_5: 0.5183 - val_f05: 0.4813 - val_loss: 0.5806 - val_precision_5: 0.5414 - val_precision_focused: 0.4799 - val_recall_5: 0.4395 - learning_rate: 4.3640e-05\n",
      "Epoch 68/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5744 - auc_5: 0.6026 - f05: 0.5219 - loss: 0.5664 - precision_5: 0.5735 - precision_focused: 0.5262 - recall_5: 0.6427\n",
      "Epoch 68: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 68: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5744 - auc_5: 0.6026 - f05: 0.5219 - loss: 0.5664 - precision_5: 0.5735 - precision_focused: 0.5262 - recall_5: 0.6426 - val_accuracy: 0.5124 - val_auc_5: 0.5182 - val_f05: 0.4378 - val_loss: 0.5825 - val_precision_5: 0.5556 - val_precision_focused: 0.4476 - val_recall_5: 0.3272 - learning_rate: 4.3640e-05\n",
      "Epoch 69/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5695 - auc_5: 0.5903 - f05: 0.5161 - loss: 0.5695 - precision_5: 0.5741 - precision_focused: 0.5174 - recall_5: 0.6003\n",
      "Epoch 69: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 69: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5695 - auc_5: 0.5903 - f05: 0.5161 - loss: 0.5695 - precision_5: 0.5741 - precision_focused: 0.5174 - recall_5: 0.6003 - val_accuracy: 0.5156 - val_auc_5: 0.5184 - val_f05: 0.4707 - val_loss: 0.5812 - val_precision_5: 0.5475 - val_precision_focused: 0.4706 - val_recall_5: 0.4130 - learning_rate: 4.3640e-05\n",
      "Epoch 70/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5598 - auc_5: 0.5883 - f05: 0.5156 - loss: 0.5695 - precision_5: 0.5635 - precision_focused: 0.5176 - recall_5: 0.5990\n",
      "Epoch 70: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 70: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5599 - auc_5: 0.5883 - f05: 0.5155 - loss: 0.5695 - precision_5: 0.5635 - precision_focused: 0.5176 - recall_5: 0.5989 - val_accuracy: 0.5130 - val_auc_5: 0.5195 - val_f05: 0.4682 - val_loss: 0.5815 - val_precision_5: 0.5452 - val_precision_focused: 0.4686 - val_recall_5: 0.4025 - learning_rate: 4.3640e-05\n",
      "Epoch 71/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5586 - auc_5: 0.5826 - f05: 0.5192 - loss: 0.5704 - precision_5: 0.5621 - precision_focused: 0.5218 - recall_5: 0.6079\n",
      "Epoch 71: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 71: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5586 - auc_5: 0.5826 - f05: 0.5192 - loss: 0.5704 - precision_5: 0.5621 - precision_focused: 0.5218 - recall_5: 0.6078 - val_accuracy: 0.5147 - val_auc_5: 0.5190 - val_f05: 0.4578 - val_loss: 0.5820 - val_precision_5: 0.5513 - val_precision_focused: 0.4608 - val_recall_5: 0.3747 - learning_rate: 4.3640e-05\n",
      "Epoch 72/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5657 - auc_5: 0.5955 - f05: 0.5192 - loss: 0.5684 - precision_5: 0.5671 - precision_focused: 0.5227 - recall_5: 0.6255\n",
      "Epoch 72: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 72: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5657 - auc_5: 0.5955 - f05: 0.5192 - loss: 0.5684 - precision_5: 0.5671 - precision_focused: 0.5227 - recall_5: 0.6255 - val_accuracy: 0.5082 - val_auc_5: 0.5197 - val_f05: 0.4449 - val_loss: 0.5821 - val_precision_5: 0.5465 - val_precision_focused: 0.4521 - val_recall_5: 0.3377 - learning_rate: 4.3640e-05\n",
      "Epoch 73/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5635 - auc_5: 0.5915 - f05: 0.5051 - loss: 0.5708 - precision_5: 0.5640 - precision_focused: 0.5057 - recall_5: 0.5765\n",
      "Epoch 73: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 73: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5634 - auc_5: 0.5915 - f05: 0.5051 - loss: 0.5708 - precision_5: 0.5640 - precision_focused: 0.5058 - recall_5: 0.5765 - val_accuracy: 0.5140 - val_auc_5: 0.5184 - val_f05: 0.4658 - val_loss: 0.5816 - val_precision_5: 0.5468 - val_precision_focused: 0.4665 - val_recall_5: 0.4000 - learning_rate: 4.3640e-05\n",
      "Epoch 74/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5720 - auc_5: 0.5987 - f05: 0.5247 - loss: 0.5665 - precision_5: 0.5770 - precision_focused: 0.5273 - recall_5: 0.6219\n",
      "Epoch 74: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 74: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5720 - auc_5: 0.5986 - f05: 0.5247 - loss: 0.5665 - precision_5: 0.5770 - precision_focused: 0.5272 - recall_5: 0.6217 - val_accuracy: 0.5156 - val_auc_5: 0.5189 - val_f05: 0.4601 - val_loss: 0.5821 - val_precision_5: 0.5506 - val_precision_focused: 0.4617 - val_recall_5: 0.3895 - learning_rate: 4.3640e-05\n",
      "Epoch 75/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5704 - auc_5: 0.5965 - f05: 0.5215 - loss: 0.5669 - precision_5: 0.5788 - precision_focused: 0.5230 - recall_5: 0.6067\n",
      "Epoch 75: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 75: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5704 - auc_5: 0.5965 - f05: 0.5214 - loss: 0.5669 - precision_5: 0.5788 - precision_focused: 0.5230 - recall_5: 0.6066 - val_accuracy: 0.5137 - val_auc_5: 0.5199 - val_f05: 0.4549 - val_loss: 0.5818 - val_precision_5: 0.5506 - val_precision_focused: 0.4583 - val_recall_5: 0.3698 - learning_rate: 4.3640e-05\n",
      "Epoch 76/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5709 - auc_5: 0.5983 - f05: 0.5228 - loss: 0.5660 - precision_5: 0.5784 - precision_focused: 0.5249 - recall_5: 0.6162\n",
      "Epoch 76: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 76: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.5708 - auc_5: 0.5982 - f05: 0.5228 - loss: 0.5660 - precision_5: 0.5783 - precision_focused: 0.5249 - recall_5: 0.6161 - val_accuracy: 0.5130 - val_auc_5: 0.5196 - val_f05: 0.4593 - val_loss: 0.5817 - val_precision_5: 0.5488 - val_precision_focused: 0.4622 - val_recall_5: 0.3747 - learning_rate: 4.3640e-05\n",
      "Epoch 77/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5689 - auc_5: 0.6005 - f05: 0.5174 - loss: 0.5664 - precision_5: 0.5753 - precision_focused: 0.5185 - recall_5: 0.5977\n",
      "Epoch 77: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 77: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5689 - auc_5: 0.6005 - f05: 0.5173 - loss: 0.5664 - precision_5: 0.5753 - precision_focused: 0.5185 - recall_5: 0.5978 - val_accuracy: 0.5118 - val_auc_5: 0.5181 - val_f05: 0.4620 - val_loss: 0.5817 - val_precision_5: 0.5455 - val_precision_focused: 0.4637 - val_recall_5: 0.3852 - learning_rate: 4.3640e-05\n",
      "Epoch 78/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5754 - auc_5: 0.5949 - f05: 0.5131 - loss: 0.5687 - precision_5: 0.5789 - precision_focused: 0.5143 - recall_5: 0.6022\n",
      "Epoch 78: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 78: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5754 - auc_5: 0.5949 - f05: 0.5131 - loss: 0.5687 - precision_5: 0.5789 - precision_focused: 0.5143 - recall_5: 0.6021 - val_accuracy: 0.5130 - val_auc_5: 0.5192 - val_f05: 0.4389 - val_loss: 0.5833 - val_precision_5: 0.5566 - val_precision_focused: 0.4488 - val_recall_5: 0.3278 - learning_rate: 4.3640e-05\n",
      "Epoch 79/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5701 - auc_5: 0.5959 - f05: 0.5083 - loss: 0.5681 - precision_5: 0.5775 - precision_focused: 0.5082 - recall_5: 0.5749\n",
      "Epoch 79: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 79: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5701 - auc_5: 0.5959 - f05: 0.5083 - loss: 0.5681 - precision_5: 0.5775 - precision_focused: 0.5082 - recall_5: 0.5749 - val_accuracy: 0.5040 - val_auc_5: 0.5181 - val_f05: 0.4415 - val_loss: 0.5832 - val_precision_5: 0.5410 - val_precision_focused: 0.4501 - val_recall_5: 0.3259 - learning_rate: 4.3640e-05\n",
      "Epoch 80/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5673 - auc_5: 0.5976 - f05: 0.5162 - loss: 0.5673 - precision_5: 0.5704 - precision_focused: 0.5181 - recall_5: 0.6014\n",
      "Epoch 80: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 80: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5673 - auc_5: 0.5976 - f05: 0.5162 - loss: 0.5673 - precision_5: 0.5704 - precision_focused: 0.5180 - recall_5: 0.6013 - val_accuracy: 0.5085 - val_auc_5: 0.5182 - val_f05: 0.4589 - val_loss: 0.5820 - val_precision_5: 0.5424 - val_precision_focused: 0.4617 - val_recall_5: 0.3710 - learning_rate: 4.3640e-05\n",
      "Epoch 81/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5817 - auc_5: 0.6085 - f05: 0.5176 - loss: 0.5653 - precision_5: 0.5833 - precision_focused: 0.5198 - recall_5: 0.6209\n",
      "Epoch 81: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 81: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5817 - auc_5: 0.6085 - f05: 0.5176 - loss: 0.5653 - precision_5: 0.5833 - precision_focused: 0.5197 - recall_5: 0.6208 - val_accuracy: 0.5069 - val_auc_5: 0.5177 - val_f05: 0.4607 - val_loss: 0.5822 - val_precision_5: 0.5400 - val_precision_focused: 0.4633 - val_recall_5: 0.3710 - learning_rate: 4.3640e-05\n",
      "Epoch 82/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5749 - auc_5: 0.6025 - f05: 0.5123 - loss: 0.5674 - precision_5: 0.5729 - precision_focused: 0.5146 - recall_5: 0.6141\n",
      "Epoch 82: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 82: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5749 - auc_5: 0.6025 - f05: 0.5123 - loss: 0.5674 - precision_5: 0.5729 - precision_focused: 0.5146 - recall_5: 0.6141 - val_accuracy: 0.5043 - val_auc_5: 0.5196 - val_f05: 0.4535 - val_loss: 0.5825 - val_precision_5: 0.5385 - val_precision_focused: 0.4585 - val_recall_5: 0.3494 - learning_rate: 4.3640e-05\n",
      "Epoch 83/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5746 - auc_5: 0.6038 - f05: 0.5262 - loss: 0.5648 - precision_5: 0.5774 - precision_focused: 0.5299 - recall_5: 0.6394\n",
      "Epoch 83: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 83: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5746 - auc_5: 0.6038 - f05: 0.5261 - loss: 0.5648 - precision_5: 0.5774 - precision_focused: 0.5298 - recall_5: 0.6392 - val_accuracy: 0.5040 - val_auc_5: 0.5192 - val_f05: 0.4524 - val_loss: 0.5837 - val_precision_5: 0.5380 - val_precision_focused: 0.4573 - val_recall_5: 0.3494 - learning_rate: 4.3640e-05\n",
      "Epoch 84/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5748 - auc_5: 0.6025 - f05: 0.4990 - loss: 0.5696 - precision_5: 0.5707 - precision_focused: 0.4998 - recall_5: 0.5814\n",
      "Epoch 84: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 84: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5747 - auc_5: 0.6025 - f05: 0.4991 - loss: 0.5696 - precision_5: 0.5707 - precision_focused: 0.4999 - recall_5: 0.5813 - val_accuracy: 0.5089 - val_auc_5: 0.5210 - val_f05: 0.4622 - val_loss: 0.5820 - val_precision_5: 0.5421 - val_precision_focused: 0.4643 - val_recall_5: 0.3778 - learning_rate: 4.3640e-05\n",
      "Epoch 85/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5649 - auc_5: 0.5978 - f05: 0.5197 - loss: 0.5669 - precision_5: 0.5672 - precision_focused: 0.5225 - recall_5: 0.6153\n",
      "Epoch 85: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 85: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5649 - auc_5: 0.5978 - f05: 0.5197 - loss: 0.5669 - precision_5: 0.5673 - precision_focused: 0.5225 - recall_5: 0.6152 - val_accuracy: 0.5095 - val_auc_5: 0.5199 - val_f05: 0.4369 - val_loss: 0.5840 - val_precision_5: 0.5517 - val_precision_focused: 0.4477 - val_recall_5: 0.3198 - learning_rate: 4.3640e-05\n",
      "Epoch 86/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5748 - auc_5: 0.6015 - f05: 0.5158 - loss: 0.5665 - precision_5: 0.5786 - precision_focused: 0.5173 - recall_5: 0.6071\n",
      "Epoch 86: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 86: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5748 - auc_5: 0.6015 - f05: 0.5158 - loss: 0.5665 - precision_5: 0.5786 - precision_focused: 0.5173 - recall_5: 0.6071 - val_accuracy: 0.5124 - val_auc_5: 0.5213 - val_f05: 0.4851 - val_loss: 0.5810 - val_precision_5: 0.5397 - val_precision_focused: 0.4836 - val_recall_5: 0.4444 - learning_rate: 4.3640e-05\n",
      "Epoch 87/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5897 - auc_5: 0.6172 - f05: 0.5209 - loss: 0.5627 - precision_5: 0.5912 - precision_focused: 0.5230 - recall_5: 0.6301\n",
      "Epoch 87: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 87: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5896 - auc_5: 0.6172 - f05: 0.5209 - loss: 0.5627 - precision_5: 0.5912 - precision_focused: 0.5230 - recall_5: 0.6301 - val_accuracy: 0.5089 - val_auc_5: 0.5208 - val_f05: 0.4321 - val_loss: 0.5850 - val_precision_5: 0.5527 - val_precision_focused: 0.4455 - val_recall_5: 0.3074 - learning_rate: 4.3640e-05\n",
      "Epoch 88/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5808 - auc_5: 0.6098 - f05: 0.5171 - loss: 0.5643 - precision_5: 0.5915 - precision_focused: 0.5175 - recall_5: 0.5971\n",
      "Epoch 88: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 88: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5807 - auc_5: 0.6098 - f05: 0.5170 - loss: 0.5643 - precision_5: 0.5914 - precision_focused: 0.5175 - recall_5: 0.5971 - val_accuracy: 0.5095 - val_auc_5: 0.5199 - val_f05: 0.4692 - val_loss: 0.5822 - val_precision_5: 0.5404 - val_precision_focused: 0.4695 - val_recall_5: 0.4006 - learning_rate: 4.3640e-05\n",
      "Epoch 89/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5739 - auc_5: 0.6032 - f05: 0.5102 - loss: 0.5670 - precision_5: 0.5752 - precision_focused: 0.5112 - recall_5: 0.5945\n",
      "Epoch 89: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 89: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5739 - auc_5: 0.6032 - f05: 0.5102 - loss: 0.5670 - precision_5: 0.5752 - precision_focused: 0.5112 - recall_5: 0.5945 - val_accuracy: 0.5101 - val_auc_5: 0.5195 - val_f05: 0.4724 - val_loss: 0.5817 - val_precision_5: 0.5407 - val_precision_focused: 0.4723 - val_recall_5: 0.4062 - learning_rate: 4.3640e-05\n",
      "Epoch 90/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5720 - auc_5: 0.5935 - f05: 0.5166 - loss: 0.5692 - precision_5: 0.5731 - precision_focused: 0.5189 - recall_5: 0.6144\n",
      "Epoch 90: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 90: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5720 - auc_5: 0.5936 - f05: 0.5165 - loss: 0.5692 - precision_5: 0.5731 - precision_focused: 0.5189 - recall_5: 0.6143 - val_accuracy: 0.5066 - val_auc_5: 0.5204 - val_f05: 0.4747 - val_loss: 0.5823 - val_precision_5: 0.5355 - val_precision_focused: 0.4743 - val_recall_5: 0.4093 - learning_rate: 4.3640e-05\n",
      "Epoch 91/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5789 - auc_5: 0.6081 - f05: 0.5176 - loss: 0.5634 - precision_5: 0.5852 - precision_focused: 0.5187 - recall_5: 0.6065\n",
      "Epoch 91: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 91: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5789 - auc_5: 0.6081 - f05: 0.5176 - loss: 0.5635 - precision_5: 0.5852 - precision_focused: 0.5187 - recall_5: 0.6065 - val_accuracy: 0.5011 - val_auc_5: 0.5210 - val_f05: 0.4642 - val_loss: 0.5832 - val_precision_5: 0.5312 - val_precision_focused: 0.4661 - val_recall_5: 0.3728 - learning_rate: 4.3640e-05\n",
      "Epoch 92/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5826 - auc_5: 0.6108 - f05: 0.5273 - loss: 0.5633 - precision_5: 0.5873 - precision_focused: 0.5302 - recall_5: 0.6372\n",
      "Epoch 92: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 92: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5826 - auc_5: 0.6108 - f05: 0.5273 - loss: 0.5633 - precision_5: 0.5873 - precision_focused: 0.5302 - recall_5: 0.6371 - val_accuracy: 0.5082 - val_auc_5: 0.5197 - val_f05: 0.4635 - val_loss: 0.5837 - val_precision_5: 0.5405 - val_precision_focused: 0.4650 - val_recall_5: 0.3833 - learning_rate: 4.3640e-05\n",
      "Epoch 93/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5797 - auc_5: 0.6049 - f05: 0.5241 - loss: 0.5656 - precision_5: 0.5836 - precision_focused: 0.5265 - recall_5: 0.6270\n",
      "Epoch 93: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 93: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5797 - auc_5: 0.6050 - f05: 0.5240 - loss: 0.5656 - precision_5: 0.5836 - precision_focused: 0.5264 - recall_5: 0.6269 - val_accuracy: 0.5066 - val_auc_5: 0.5200 - val_f05: 0.4482 - val_loss: 0.5842 - val_precision_5: 0.5440 - val_precision_focused: 0.4556 - val_recall_5: 0.3358 - learning_rate: 4.3640e-05\n",
      "Epoch 94/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5843 - auc_5: 0.6130 - f05: 0.5113 - loss: 0.5645 - precision_5: 0.5858 - precision_focused: 0.5124 - recall_5: 0.6080\n",
      "Epoch 94: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 94: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5843 - auc_5: 0.6130 - f05: 0.5113 - loss: 0.5645 - precision_5: 0.5858 - precision_focused: 0.5125 - recall_5: 0.6080 - val_accuracy: 0.5056 - val_auc_5: 0.5193 - val_f05: 0.4552 - val_loss: 0.5838 - val_precision_5: 0.5401 - val_precision_focused: 0.4598 - val_recall_5: 0.3531 - learning_rate: 4.3640e-05\n",
      "Epoch 95/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5751 - auc_5: 0.5980 - f05: 0.5124 - loss: 0.5674 - precision_5: 0.5772 - precision_focused: 0.5138 - recall_5: 0.6028\n",
      "Epoch 95: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 95: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5751 - auc_5: 0.5980 - f05: 0.5125 - loss: 0.5674 - precision_5: 0.5773 - precision_focused: 0.5138 - recall_5: 0.6028 - val_accuracy: 0.5066 - val_auc_5: 0.5173 - val_f05: 0.4632 - val_loss: 0.5835 - val_precision_5: 0.5385 - val_precision_focused: 0.4650 - val_recall_5: 0.3796 - learning_rate: 4.3640e-05\n",
      "Epoch 96/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5735 - auc_5: 0.5988 - f05: 0.5160 - loss: 0.5669 - precision_5: 0.5845 - precision_focused: 0.5164 - recall_5: 0.5904\n",
      "Epoch 96: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 96: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5735 - auc_5: 0.5988 - f05: 0.5159 - loss: 0.5669 - precision_5: 0.5845 - precision_focused: 0.5164 - recall_5: 0.5905 - val_accuracy: 0.5092 - val_auc_5: 0.5177 - val_f05: 0.4524 - val_loss: 0.5847 - val_precision_5: 0.5457 - val_precision_focused: 0.4574 - val_recall_5: 0.3537 - learning_rate: 4.3640e-05\n",
      "Epoch 97/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5853 - auc_5: 0.6123 - f05: 0.5300 - loss: 0.5615 - precision_5: 0.5950 - precision_focused: 0.5321 - recall_5: 0.6326\n",
      "Epoch 97: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 97: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5853 - auc_5: 0.6123 - f05: 0.5299 - loss: 0.5615 - precision_5: 0.5950 - precision_focused: 0.5320 - recall_5: 0.6325 - val_accuracy: 0.5076 - val_auc_5: 0.5181 - val_f05: 0.4513 - val_loss: 0.5845 - val_precision_5: 0.5440 - val_precision_focused: 0.4570 - val_recall_5: 0.3469 - learning_rate: 4.3640e-05\n",
      "Epoch 98/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5869 - auc_5: 0.6191 - f05: 0.5170 - loss: 0.5611 - precision_5: 0.5962 - precision_focused: 0.5173 - recall_5: 0.6021\n",
      "Epoch 98: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 98: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5869 - auc_5: 0.6190 - f05: 0.5169 - loss: 0.5611 - precision_5: 0.5961 - precision_focused: 0.5172 - recall_5: 0.6020 - val_accuracy: 0.5053 - val_auc_5: 0.5199 - val_f05: 0.4447 - val_loss: 0.5837 - val_precision_5: 0.5427 - val_precision_focused: 0.4529 - val_recall_5: 0.3296 - learning_rate: 4.3640e-05\n",
      "Epoch 99/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5826 - auc_5: 0.6131 - f05: 0.5051 - loss: 0.5653 - precision_5: 0.5836 - precision_focused: 0.5051 - recall_5: 0.5838\n",
      "Epoch 99: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 99: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5826 - auc_5: 0.6131 - f05: 0.5051 - loss: 0.5653 - precision_5: 0.5836 - precision_focused: 0.5052 - recall_5: 0.5839 - val_accuracy: 0.5069 - val_auc_5: 0.5199 - val_f05: 0.4702 - val_loss: 0.5831 - val_precision_5: 0.5372 - val_precision_focused: 0.4707 - val_recall_5: 0.3963 - learning_rate: 4.3640e-05\n",
      "Epoch 100/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5826 - auc_5: 0.6116 - f05: 0.5229 - loss: 0.5631 - precision_5: 0.5853 - precision_focused: 0.5260 - recall_5: 0.6399\n",
      "Epoch 100: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 100: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5826 - auc_5: 0.6116 - f05: 0.5229 - loss: 0.5631 - precision_5: 0.5853 - precision_focused: 0.5260 - recall_5: 0.6398 - val_accuracy: 0.5050 - val_auc_5: 0.5215 - val_f05: 0.4557 - val_loss: 0.5836 - val_precision_5: 0.5390 - val_precision_focused: 0.4602 - val_recall_5: 0.3537 - learning_rate: 4.3640e-05\n",
      "Epoch 101/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5808 - auc_5: 0.6189 - f05: 0.5031 - loss: 0.5634 - precision_5: 0.5819 - precision_focused: 0.5032 - recall_5: 0.5851\n",
      "Epoch 101: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 101: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.5808 - auc_5: 0.6188 - f05: 0.5031 - loss: 0.5634 - precision_5: 0.5819 - precision_focused: 0.5033 - recall_5: 0.5851 - val_accuracy: 0.5047 - val_auc_5: 0.5191 - val_f05: 0.4613 - val_loss: 0.5843 - val_precision_5: 0.5371 - val_precision_focused: 0.4642 - val_recall_5: 0.3660 - learning_rate: 4.3640e-05\n",
      "Epoch 102/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5767 - auc_5: 0.6110 - f05: 0.5164 - loss: 0.5647 - precision_5: 0.5779 - precision_focused: 0.5185 - recall_5: 0.6163\n",
      "Epoch 102: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 102: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5767 - auc_5: 0.6110 - f05: 0.5163 - loss: 0.5647 - precision_5: 0.5780 - precision_focused: 0.5184 - recall_5: 0.6162 - val_accuracy: 0.5018 - val_auc_5: 0.5206 - val_f05: 0.4583 - val_loss: 0.5841 - val_precision_5: 0.5334 - val_precision_focused: 0.4616 - val_recall_5: 0.3599 - learning_rate: 4.3640e-05\n",
      "Epoch 103/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5804 - auc_5: 0.6136 - f05: 0.5235 - loss: 0.5614 - precision_5: 0.5859 - precision_focused: 0.5258 - recall_5: 0.6263\n",
      "Epoch 103: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 103: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5804 - auc_5: 0.6136 - f05: 0.5235 - loss: 0.5614 - precision_5: 0.5859 - precision_focused: 0.5258 - recall_5: 0.6262 - val_accuracy: 0.5040 - val_auc_5: 0.5200 - val_f05: 0.4606 - val_loss: 0.5843 - val_precision_5: 0.5357 - val_precision_focused: 0.4630 - val_recall_5: 0.3704 - learning_rate: 4.3640e-05\n",
      "Epoch 104/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5809 - auc_5: 0.6116 - f05: 0.5191 - loss: 0.5620 - precision_5: 0.5914 - precision_focused: 0.5198 - recall_5: 0.6061\n",
      "Epoch 104: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 104: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5809 - auc_5: 0.6116 - f05: 0.5190 - loss: 0.5620 - precision_5: 0.5914 - precision_focused: 0.5197 - recall_5: 0.6060 - val_accuracy: 0.5008 - val_auc_5: 0.5195 - val_f05: 0.4592 - val_loss: 0.5840 - val_precision_5: 0.5317 - val_precision_focused: 0.4622 - val_recall_5: 0.3623 - learning_rate: 4.3640e-05\n",
      "Epoch 105/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5744 - auc_5: 0.6043 - f05: 0.5080 - loss: 0.5673 - precision_5: 0.5752 - precision_focused: 0.5089 - recall_5: 0.5907\n",
      "Epoch 105: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 105: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5744 - auc_5: 0.6043 - f05: 0.5080 - loss: 0.5673 - precision_5: 0.5753 - precision_focused: 0.5089 - recall_5: 0.5907 - val_accuracy: 0.4986 - val_auc_5: 0.5205 - val_f05: 0.4567 - val_loss: 0.5835 - val_precision_5: 0.5293 - val_precision_focused: 0.4607 - val_recall_5: 0.3512 - learning_rate: 4.3640e-05\n",
      "Epoch 106/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5790 - auc_5: 0.6123 - f05: 0.5045 - loss: 0.5661 - precision_5: 0.5774 - precision_focused: 0.5054 - recall_5: 0.5932\n",
      "Epoch 106: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 106: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5790 - auc_5: 0.6123 - f05: 0.5045 - loss: 0.5661 - precision_5: 0.5774 - precision_focused: 0.5054 - recall_5: 0.5932 - val_accuracy: 0.5014 - val_auc_5: 0.5184 - val_f05: 0.4618 - val_loss: 0.5845 - val_precision_5: 0.5317 - val_precision_focused: 0.4638 - val_recall_5: 0.3722 - learning_rate: 4.3640e-05\n",
      "Epoch 107/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5814 - auc_5: 0.6136 - f05: 0.5228 - loss: 0.5632 - precision_5: 0.5845 - precision_focused: 0.5254 - recall_5: 0.6307\n",
      "Epoch 107: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 107: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.491200041025877e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5814 - auc_5: 0.6136 - f05: 0.5228 - loss: 0.5632 - precision_5: 0.5845 - precision_focused: 0.5254 - recall_5: 0.6306 - val_accuracy: 0.4982 - val_auc_5: 0.5199 - val_f05: 0.4490 - val_loss: 0.5841 - val_precision_5: 0.5309 - val_precision_focused: 0.4562 - val_recall_5: 0.3290 - learning_rate: 4.3640e-05\n",
      "Epoch 108/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5958 - auc_5: 0.6237 - f05: 0.5163 - loss: 0.5606 - precision_5: 0.6057 - precision_focused: 0.5165 - recall_5: 0.6086\n",
      "Epoch 108: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 108: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5958 - auc_5: 0.6236 - f05: 0.5162 - loss: 0.5607 - precision_5: 0.6055 - precision_focused: 0.5165 - recall_5: 0.6086 - val_accuracy: 0.5024 - val_auc_5: 0.5200 - val_f05: 0.4659 - val_loss: 0.5830 - val_precision_5: 0.5328 - val_precision_focused: 0.4676 - val_recall_5: 0.3765 - learning_rate: 3.4912e-05\n",
      "Epoch 109/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5795 - auc_5: 0.6084 - f05: 0.5030 - loss: 0.5656 - precision_5: 0.5827 - precision_focused: 0.5026 - recall_5: 0.5734\n",
      "Epoch 109: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 109: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5795 - auc_5: 0.6084 - f05: 0.5030 - loss: 0.5656 - precision_5: 0.5828 - precision_focused: 0.5027 - recall_5: 0.5735 - val_accuracy: 0.4989 - val_auc_5: 0.5202 - val_f05: 0.4764 - val_loss: 0.5825 - val_precision_5: 0.5260 - val_precision_focused: 0.4761 - val_recall_5: 0.3994 - learning_rate: 3.4912e-05\n",
      "Epoch 110/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5804 - auc_5: 0.6151 - f05: 0.5175 - loss: 0.5624 - precision_5: 0.5809 - precision_focused: 0.5201 - recall_5: 0.6274\n",
      "Epoch 110: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 110: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5804 - auc_5: 0.6151 - f05: 0.5175 - loss: 0.5624 - precision_5: 0.5809 - precision_focused: 0.5201 - recall_5: 0.6272 - val_accuracy: 0.5031 - val_auc_5: 0.5203 - val_f05: 0.4519 - val_loss: 0.5843 - val_precision_5: 0.5377 - val_precision_focused: 0.4582 - val_recall_5: 0.3389 - learning_rate: 3.4912e-05\n",
      "Epoch 111/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5777 - auc_5: 0.6099 - f05: 0.5040 - loss: 0.5660 - precision_5: 0.5798 - precision_focused: 0.5038 - recall_5: 0.5733\n",
      "Epoch 111: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 111: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5777 - auc_5: 0.6098 - f05: 0.5041 - loss: 0.5660 - precision_5: 0.5798 - precision_focused: 0.5040 - recall_5: 0.5736 - val_accuracy: 0.5005 - val_auc_5: 0.5197 - val_f05: 0.4688 - val_loss: 0.5834 - val_precision_5: 0.5293 - val_precision_focused: 0.4696 - val_recall_5: 0.3852 - learning_rate: 3.4912e-05\n",
      "Epoch 112/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5871 - auc_5: 0.6112 - f05: 0.5222 - loss: 0.5625 - precision_5: 0.5927 - precision_focused: 0.5243 - recall_5: 0.6323\n",
      "Epoch 112: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 112: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5871 - auc_5: 0.6112 - f05: 0.5221 - loss: 0.5625 - precision_5: 0.5926 - precision_focused: 0.5242 - recall_5: 0.6322 - val_accuracy: 0.5085 - val_auc_5: 0.5206 - val_f05: 0.4299 - val_loss: 0.5858 - val_precision_5: 0.5543 - val_precision_focused: 0.4461 - val_recall_5: 0.2963 - learning_rate: 3.4912e-05\n",
      "Epoch 113/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5862 - auc_5: 0.6161 - f05: 0.5170 - loss: 0.5617 - precision_5: 0.5958 - precision_focused: 0.5174 - recall_5: 0.6024\n",
      "Epoch 113: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 113: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5863 - auc_5: 0.6161 - f05: 0.5169 - loss: 0.5617 - precision_5: 0.5957 - precision_focused: 0.5173 - recall_5: 0.6024 - val_accuracy: 0.5014 - val_auc_5: 0.5210 - val_f05: 0.4580 - val_loss: 0.5851 - val_precision_5: 0.5339 - val_precision_focused: 0.4626 - val_recall_5: 0.3500 - learning_rate: 3.4912e-05\n",
      "Epoch 114/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5786 - auc_5: 0.6042 - f05: 0.5166 - loss: 0.5654 - precision_5: 0.5815 - precision_focused: 0.5187 - recall_5: 0.6193\n",
      "Epoch 114: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 114: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5786 - auc_5: 0.6043 - f05: 0.5166 - loss: 0.5654 - precision_5: 0.5815 - precision_focused: 0.5187 - recall_5: 0.6192 - val_accuracy: 0.5056 - val_auc_5: 0.5209 - val_f05: 0.4390 - val_loss: 0.5862 - val_precision_5: 0.5461 - val_precision_focused: 0.4511 - val_recall_5: 0.3105 - learning_rate: 3.4912e-05\n",
      "Epoch 115/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5857 - auc_5: 0.6152 - f05: 0.5163 - loss: 0.5631 - precision_5: 0.5884 - precision_focused: 0.5179 - recall_5: 0.6193\n",
      "Epoch 115: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 115: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5857 - auc_5: 0.6152 - f05: 0.5163 - loss: 0.5631 - precision_5: 0.5884 - precision_focused: 0.5179 - recall_5: 0.6192 - val_accuracy: 0.5014 - val_auc_5: 0.5209 - val_f05: 0.4398 - val_loss: 0.5860 - val_precision_5: 0.5385 - val_precision_focused: 0.4510 - val_recall_5: 0.3105 - learning_rate: 3.4912e-05\n",
      "Epoch 116/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5840 - auc_5: 0.6181 - f05: 0.5079 - loss: 0.5637 - precision_5: 0.5832 - precision_focused: 0.5088 - recall_5: 0.6011\n",
      "Epoch 116: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 116: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5840 - auc_5: 0.6181 - f05: 0.5080 - loss: 0.5637 - precision_5: 0.5833 - precision_focused: 0.5089 - recall_5: 0.6012 - val_accuracy: 0.5037 - val_auc_5: 0.5199 - val_f05: 0.4481 - val_loss: 0.5854 - val_precision_5: 0.5403 - val_precision_focused: 0.4567 - val_recall_5: 0.3265 - learning_rate: 3.4912e-05\n",
      "Epoch 117/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5822 - auc_5: 0.6147 - f05: 0.5091 - loss: 0.5648 - precision_5: 0.5811 - precision_focused: 0.5103 - recall_5: 0.6036\n",
      "Epoch 117: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 117: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5823 - auc_5: 0.6148 - f05: 0.5092 - loss: 0.5648 - precision_5: 0.5812 - precision_focused: 0.5103 - recall_5: 0.6036 - val_accuracy: 0.5060 - val_auc_5: 0.5184 - val_f05: 0.4552 - val_loss: 0.5855 - val_precision_5: 0.5414 - val_precision_focused: 0.4607 - val_recall_5: 0.3469 - learning_rate: 3.4912e-05\n",
      "Epoch 118/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5863 - auc_5: 0.6212 - f05: 0.5167 - loss: 0.5612 - precision_5: 0.5870 - precision_focused: 0.5192 - recall_5: 0.6310\n",
      "Epoch 118: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 118: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5863 - auc_5: 0.6211 - f05: 0.5167 - loss: 0.5612 - precision_5: 0.5870 - precision_focused: 0.5192 - recall_5: 0.6307 - val_accuracy: 0.5031 - val_auc_5: 0.5183 - val_f05: 0.4642 - val_loss: 0.5846 - val_precision_5: 0.5345 - val_precision_focused: 0.4667 - val_recall_5: 0.3685 - learning_rate: 3.4912e-05\n",
      "Epoch 119/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5830 - auc_5: 0.6167 - f05: 0.5308 - loss: 0.5603 - precision_5: 0.5885 - precision_focused: 0.5339 - recall_5: 0.6434\n",
      "Epoch 119: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 119: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5830 - auc_5: 0.6167 - f05: 0.5307 - loss: 0.5603 - precision_5: 0.5885 - precision_focused: 0.5339 - recall_5: 0.6433 - val_accuracy: 0.5089 - val_auc_5: 0.5196 - val_f05: 0.4241 - val_loss: 0.5887 - val_precision_5: 0.5570 - val_precision_focused: 0.4430 - val_recall_5: 0.2864 - learning_rate: 3.4912e-05\n",
      "Epoch 120/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5816 - auc_5: 0.6169 - f05: 0.5113 - loss: 0.5616 - precision_5: 0.5900 - precision_focused: 0.5112 - recall_5: 0.5851\n",
      "Epoch 120: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 120: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5815 - auc_5: 0.6169 - f05: 0.5113 - loss: 0.5616 - precision_5: 0.5899 - precision_focused: 0.5112 - recall_5: 0.5852 - val_accuracy: 0.5040 - val_auc_5: 0.5183 - val_f05: 0.4441 - val_loss: 0.5866 - val_precision_5: 0.5417 - val_precision_focused: 0.4537 - val_recall_5: 0.3210 - learning_rate: 3.4912e-05\n",
      "Epoch 121/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5806 - auc_5: 0.6093 - f05: 0.5172 - loss: 0.5634 - precision_5: 0.5863 - precision_focused: 0.5184 - recall_5: 0.6103\n",
      "Epoch 121: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 121: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5806 - auc_5: 0.6093 - f05: 0.5171 - loss: 0.5634 - precision_5: 0.5863 - precision_focused: 0.5184 - recall_5: 0.6103 - val_accuracy: 0.5011 - val_auc_5: 0.5204 - val_f05: 0.4488 - val_loss: 0.5858 - val_precision_5: 0.5360 - val_precision_focused: 0.4570 - val_recall_5: 0.3259 - learning_rate: 3.4912e-05\n",
      "Epoch 122/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5894 - auc_5: 0.6288 - f05: 0.5201 - loss: 0.5578 - precision_5: 0.5913 - precision_focused: 0.5223 - recall_5: 0.6322\n",
      "Epoch 122: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 122: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5894 - auc_5: 0.6287 - f05: 0.5201 - loss: 0.5578 - precision_5: 0.5913 - precision_focused: 0.5223 - recall_5: 0.6320 - val_accuracy: 0.5014 - val_auc_5: 0.5197 - val_f05: 0.4652 - val_loss: 0.5849 - val_precision_5: 0.5322 - val_precision_focused: 0.4677 - val_recall_5: 0.3673 - learning_rate: 3.4912e-05\n",
      "Epoch 123/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5881 - auc_5: 0.6211 - f05: 0.5174 - loss: 0.5610 - precision_5: 0.5916 - precision_focused: 0.5189 - recall_5: 0.6199\n",
      "Epoch 123: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 123: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5881 - auc_5: 0.6211 - f05: 0.5174 - loss: 0.5610 - precision_5: 0.5916 - precision_focused: 0.5188 - recall_5: 0.6198 - val_accuracy: 0.5005 - val_auc_5: 0.5189 - val_f05: 0.4550 - val_loss: 0.5857 - val_precision_5: 0.5334 - val_precision_focused: 0.4608 - val_recall_5: 0.3401 - learning_rate: 3.4912e-05\n",
      "Epoch 124/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5893 - auc_5: 0.6256 - f05: 0.5163 - loss: 0.5593 - precision_5: 0.5906 - precision_focused: 0.5180 - recall_5: 0.6234\n",
      "Epoch 124: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 124: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5893 - auc_5: 0.6256 - f05: 0.5163 - loss: 0.5593 - precision_5: 0.5906 - precision_focused: 0.5180 - recall_5: 0.6234 - val_accuracy: 0.4973 - val_auc_5: 0.5193 - val_f05: 0.4512 - val_loss: 0.5862 - val_precision_5: 0.5298 - val_precision_focused: 0.4591 - val_recall_5: 0.3241 - learning_rate: 3.4912e-05\n",
      "Epoch 125/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5886 - auc_5: 0.6234 - f05: 0.5032 - loss: 0.5626 - precision_5: 0.5847 - precision_focused: 0.5042 - recall_5: 0.6023\n",
      "Epoch 125: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 125: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5886 - auc_5: 0.6234 - f05: 0.5033 - loss: 0.5626 - precision_5: 0.5847 - precision_focused: 0.5043 - recall_5: 0.6023 - val_accuracy: 0.5021 - val_auc_5: 0.5187 - val_f05: 0.4714 - val_loss: 0.5857 - val_precision_5: 0.5310 - val_precision_focused: 0.4719 - val_recall_5: 0.3907 - learning_rate: 3.4912e-05\n",
      "Epoch 126/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5881 - auc_5: 0.6179 - f05: 0.5143 - loss: 0.5612 - precision_5: 0.5902 - precision_focused: 0.5159 - recall_5: 0.6208\n",
      "Epoch 126: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 126: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5881 - auc_5: 0.6179 - f05: 0.5143 - loss: 0.5612 - precision_5: 0.5902 - precision_focused: 0.5159 - recall_5: 0.6207 - val_accuracy: 0.5011 - val_auc_5: 0.5188 - val_f05: 0.4595 - val_loss: 0.5863 - val_precision_5: 0.5326 - val_precision_focused: 0.4630 - val_recall_5: 0.3580 - learning_rate: 3.4912e-05\n",
      "Epoch 127/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5868 - auc_5: 0.6216 - f05: 0.5256 - loss: 0.5582 - precision_5: 0.5914 - precision_focused: 0.5280 - recall_5: 0.6363\n",
      "Epoch 127: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 127: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5867 - auc_5: 0.6215 - f05: 0.5254 - loss: 0.5583 - precision_5: 0.5913 - precision_focused: 0.5279 - recall_5: 0.6359 - val_accuracy: 0.5018 - val_auc_5: 0.5193 - val_f05: 0.4556 - val_loss: 0.5858 - val_precision_5: 0.5348 - val_precision_focused: 0.4607 - val_recall_5: 0.3463 - learning_rate: 3.4912e-05\n",
      "Epoch 128/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5881 - auc_5: 0.6223 - f05: 0.5165 - loss: 0.5608 - precision_5: 0.5904 - precision_focused: 0.5179 - recall_5: 0.6172\n",
      "Epoch 128: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 128: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5881 - auc_5: 0.6223 - f05: 0.5165 - loss: 0.5608 - precision_5: 0.5904 - precision_focused: 0.5179 - recall_5: 0.6172 - val_accuracy: 0.5043 - val_auc_5: 0.5192 - val_f05: 0.4468 - val_loss: 0.5875 - val_precision_5: 0.5405 - val_precision_focused: 0.4542 - val_recall_5: 0.3333 - learning_rate: 3.4912e-05\n",
      "Epoch 129/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5879 - auc_5: 0.6209 - f05: 0.5166 - loss: 0.5600 - precision_5: 0.5980 - precision_focused: 0.5166 - recall_5: 0.5973\n",
      "Epoch 129: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 129: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5878 - auc_5: 0.6210 - f05: 0.5165 - loss: 0.5600 - precision_5: 0.5979 - precision_focused: 0.5166 - recall_5: 0.5972 - val_accuracy: 0.5014 - val_auc_5: 0.5177 - val_f05: 0.4691 - val_loss: 0.5859 - val_precision_5: 0.5305 - val_precision_focused: 0.4698 - val_recall_5: 0.3870 - learning_rate: 3.4912e-05\n",
      "Epoch 130/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5958 - auc_5: 0.6320 - f05: 0.5151 - loss: 0.5573 - precision_5: 0.6015 - precision_focused: 0.5162 - recall_5: 0.6227\n",
      "Epoch 130: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 130: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5958 - auc_5: 0.6319 - f05: 0.5151 - loss: 0.5573 - precision_5: 0.6014 - precision_focused: 0.5162 - recall_5: 0.6226 - val_accuracy: 0.5011 - val_auc_5: 0.5187 - val_f05: 0.4661 - val_loss: 0.5856 - val_precision_5: 0.5315 - val_precision_focused: 0.4683 - val_recall_5: 0.3698 - learning_rate: 3.4912e-05\n",
      "Epoch 131/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5786 - auc_5: 0.6179 - f05: 0.5012 - loss: 0.5623 - precision_5: 0.5810 - precision_focused: 0.5011 - recall_5: 0.5748\n",
      "Epoch 131: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 131: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5786 - auc_5: 0.6179 - f05: 0.5012 - loss: 0.5623 - precision_5: 0.5811 - precision_focused: 0.5011 - recall_5: 0.5749 - val_accuracy: 0.5018 - val_auc_5: 0.5193 - val_f05: 0.4696 - val_loss: 0.5847 - val_precision_5: 0.5320 - val_precision_focused: 0.4714 - val_recall_5: 0.3747 - learning_rate: 3.4912e-05\n",
      "Epoch 132/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5872 - auc_5: 0.6274 - f05: 0.5234 - loss: 0.5581 - precision_5: 0.5902 - precision_focused: 0.5262 - recall_5: 0.6396\n",
      "Epoch 132: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 132: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5872 - auc_5: 0.6274 - f05: 0.5233 - loss: 0.5582 - precision_5: 0.5902 - precision_focused: 0.5261 - recall_5: 0.6394 - val_accuracy: 0.4989 - val_auc_5: 0.5196 - val_f05: 0.4436 - val_loss: 0.5871 - val_precision_5: 0.5336 - val_precision_focused: 0.4538 - val_recall_5: 0.3136 - learning_rate: 3.4912e-05\n",
      "Epoch 133/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5834 - auc_5: 0.6212 - f05: 0.5225 - loss: 0.5588 - precision_5: 0.5899 - precision_focused: 0.5242 - recall_5: 0.6215\n",
      "Epoch 133: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 133: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5834 - auc_5: 0.6212 - f05: 0.5225 - loss: 0.5588 - precision_5: 0.5899 - precision_focused: 0.5241 - recall_5: 0.6215 - val_accuracy: 0.4992 - val_auc_5: 0.5198 - val_f05: 0.4496 - val_loss: 0.5876 - val_precision_5: 0.5327 - val_precision_focused: 0.4573 - val_recall_5: 0.3272 - learning_rate: 3.4912e-05\n",
      "Epoch 134/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5904 - auc_5: 0.6275 - f05: 0.5129 - loss: 0.5594 - precision_5: 0.5948 - precision_focused: 0.5138 - recall_5: 0.6126\n",
      "Epoch 134: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 134: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5904 - auc_5: 0.6275 - f05: 0.5129 - loss: 0.5594 - precision_5: 0.5948 - precision_focused: 0.5138 - recall_5: 0.6125 - val_accuracy: 0.5008 - val_auc_5: 0.5195 - val_f05: 0.4688 - val_loss: 0.5861 - val_precision_5: 0.5304 - val_precision_focused: 0.4703 - val_recall_5: 0.3765 - learning_rate: 3.4912e-05\n",
      "Epoch 135/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5847 - auc_5: 0.6225 - f05: 0.5089 - loss: 0.5608 - precision_5: 0.5851 - precision_focused: 0.5099 - recall_5: 0.6052\n",
      "Epoch 135: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 135: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5847 - auc_5: 0.6225 - f05: 0.5089 - loss: 0.5608 - precision_5: 0.5851 - precision_focused: 0.5099 - recall_5: 0.6052 - val_accuracy: 0.4957 - val_auc_5: 0.5206 - val_f05: 0.4467 - val_loss: 0.5869 - val_precision_5: 0.5282 - val_precision_focused: 0.4565 - val_recall_5: 0.3123 - learning_rate: 3.4912e-05\n",
      "Epoch 136/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5907 - auc_5: 0.6256 - f05: 0.5085 - loss: 0.5605 - precision_5: 0.5943 - precision_focused: 0.5089 - recall_5: 0.6011\n",
      "Epoch 136: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 136: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5907 - auc_5: 0.6256 - f05: 0.5086 - loss: 0.5605 - precision_5: 0.5943 - precision_focused: 0.5089 - recall_5: 0.6011 - val_accuracy: 0.5027 - val_auc_5: 0.5204 - val_f05: 0.4286 - val_loss: 0.5890 - val_precision_5: 0.5455 - val_precision_focused: 0.4475 - val_recall_5: 0.2815 - learning_rate: 3.4912e-05\n",
      "Epoch 137/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5873 - auc_5: 0.6244 - f05: 0.5184 - loss: 0.5583 - precision_5: 0.5975 - precision_focused: 0.5187 - recall_5: 0.6040\n",
      "Epoch 137: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 137: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5873 - auc_5: 0.6244 - f05: 0.5183 - loss: 0.5583 - precision_5: 0.5975 - precision_focused: 0.5186 - recall_5: 0.6039 - val_accuracy: 0.5005 - val_auc_5: 0.5187 - val_f05: 0.4547 - val_loss: 0.5876 - val_precision_5: 0.5335 - val_precision_focused: 0.4606 - val_recall_5: 0.3395 - learning_rate: 3.4912e-05\n",
      "Epoch 138/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5905 - auc_5: 0.6291 - f05: 0.5072 - loss: 0.5590 - precision_5: 0.5906 - precision_focused: 0.5079 - recall_5: 0.6033\n",
      "Epoch 138: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 138: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5905 - auc_5: 0.6291 - f05: 0.5072 - loss: 0.5590 - precision_5: 0.5906 - precision_focused: 0.5079 - recall_5: 0.6033 - val_accuracy: 0.5056 - val_auc_5: 0.5180 - val_f05: 0.4825 - val_loss: 0.5856 - val_precision_5: 0.5338 - val_precision_focused: 0.4818 - val_recall_5: 0.4142 - learning_rate: 3.4912e-05\n",
      "Epoch 139/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5984 - auc_5: 0.6330 - f05: 0.5237 - loss: 0.5560 - precision_5: 0.5975 - precision_focused: 0.5274 - recall_5: 0.6602\n",
      "Epoch 139: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 139: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5984 - auc_5: 0.6329 - f05: 0.5236 - loss: 0.5560 - precision_5: 0.5975 - precision_focused: 0.5273 - recall_5: 0.6600 - val_accuracy: 0.5008 - val_auc_5: 0.5193 - val_f05: 0.4345 - val_loss: 0.5884 - val_precision_5: 0.5400 - val_precision_focused: 0.4499 - val_recall_5: 0.2920 - learning_rate: 3.4912e-05\n",
      "Epoch 140/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5865 - auc_5: 0.6216 - f05: 0.5148 - loss: 0.5598 - precision_5: 0.5955 - precision_focused: 0.5151 - recall_5: 0.6010\n",
      "Epoch 140: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 140: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5866 - auc_5: 0.6217 - f05: 0.5147 - loss: 0.5598 - precision_5: 0.5955 - precision_focused: 0.5150 - recall_5: 0.6010 - val_accuracy: 0.4944 - val_auc_5: 0.5186 - val_f05: 0.4517 - val_loss: 0.5874 - val_precision_5: 0.5252 - val_precision_focused: 0.4595 - val_recall_5: 0.3222 - learning_rate: 3.4912e-05\n",
      "Epoch 141/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5893 - auc_5: 0.6260 - f05: 0.5100 - loss: 0.5603 - precision_5: 0.5924 - precision_focused: 0.5105 - recall_5: 0.6043\n",
      "Epoch 141: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 141: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5893 - auc_5: 0.6260 - f05: 0.5100 - loss: 0.5603 - precision_5: 0.5924 - precision_focused: 0.5106 - recall_5: 0.6043 - val_accuracy: 0.4979 - val_auc_5: 0.5198 - val_f05: 0.4439 - val_loss: 0.5870 - val_precision_5: 0.5329 - val_precision_focused: 0.4557 - val_recall_5: 0.3049 - learning_rate: 3.4912e-05\n",
      "Epoch 142/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5970 - auc_5: 0.6338 - f05: 0.5209 - loss: 0.5566 - precision_5: 0.6000 - precision_focused: 0.5230 - recall_5: 0.6394\n",
      "Epoch 142: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 142: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5969 - auc_5: 0.6337 - f05: 0.5209 - loss: 0.5566 - precision_5: 0.5999 - precision_focused: 0.5229 - recall_5: 0.6392 - val_accuracy: 0.4963 - val_auc_5: 0.5201 - val_f05: 0.4616 - val_loss: 0.5859 - val_precision_5: 0.5261 - val_precision_focused: 0.4657 - val_recall_5: 0.3481 - learning_rate: 3.4912e-05\n",
      "Epoch 143/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5939 - auc_5: 0.6296 - f05: 0.5093 - loss: 0.5589 - precision_5: 0.5952 - precision_focused: 0.5101 - recall_5: 0.6108\n",
      "Epoch 143: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 143: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5939 - auc_5: 0.6296 - f05: 0.5093 - loss: 0.5589 - precision_5: 0.5952 - precision_focused: 0.5101 - recall_5: 0.6108 - val_accuracy: 0.4950 - val_auc_5: 0.5202 - val_f05: 0.4594 - val_loss: 0.5861 - val_precision_5: 0.5248 - val_precision_focused: 0.4645 - val_recall_5: 0.3395 - learning_rate: 3.4912e-05\n",
      "Epoch 144/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5941 - auc_5: 0.6368 - f05: 0.5235 - loss: 0.5547 - precision_5: 0.6003 - precision_focused: 0.5253 - recall_5: 0.6340\n",
      "Epoch 144: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 144: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5941 - auc_5: 0.6367 - f05: 0.5234 - loss: 0.5547 - precision_5: 0.6003 - precision_focused: 0.5252 - recall_5: 0.6338 - val_accuracy: 0.4950 - val_auc_5: 0.5202 - val_f05: 0.4489 - val_loss: 0.5872 - val_precision_5: 0.5268 - val_precision_focused: 0.4580 - val_recall_5: 0.3154 - learning_rate: 3.4912e-05\n",
      "Epoch 145/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6009 - auc_5: 0.6290 - f05: 0.5246 - loss: 0.5562 - precision_5: 0.6085 - precision_focused: 0.5258 - recall_5: 0.6333\n",
      "Epoch 145: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 145: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6009 - auc_5: 0.6290 - f05: 0.5246 - loss: 0.5563 - precision_5: 0.6085 - precision_focused: 0.5258 - recall_5: 0.6332 - val_accuracy: 0.5018 - val_auc_5: 0.5198 - val_f05: 0.4345 - val_loss: 0.5894 - val_precision_5: 0.5417 - val_precision_focused: 0.4500 - val_recall_5: 0.2926 - learning_rate: 3.4912e-05\n",
      "Epoch 146/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5989 - auc_5: 0.6374 - f05: 0.5113 - loss: 0.5552 - precision_5: 0.6081 - precision_focused: 0.5110 - recall_5: 0.5949\n",
      "Epoch 146: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 146: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5989 - auc_5: 0.6374 - f05: 0.5113 - loss: 0.5552 - precision_5: 0.6081 - precision_focused: 0.5110 - recall_5: 0.5949 - val_accuracy: 0.4969 - val_auc_5: 0.5184 - val_f05: 0.4474 - val_loss: 0.5888 - val_precision_5: 0.5305 - val_precision_focused: 0.4576 - val_recall_5: 0.3117 - learning_rate: 3.4912e-05\n",
      "Epoch 147/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5881 - auc_5: 0.6262 - f05: 0.5115 - loss: 0.5583 - precision_5: 0.5952 - precision_focused: 0.5119 - recall_5: 0.6030\n",
      "Epoch 147: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 147: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5881 - auc_5: 0.6262 - f05: 0.5115 - loss: 0.5583 - precision_5: 0.5951 - precision_focused: 0.5119 - recall_5: 0.6029 - val_accuracy: 0.5014 - val_auc_5: 0.5189 - val_f05: 0.4535 - val_loss: 0.5880 - val_precision_5: 0.5359 - val_precision_focused: 0.4607 - val_recall_5: 0.3321 - learning_rate: 3.4912e-05\n",
      "Epoch 148/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5994 - auc_5: 0.6382 - f05: 0.5145 - loss: 0.5539 - precision_5: 0.6089 - precision_focused: 0.5147 - recall_5: 0.6123\n",
      "Epoch 148: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 148: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5994 - auc_5: 0.6382 - f05: 0.5145 - loss: 0.5540 - precision_5: 0.6088 - precision_focused: 0.5146 - recall_5: 0.6123 - val_accuracy: 0.4966 - val_auc_5: 0.5190 - val_f05: 0.4539 - val_loss: 0.5872 - val_precision_5: 0.5284 - val_precision_focused: 0.4612 - val_recall_5: 0.3272 - learning_rate: 3.4912e-05\n",
      "Epoch 149/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5906 - auc_5: 0.6223 - f05: 0.5243 - loss: 0.5581 - precision_5: 0.5982 - precision_focused: 0.5259 - recall_5: 0.6295\n",
      "Epoch 149: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 149: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5906 - auc_5: 0.6224 - f05: 0.5243 - loss: 0.5581 - precision_5: 0.5982 - precision_focused: 0.5258 - recall_5: 0.6294 - val_accuracy: 0.5002 - val_auc_5: 0.5177 - val_f05: 0.4413 - val_loss: 0.5890 - val_precision_5: 0.5374 - val_precision_focused: 0.4543 - val_recall_5: 0.3019 - learning_rate: 3.4912e-05\n",
      "Epoch 150/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5931 - auc_5: 0.6261 - f05: 0.5147 - loss: 0.5589 - precision_5: 0.6001 - precision_focused: 0.5152 - recall_5: 0.6099\n",
      "Epoch 150: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 150: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5931 - auc_5: 0.6261 - f05: 0.5147 - loss: 0.5589 - precision_5: 0.6001 - precision_focused: 0.5152 - recall_5: 0.6099 - val_accuracy: 0.5018 - val_auc_5: 0.5189 - val_f05: 0.4489 - val_loss: 0.5873 - val_precision_5: 0.5382 - val_precision_focused: 0.4590 - val_recall_5: 0.3173 - learning_rate: 3.4912e-05\n",
      "Epoch 151/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5952 - auc_5: 0.6354 - f05: 0.5123 - loss: 0.5566 - precision_5: 0.5996 - precision_focused: 0.5127 - recall_5: 0.6071\n",
      "Epoch 151: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 151: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5952 - auc_5: 0.6353 - f05: 0.5123 - loss: 0.5566 - precision_5: 0.5996 - precision_focused: 0.5127 - recall_5: 0.6070 - val_accuracy: 0.4986 - val_auc_5: 0.5191 - val_f05: 0.4571 - val_loss: 0.5866 - val_precision_5: 0.5306 - val_precision_focused: 0.4631 - val_recall_5: 0.3370 - learning_rate: 3.4912e-05\n",
      "Epoch 152/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5891 - auc_5: 0.6234 - f05: 0.5128 - loss: 0.5612 - precision_5: 0.5892 - precision_focused: 0.5141 - recall_5: 0.6145\n",
      "Epoch 152: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 152: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5891 - auc_5: 0.6234 - f05: 0.5128 - loss: 0.5612 - precision_5: 0.5892 - precision_focused: 0.5141 - recall_5: 0.6144 - val_accuracy: 0.4944 - val_auc_5: 0.5199 - val_f05: 0.4638 - val_loss: 0.5851 - val_precision_5: 0.5231 - val_precision_focused: 0.4676 - val_recall_5: 0.3488 - learning_rate: 3.4912e-05\n",
      "Epoch 153/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5993 - auc_5: 0.6365 - f05: 0.5083 - loss: 0.5564 - precision_5: 0.6052 - precision_focused: 0.5082 - recall_5: 0.6012\n",
      "Epoch 153: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 153: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5992 - auc_5: 0.6364 - f05: 0.5083 - loss: 0.5564 - precision_5: 0.6051 - precision_focused: 0.5082 - recall_5: 0.6012 - val_accuracy: 0.4986 - val_auc_5: 0.5199 - val_f05: 0.4524 - val_loss: 0.5860 - val_precision_5: 0.5316 - val_precision_focused: 0.4600 - val_recall_5: 0.3272 - learning_rate: 3.4912e-05\n",
      "Epoch 154/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5908 - auc_5: 0.6277 - f05: 0.5119 - loss: 0.5597 - precision_5: 0.5947 - precision_focused: 0.5123 - recall_5: 0.6010\n",
      "Epoch 154: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 154: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5909 - auc_5: 0.6277 - f05: 0.5119 - loss: 0.5597 - precision_5: 0.5947 - precision_focused: 0.5123 - recall_5: 0.6011 - val_accuracy: 0.4989 - val_auc_5: 0.5178 - val_f05: 0.4731 - val_loss: 0.5860 - val_precision_5: 0.5276 - val_precision_focused: 0.4744 - val_recall_5: 0.3778 - learning_rate: 3.4912e-05\n",
      "Epoch 155/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5981 - auc_5: 0.6345 - f05: 0.5159 - loss: 0.5563 - precision_5: 0.5979 - precision_focused: 0.5179 - recall_5: 0.6344\n",
      "Epoch 155: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 155: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5981 - auc_5: 0.6345 - f05: 0.5159 - loss: 0.5563 - precision_5: 0.5980 - precision_focused: 0.5178 - recall_5: 0.6343 - val_accuracy: 0.5031 - val_auc_5: 0.5195 - val_f05: 0.4415 - val_loss: 0.5887 - val_precision_5: 0.5419 - val_precision_focused: 0.4539 - val_recall_5: 0.3074 - learning_rate: 3.4912e-05\n",
      "Epoch 156/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5977 - auc_5: 0.6336 - f05: 0.5178 - loss: 0.5561 - precision_5: 0.6077 - precision_focused: 0.5180 - recall_5: 0.6108\n",
      "Epoch 156: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 156: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.5976 - auc_5: 0.6336 - f05: 0.5178 - loss: 0.5561 - precision_5: 0.6077 - precision_focused: 0.5180 - recall_5: 0.6107 - val_accuracy: 0.5021 - val_auc_5: 0.5191 - val_f05: 0.4509 - val_loss: 0.5878 - val_precision_5: 0.5378 - val_precision_focused: 0.4595 - val_recall_5: 0.3247 - learning_rate: 3.4912e-05\n",
      "Epoch 157/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5983 - auc_5: 0.6361 - f05: 0.5198 - loss: 0.5561 - precision_5: 0.6016 - precision_focused: 0.5217 - recall_5: 0.6362\n",
      "Epoch 157: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 157: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.7929601492360236e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5982 - auc_5: 0.6361 - f05: 0.5198 - loss: 0.5562 - precision_5: 0.6016 - precision_focused: 0.5216 - recall_5: 0.6360 - val_accuracy: 0.5053 - val_auc_5: 0.5182 - val_f05: 0.4587 - val_loss: 0.5877 - val_precision_5: 0.5405 - val_precision_focused: 0.4644 - val_recall_5: 0.3457 - learning_rate: 3.4912e-05\n",
      "Epoch 158/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5968 - auc_5: 0.6313 - f05: 0.5249 - loss: 0.5560 - precision_5: 0.6027 - precision_focused: 0.5271 - recall_5: 0.6437\n",
      "Epoch 158: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 158: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5968 - auc_5: 0.6313 - f05: 0.5248 - loss: 0.5560 - precision_5: 0.6027 - precision_focused: 0.5270 - recall_5: 0.6436 - val_accuracy: 0.5024 - val_auc_5: 0.5194 - val_f05: 0.4381 - val_loss: 0.5900 - val_precision_5: 0.5415 - val_precision_focused: 0.4515 - val_recall_5: 0.3019 - learning_rate: 2.7930e-05\n",
      "Epoch 159/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5934 - auc_5: 0.6292 - f05: 0.5026 - loss: 0.5593 - precision_5: 0.5974 - precision_focused: 0.5023 - recall_5: 0.5873\n",
      "Epoch 159: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 159: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5934 - auc_5: 0.6292 - f05: 0.5027 - loss: 0.5593 - precision_5: 0.5974 - precision_focused: 0.5023 - recall_5: 0.5873 - val_accuracy: 0.4992 - val_auc_5: 0.5190 - val_f05: 0.4530 - val_loss: 0.5887 - val_precision_5: 0.5323 - val_precision_focused: 0.4602 - val_recall_5: 0.3302 - learning_rate: 2.7930e-05\n",
      "Epoch 160/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5890 - auc_5: 0.6305 - f05: 0.5121 - loss: 0.5571 - precision_5: 0.5930 - precision_focused: 0.5129 - recall_5: 0.6087\n",
      "Epoch 160: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 160: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5890 - auc_5: 0.6305 - f05: 0.5121 - loss: 0.5571 - precision_5: 0.5931 - precision_focused: 0.5129 - recall_5: 0.6087 - val_accuracy: 0.5011 - val_auc_5: 0.5196 - val_f05: 0.4419 - val_loss: 0.5892 - val_precision_5: 0.5380 - val_precision_focused: 0.4531 - val_recall_5: 0.3105 - learning_rate: 2.7930e-05\n",
      "Epoch 161/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5922 - auc_5: 0.6269 - f05: 0.5070 - loss: 0.5599 - precision_5: 0.5947 - precision_focused: 0.5072 - recall_5: 0.5972\n",
      "Epoch 161: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 161: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5922 - auc_5: 0.6270 - f05: 0.5070 - loss: 0.5599 - precision_5: 0.5947 - precision_focused: 0.5072 - recall_5: 0.5972 - val_accuracy: 0.5021 - val_auc_5: 0.5187 - val_f05: 0.4710 - val_loss: 0.5871 - val_precision_5: 0.5325 - val_precision_focused: 0.4729 - val_recall_5: 0.3747 - learning_rate: 2.7930e-05\n",
      "Epoch 162/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6014 - auc_5: 0.6441 - f05: 0.5140 - loss: 0.5541 - precision_5: 0.5980 - precision_focused: 0.5165 - recall_5: 0.6433\n",
      "Epoch 162: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 162: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6014 - auc_5: 0.6440 - f05: 0.5140 - loss: 0.5541 - precision_5: 0.5980 - precision_focused: 0.5165 - recall_5: 0.6431 - val_accuracy: 0.4982 - val_auc_5: 0.5194 - val_f05: 0.4458 - val_loss: 0.5894 - val_precision_5: 0.5328 - val_precision_focused: 0.4563 - val_recall_5: 0.3111 - learning_rate: 2.7930e-05\n",
      "Epoch 163/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5921 - auc_5: 0.6296 - f05: 0.5048 - loss: 0.5588 - precision_5: 0.5927 - precision_focused: 0.5052 - recall_5: 0.5996\n",
      "Epoch 163: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 163: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5921 - auc_5: 0.6297 - f05: 0.5048 - loss: 0.5587 - precision_5: 0.5928 - precision_focused: 0.5053 - recall_5: 0.5996 - val_accuracy: 0.5002 - val_auc_5: 0.5196 - val_f05: 0.4521 - val_loss: 0.5888 - val_precision_5: 0.5348 - val_precision_focused: 0.4609 - val_recall_5: 0.3222 - learning_rate: 2.7930e-05\n",
      "Epoch 164/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5919 - auc_5: 0.6330 - f05: 0.5192 - loss: 0.5558 - precision_5: 0.5974 - precision_focused: 0.5209 - recall_5: 0.6270\n",
      "Epoch 164: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 164: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.5919 - auc_5: 0.6330 - f05: 0.5192 - loss: 0.5558 - precision_5: 0.5974 - precision_focused: 0.5208 - recall_5: 0.6269 - val_accuracy: 0.4979 - val_auc_5: 0.5195 - val_f05: 0.4557 - val_loss: 0.5883 - val_precision_5: 0.5302 - val_precision_focused: 0.4626 - val_recall_5: 0.3309 - learning_rate: 2.7930e-05\n",
      "Epoch 165/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5970 - auc_5: 0.6384 - f05: 0.5216 - loss: 0.5541 - precision_5: 0.6026 - precision_focused: 0.5231 - recall_5: 0.6328\n",
      "Epoch 165: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 165: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5970 - auc_5: 0.6384 - f05: 0.5216 - loss: 0.5542 - precision_5: 0.6026 - precision_focused: 0.5231 - recall_5: 0.6328 - val_accuracy: 0.5002 - val_auc_5: 0.5185 - val_f05: 0.4426 - val_loss: 0.5903 - val_precision_5: 0.5369 - val_precision_focused: 0.4547 - val_recall_5: 0.3056 - learning_rate: 2.7930e-05\n",
      "Epoch 166/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5950 - auc_5: 0.6358 - f05: 0.5072 - loss: 0.5557 - precision_5: 0.5990 - precision_focused: 0.5071 - recall_5: 0.5967\n",
      "Epoch 166: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 166: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5950 - auc_5: 0.6357 - f05: 0.5072 - loss: 0.5557 - precision_5: 0.5990 - precision_focused: 0.5072 - recall_5: 0.5967 - val_accuracy: 0.5050 - val_auc_5: 0.5180 - val_f05: 0.4464 - val_loss: 0.5905 - val_precision_5: 0.5437 - val_precision_focused: 0.4568 - val_recall_5: 0.3185 - learning_rate: 2.7930e-05\n",
      "Epoch 167/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6030 - auc_5: 0.6426 - f05: 0.5144 - loss: 0.5527 - precision_5: 0.6092 - precision_focused: 0.5148 - recall_5: 0.6181\n",
      "Epoch 167: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 167: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6030 - auc_5: 0.6426 - f05: 0.5144 - loss: 0.5527 - precision_5: 0.6092 - precision_focused: 0.5148 - recall_5: 0.6181 - val_accuracy: 0.5018 - val_auc_5: 0.5176 - val_f05: 0.4448 - val_loss: 0.5914 - val_precision_5: 0.5385 - val_precision_focused: 0.4551 - val_recall_5: 0.3154 - learning_rate: 2.7930e-05\n",
      "Epoch 168/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5941 - auc_5: 0.6306 - f05: 0.5078 - loss: 0.5581 - precision_5: 0.6009 - precision_focused: 0.5075 - recall_5: 0.5910\n",
      "Epoch 168: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 168: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5941 - auc_5: 0.6306 - f05: 0.5078 - loss: 0.5581 - precision_5: 0.6009 - precision_focused: 0.5075 - recall_5: 0.5911 - val_accuracy: 0.5034 - val_auc_5: 0.5172 - val_f05: 0.4456 - val_loss: 0.5916 - val_precision_5: 0.5415 - val_precision_focused: 0.4565 - val_recall_5: 0.3142 - learning_rate: 2.7930e-05\n",
      "Epoch 169/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5974 - auc_5: 0.6354 - f05: 0.5136 - loss: 0.5565 - precision_5: 0.6000 - precision_focused: 0.5148 - recall_5: 0.6235\n",
      "Epoch 169: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 169: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5974 - auc_5: 0.6354 - f05: 0.5135 - loss: 0.5565 - precision_5: 0.6000 - precision_focused: 0.5148 - recall_5: 0.6234 - val_accuracy: 0.5008 - val_auc_5: 0.5179 - val_f05: 0.4602 - val_loss: 0.5894 - val_precision_5: 0.5331 - val_precision_focused: 0.4649 - val_recall_5: 0.3481 - learning_rate: 2.7930e-05\n",
      "Epoch 170/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5997 - auc_5: 0.6351 - f05: 0.5278 - loss: 0.5539 - precision_5: 0.6069 - precision_focused: 0.5299 - recall_5: 0.6471\n",
      "Epoch 170: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 170: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.5997 - auc_5: 0.6351 - f05: 0.5276 - loss: 0.5540 - precision_5: 0.6068 - precision_focused: 0.5297 - recall_5: 0.6468 - val_accuracy: 0.5014 - val_auc_5: 0.5185 - val_f05: 0.4411 - val_loss: 0.5911 - val_precision_5: 0.5391 - val_precision_focused: 0.4533 - val_recall_5: 0.3062 - learning_rate: 2.7930e-05\n",
      "Epoch 171/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6127 - auc_5: 0.6523 - f05: 0.5124 - loss: 0.5505 - precision_5: 0.6186 - precision_focused: 0.5126 - recall_5: 0.6245\n",
      "Epoch 171: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 171: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6126 - auc_5: 0.6523 - f05: 0.5124 - loss: 0.5505 - precision_5: 0.6185 - precision_focused: 0.5126 - recall_5: 0.6244 - val_accuracy: 0.5027 - val_auc_5: 0.5180 - val_f05: 0.4667 - val_loss: 0.5895 - val_precision_5: 0.5341 - val_precision_focused: 0.4693 - val_recall_5: 0.3679 - learning_rate: 2.7930e-05\n",
      "Epoch 172/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6042 - auc_5: 0.6477 - f05: 0.5142 - loss: 0.5524 - precision_5: 0.6044 - precision_focused: 0.5156 - recall_5: 0.6311\n",
      "Epoch 172: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 172: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6042 - auc_5: 0.6477 - f05: 0.5142 - loss: 0.5524 - precision_5: 0.6044 - precision_focused: 0.5156 - recall_5: 0.6311 - val_accuracy: 0.5014 - val_auc_5: 0.5185 - val_f05: 0.4642 - val_loss: 0.5894 - val_precision_5: 0.5333 - val_precision_focused: 0.4680 - val_recall_5: 0.3562 - learning_rate: 2.7930e-05\n",
      "Epoch 173/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5995 - auc_5: 0.6402 - f05: 0.5184 - loss: 0.5537 - precision_5: 0.6038 - precision_focused: 0.5196 - recall_5: 0.6261\n",
      "Epoch 173: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 173: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5994 - auc_5: 0.6402 - f05: 0.5184 - loss: 0.5537 - precision_5: 0.6037 - precision_focused: 0.5196 - recall_5: 0.6260 - val_accuracy: 0.5050 - val_auc_5: 0.5192 - val_f05: 0.4472 - val_loss: 0.5903 - val_precision_5: 0.5431 - val_precision_focused: 0.4567 - val_recall_5: 0.3228 - learning_rate: 2.7930e-05\n",
      "Epoch 174/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6064 - auc_5: 0.6470 - f05: 0.5113 - loss: 0.5518 - precision_5: 0.6086 - precision_focused: 0.5120 - recall_5: 0.6227\n",
      "Epoch 174: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 174: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6063 - auc_5: 0.6470 - f05: 0.5113 - loss: 0.5518 - precision_5: 0.6086 - precision_focused: 0.5120 - recall_5: 0.6227 - val_accuracy: 0.5043 - val_auc_5: 0.5206 - val_f05: 0.4578 - val_loss: 0.5899 - val_precision_5: 0.5392 - val_precision_focused: 0.4636 - val_recall_5: 0.3438 - learning_rate: 2.7930e-05\n",
      "Epoch 175/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5884 - auc_5: 0.6274 - f05: 0.5103 - loss: 0.5595 - precision_5: 0.5887 - precision_focused: 0.5114 - recall_5: 0.6102\n",
      "Epoch 175: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 175: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5884 - auc_5: 0.6274 - f05: 0.5103 - loss: 0.5595 - precision_5: 0.5888 - precision_focused: 0.5114 - recall_5: 0.6102 - val_accuracy: 0.5040 - val_auc_5: 0.5184 - val_f05: 0.4771 - val_loss: 0.5885 - val_precision_5: 0.5334 - val_precision_focused: 0.4774 - val_recall_5: 0.3944 - learning_rate: 2.7930e-05\n",
      "Epoch 176/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5986 - auc_5: 0.6438 - f05: 0.5098 - loss: 0.5542 - precision_5: 0.5959 - precision_focused: 0.5111 - recall_5: 0.6221\n",
      "Epoch 176: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 176: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5986 - auc_5: 0.6438 - f05: 0.5098 - loss: 0.5542 - precision_5: 0.5959 - precision_focused: 0.5111 - recall_5: 0.6221 - val_accuracy: 0.5014 - val_auc_5: 0.5180 - val_f05: 0.4748 - val_loss: 0.5893 - val_precision_5: 0.5308 - val_precision_focused: 0.4758 - val_recall_5: 0.3827 - learning_rate: 2.7930e-05\n",
      "Epoch 177/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5925 - auc_5: 0.6333 - f05: 0.5123 - loss: 0.5579 - precision_5: 0.5905 - precision_focused: 0.5143 - recall_5: 0.6282\n",
      "Epoch 177: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 177: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5925 - auc_5: 0.6333 - f05: 0.5123 - loss: 0.5579 - precision_5: 0.5906 - precision_focused: 0.5143 - recall_5: 0.6282 - val_accuracy: 0.5040 - val_auc_5: 0.5188 - val_f05: 0.4588 - val_loss: 0.5899 - val_precision_5: 0.5388 - val_precision_focused: 0.4648 - val_recall_5: 0.3426 - learning_rate: 2.7930e-05\n",
      "Epoch 178/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5970 - auc_5: 0.6383 - f05: 0.5103 - loss: 0.5548 - precision_5: 0.5993 - precision_focused: 0.5110 - recall_5: 0.6118\n",
      "Epoch 178: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 178: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5970 - auc_5: 0.6383 - f05: 0.5103 - loss: 0.5548 - precision_5: 0.5993 - precision_focused: 0.5110 - recall_5: 0.6118 - val_accuracy: 0.5027 - val_auc_5: 0.5186 - val_f05: 0.4541 - val_loss: 0.5904 - val_precision_5: 0.5383 - val_precision_focused: 0.4620 - val_recall_5: 0.3296 - learning_rate: 2.7930e-05\n",
      "Epoch 179/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5989 - auc_5: 0.6399 - f05: 0.5127 - loss: 0.5551 - precision_5: 0.5992 - precision_focused: 0.5137 - recall_5: 0.6181\n",
      "Epoch 179: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 179: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5989 - auc_5: 0.6399 - f05: 0.5127 - loss: 0.5551 - precision_5: 0.5993 - precision_focused: 0.5137 - recall_5: 0.6180 - val_accuracy: 0.5047 - val_auc_5: 0.5183 - val_f05: 0.4597 - val_loss: 0.5906 - val_precision_5: 0.5393 - val_precision_focused: 0.4650 - val_recall_5: 0.3475 - learning_rate: 2.7930e-05\n",
      "Epoch 180/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6076 - auc_5: 0.6486 - f05: 0.5126 - loss: 0.5516 - precision_5: 0.6121 - precision_focused: 0.5133 - recall_5: 0.6248\n",
      "Epoch 180: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 180: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6075 - auc_5: 0.6485 - f05: 0.5126 - loss: 0.5516 - precision_5: 0.6120 - precision_focused: 0.5132 - recall_5: 0.6247 - val_accuracy: 0.5031 - val_auc_5: 0.5183 - val_f05: 0.4541 - val_loss: 0.5904 - val_precision_5: 0.5383 - val_precision_focused: 0.4613 - val_recall_5: 0.3340 - learning_rate: 2.7930e-05\n",
      "Epoch 181/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6019 - auc_5: 0.6453 - f05: 0.5135 - loss: 0.5523 - precision_5: 0.6044 - precision_focused: 0.5146 - recall_5: 0.6265\n",
      "Epoch 181: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 181: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6019 - auc_5: 0.6453 - f05: 0.5135 - loss: 0.5524 - precision_5: 0.6044 - precision_focused: 0.5146 - recall_5: 0.6264 - val_accuracy: 0.5024 - val_auc_5: 0.5183 - val_f05: 0.4498 - val_loss: 0.5913 - val_precision_5: 0.5381 - val_precision_focused: 0.4582 - val_recall_5: 0.3265 - learning_rate: 2.7930e-05\n",
      "Epoch 182/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5996 - auc_5: 0.6338 - f05: 0.5126 - loss: 0.5568 - precision_5: 0.6055 - precision_focused: 0.5128 - recall_5: 0.6092\n",
      "Epoch 182: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 182: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5996 - auc_5: 0.6338 - f05: 0.5126 - loss: 0.5568 - precision_5: 0.6055 - precision_focused: 0.5128 - recall_5: 0.6093 - val_accuracy: 0.5031 - val_auc_5: 0.5195 - val_f05: 0.4591 - val_loss: 0.5905 - val_precision_5: 0.5370 - val_precision_focused: 0.4646 - val_recall_5: 0.3451 - learning_rate: 2.7930e-05\n",
      "Epoch 183/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5981 - auc_5: 0.6407 - f05: 0.5136 - loss: 0.5535 - precision_5: 0.6027 - precision_focused: 0.5144 - recall_5: 0.6186\n",
      "Epoch 183: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 183: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5981 - auc_5: 0.6407 - f05: 0.5136 - loss: 0.5535 - precision_5: 0.6027 - precision_focused: 0.5144 - recall_5: 0.6186 - val_accuracy: 0.5021 - val_auc_5: 0.5187 - val_f05: 0.4544 - val_loss: 0.5915 - val_precision_5: 0.5371 - val_precision_focused: 0.4621 - val_recall_5: 0.3302 - learning_rate: 2.7930e-05\n",
      "Epoch 184/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5850 - auc_5: 0.6242 - f05: 0.5135 - loss: 0.5595 - precision_5: 0.5930 - precision_focused: 0.5137 - recall_5: 0.5952\n",
      "Epoch 184: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 184: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.5851 - auc_5: 0.6243 - f05: 0.5135 - loss: 0.5595 - precision_5: 0.5931 - precision_focused: 0.5136 - recall_5: 0.5953 - val_accuracy: 0.4976 - val_auc_5: 0.5190 - val_f05: 0.4640 - val_loss: 0.5899 - val_precision_5: 0.5282 - val_precision_focused: 0.4686 - val_recall_5: 0.3463 - learning_rate: 2.7930e-05\n",
      "Epoch 185/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6106 - auc_5: 0.6512 - f05: 0.5293 - loss: 0.5471 - precision_5: 0.6196 - precision_focused: 0.5310 - recall_5: 0.6519\n",
      "Epoch 185: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 185: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6105 - auc_5: 0.6511 - f05: 0.5292 - loss: 0.5471 - precision_5: 0.6195 - precision_focused: 0.5308 - recall_5: 0.6517 - val_accuracy: 0.5050 - val_auc_5: 0.5199 - val_f05: 0.4462 - val_loss: 0.5915 - val_precision_5: 0.5442 - val_precision_focused: 0.4572 - val_recall_5: 0.3154 - learning_rate: 2.7930e-05\n",
      "Epoch 186/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6050 - auc_5: 0.6434 - f05: 0.5062 - loss: 0.5542 - precision_5: 0.6081 - precision_focused: 0.5062 - recall_5: 0.6051\n",
      "Epoch 186: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 186: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6050 - auc_5: 0.6434 - f05: 0.5062 - loss: 0.5542 - precision_5: 0.6081 - precision_focused: 0.5062 - recall_5: 0.6052 - val_accuracy: 0.5014 - val_auc_5: 0.5188 - val_f05: 0.4493 - val_loss: 0.5917 - val_precision_5: 0.5373 - val_precision_focused: 0.4586 - val_recall_5: 0.3204 - learning_rate: 2.7930e-05\n",
      "Epoch 187/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5910 - auc_5: 0.6370 - f05: 0.5096 - loss: 0.5550 - precision_5: 0.5993 - precision_focused: 0.5096 - recall_5: 0.5957\n",
      "Epoch 187: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 187: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5910 - auc_5: 0.6370 - f05: 0.5096 - loss: 0.5550 - precision_5: 0.5993 - precision_focused: 0.5096 - recall_5: 0.5957 - val_accuracy: 0.4995 - val_auc_5: 0.5183 - val_f05: 0.4650 - val_loss: 0.5898 - val_precision_5: 0.5308 - val_precision_focused: 0.4692 - val_recall_5: 0.3512 - learning_rate: 2.7930e-05\n",
      "Epoch 188/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6024 - auc_5: 0.6431 - f05: 0.5166 - loss: 0.5524 - precision_5: 0.6042 - precision_focused: 0.5182 - recall_5: 0.6359\n",
      "Epoch 188: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 188: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6024 - auc_5: 0.6431 - f05: 0.5166 - loss: 0.5524 - precision_5: 0.6042 - precision_focused: 0.5182 - recall_5: 0.6358 - val_accuracy: 0.5008 - val_auc_5: 0.5183 - val_f05: 0.4657 - val_loss: 0.5908 - val_precision_5: 0.5322 - val_precision_focused: 0.4694 - val_recall_5: 0.3568 - learning_rate: 2.7930e-05\n",
      "Epoch 189/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5971 - auc_5: 0.6426 - f05: 0.5211 - loss: 0.5508 - precision_5: 0.6040 - precision_focused: 0.5225 - recall_5: 0.6313\n",
      "Epoch 189: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 189: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5971 - auc_5: 0.6426 - f05: 0.5211 - loss: 0.5508 - precision_5: 0.6040 - precision_focused: 0.5225 - recall_5: 0.6312 - val_accuracy: 0.5011 - val_auc_5: 0.5204 - val_f05: 0.4511 - val_loss: 0.5912 - val_precision_5: 0.5363 - val_precision_focused: 0.4598 - val_recall_5: 0.3235 - learning_rate: 2.7930e-05\n",
      "Epoch 190/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6081 - auc_5: 0.6437 - f05: 0.5155 - loss: 0.5537 - precision_5: 0.6151 - precision_focused: 0.5159 - recall_5: 0.6227\n",
      "Epoch 190: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 190: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6081 - auc_5: 0.6436 - f05: 0.5155 - loss: 0.5537 - precision_5: 0.6150 - precision_focused: 0.5159 - recall_5: 0.6227 - val_accuracy: 0.5040 - val_auc_5: 0.5185 - val_f05: 0.4772 - val_loss: 0.5902 - val_precision_5: 0.5331 - val_precision_focused: 0.4773 - val_recall_5: 0.3975 - learning_rate: 2.7930e-05\n",
      "Epoch 191/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6009 - auc_5: 0.6411 - f05: 0.5098 - loss: 0.5551 - precision_5: 0.5988 - precision_focused: 0.5111 - recall_5: 0.6240\n",
      "Epoch 191: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 191: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6009 - auc_5: 0.6411 - f05: 0.5098 - loss: 0.5551 - precision_5: 0.5988 - precision_focused: 0.5111 - recall_5: 0.6239 - val_accuracy: 0.5056 - val_auc_5: 0.5192 - val_f05: 0.4654 - val_loss: 0.5904 - val_precision_5: 0.5395 - val_precision_focused: 0.4694 - val_recall_5: 0.3586 - learning_rate: 2.7930e-05\n",
      "Epoch 192/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6148 - auc_5: 0.6524 - f05: 0.5152 - loss: 0.5505 - precision_5: 0.6137 - precision_focused: 0.5168 - recall_5: 0.6454\n",
      "Epoch 192: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 192: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6147 - auc_5: 0.6524 - f05: 0.5151 - loss: 0.5505 - precision_5: 0.6137 - precision_focused: 0.5168 - recall_5: 0.6454 - val_accuracy: 0.5031 - val_auc_5: 0.5196 - val_f05: 0.4527 - val_loss: 0.5919 - val_precision_5: 0.5385 - val_precision_focused: 0.4601 - val_recall_5: 0.3327 - learning_rate: 2.7930e-05\n",
      "Epoch 193/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6026 - auc_5: 0.6452 - f05: 0.5034 - loss: 0.5534 - precision_5: 0.6044 - precision_focused: 0.5031 - recall_5: 0.5984\n",
      "Epoch 193: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 193: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6026 - auc_5: 0.6451 - f05: 0.5034 - loss: 0.5534 - precision_5: 0.6045 - precision_focused: 0.5032 - recall_5: 0.5984 - val_accuracy: 0.5037 - val_auc_5: 0.5188 - val_f05: 0.4575 - val_loss: 0.5909 - val_precision_5: 0.5390 - val_precision_focused: 0.4644 - val_recall_5: 0.3370 - learning_rate: 2.7930e-05\n",
      "Epoch 194/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6045 - auc_5: 0.6514 - f05: 0.5085 - loss: 0.5502 - precision_5: 0.6059 - precision_focused: 0.5094 - recall_5: 0.6230\n",
      "Epoch 194: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 194: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6045 - auc_5: 0.6514 - f05: 0.5085 - loss: 0.5502 - precision_5: 0.6059 - precision_focused: 0.5094 - recall_5: 0.6230 - val_accuracy: 0.4992 - val_auc_5: 0.5176 - val_f05: 0.4640 - val_loss: 0.5919 - val_precision_5: 0.5306 - val_precision_focused: 0.4685 - val_recall_5: 0.3481 - learning_rate: 2.7930e-05\n",
      "Epoch 195/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6001 - auc_5: 0.6421 - f05: 0.5122 - loss: 0.5535 - precision_5: 0.6027 - precision_focused: 0.5129 - recall_5: 0.6171\n",
      "Epoch 195: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 195: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6001 - auc_5: 0.6421 - f05: 0.5122 - loss: 0.5534 - precision_5: 0.6027 - precision_focused: 0.5129 - recall_5: 0.6170 - val_accuracy: 0.5018 - val_auc_5: 0.5177 - val_f05: 0.4687 - val_loss: 0.5916 - val_precision_5: 0.5328 - val_precision_focused: 0.4714 - val_recall_5: 0.3660 - learning_rate: 2.7930e-05\n",
      "Epoch 196/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5992 - auc_5: 0.6422 - f05: 0.5135 - loss: 0.5537 - precision_5: 0.6002 - precision_focused: 0.5149 - recall_5: 0.6282\n",
      "Epoch 196: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 196: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5992 - auc_5: 0.6422 - f05: 0.5134 - loss: 0.5537 - precision_5: 0.6002 - precision_focused: 0.5149 - recall_5: 0.6281 - val_accuracy: 0.5014 - val_auc_5: 0.5188 - val_f05: 0.4584 - val_loss: 0.5912 - val_precision_5: 0.5350 - val_precision_focused: 0.4645 - val_recall_5: 0.3395 - learning_rate: 2.7930e-05\n",
      "Epoch 197/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6006 - auc_5: 0.6478 - f05: 0.5102 - loss: 0.5508 - precision_5: 0.6078 - precision_focused: 0.5101 - recall_5: 0.6049\n",
      "Epoch 197: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 197: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6006 - auc_5: 0.6478 - f05: 0.5102 - loss: 0.5508 - precision_5: 0.6078 - precision_focused: 0.5101 - recall_5: 0.6049 - val_accuracy: 0.5011 - val_auc_5: 0.5180 - val_f05: 0.4722 - val_loss: 0.5902 - val_precision_5: 0.5314 - val_precision_focused: 0.4743 - val_recall_5: 0.3710 - learning_rate: 2.7930e-05\n",
      "Epoch 198/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6178 - auc_5: 0.6588 - f05: 0.5216 - loss: 0.5460 - precision_5: 0.6231 - precision_focused: 0.5232 - recall_5: 0.6524\n",
      "Epoch 198: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 198: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6177 - auc_5: 0.6587 - f05: 0.5215 - loss: 0.5460 - precision_5: 0.6230 - precision_focused: 0.5231 - recall_5: 0.6522 - val_accuracy: 0.5024 - val_auc_5: 0.5181 - val_f05: 0.4588 - val_loss: 0.5919 - val_precision_5: 0.5364 - val_precision_focused: 0.4648 - val_recall_5: 0.3414 - learning_rate: 2.7930e-05\n",
      "Epoch 199/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6117 - auc_5: 0.6532 - f05: 0.5236 - loss: 0.5474 - precision_5: 0.6180 - precision_focused: 0.5250 - recall_5: 0.6455\n",
      "Epoch 199: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 199: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6117 - auc_5: 0.6532 - f05: 0.5236 - loss: 0.5474 - precision_5: 0.6180 - precision_focused: 0.5250 - recall_5: 0.6455 - val_accuracy: 0.4998 - val_auc_5: 0.5192 - val_f05: 0.4455 - val_loss: 0.5928 - val_precision_5: 0.5358 - val_precision_focused: 0.4566 - val_recall_5: 0.3099 - learning_rate: 2.7930e-05\n",
      "Epoch 200/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5978 - auc_5: 0.6405 - f05: 0.4988 - loss: 0.5567 - precision_5: 0.5994 - precision_focused: 0.4983 - recall_5: 0.5823\n",
      "Epoch 200: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 200: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5979 - auc_5: 0.6405 - f05: 0.4989 - loss: 0.5567 - precision_5: 0.5994 - precision_focused: 0.4983 - recall_5: 0.5824 - val_accuracy: 0.5008 - val_auc_5: 0.5168 - val_f05: 0.4854 - val_loss: 0.5896 - val_precision_5: 0.5281 - val_precision_focused: 0.4848 - val_recall_5: 0.4062 - learning_rate: 2.7930e-05\n",
      "Epoch 201/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5962 - auc_5: 0.6393 - f05: 0.5168 - loss: 0.5532 - precision_5: 0.6011 - precision_focused: 0.5183 - recall_5: 0.6283\n",
      "Epoch 201: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 201: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5963 - auc_5: 0.6393 - f05: 0.5168 - loss: 0.5532 - precision_5: 0.6012 - precision_focused: 0.5183 - recall_5: 0.6282 - val_accuracy: 0.5018 - val_auc_5: 0.5182 - val_f05: 0.4479 - val_loss: 0.5921 - val_precision_5: 0.5385 - val_precision_focused: 0.4585 - val_recall_5: 0.3148 - learning_rate: 2.7930e-05\n",
      "Epoch 202/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5948 - auc_5: 0.6361 - f05: 0.5033 - loss: 0.5564 - precision_5: 0.5980 - precision_focused: 0.5032 - recall_5: 0.5950\n",
      "Epoch 202: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 202: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5948 - auc_5: 0.6361 - f05: 0.5033 - loss: 0.5564 - precision_5: 0.5980 - precision_focused: 0.5032 - recall_5: 0.5950 - val_accuracy: 0.5014 - val_auc_5: 0.5183 - val_f05: 0.4694 - val_loss: 0.5895 - val_precision_5: 0.5326 - val_precision_focused: 0.4724 - val_recall_5: 0.3630 - learning_rate: 2.7930e-05\n",
      "Epoch 203/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6003 - auc_5: 0.6468 - f05: 0.5195 - loss: 0.5503 - precision_5: 0.6052 - precision_focused: 0.5210 - recall_5: 0.6332\n",
      "Epoch 203: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 203: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6003 - auc_5: 0.6468 - f05: 0.5195 - loss: 0.5503 - precision_5: 0.6052 - precision_focused: 0.5210 - recall_5: 0.6331 - val_accuracy: 0.5005 - val_auc_5: 0.5175 - val_f05: 0.4572 - val_loss: 0.5911 - val_precision_5: 0.5341 - val_precision_focused: 0.4641 - val_recall_5: 0.3340 - learning_rate: 2.7930e-05\n",
      "Epoch 204/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6091 - auc_5: 0.6493 - f05: 0.5184 - loss: 0.5495 - precision_5: 0.6203 - precision_focused: 0.5183 - recall_5: 0.6191\n",
      "Epoch 204: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 204: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6091 - auc_5: 0.6493 - f05: 0.5183 - loss: 0.5495 - precision_5: 0.6203 - precision_focused: 0.5183 - recall_5: 0.6190 - val_accuracy: 0.5008 - val_auc_5: 0.5170 - val_f05: 0.4600 - val_loss: 0.5909 - val_precision_5: 0.5340 - val_precision_focused: 0.4660 - val_recall_5: 0.3395 - learning_rate: 2.7930e-05\n",
      "Epoch 205/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6030 - auc_5: 0.6456 - f05: 0.5174 - loss: 0.5500 - precision_5: 0.6116 - precision_focused: 0.5182 - recall_5: 0.6235\n",
      "Epoch 205: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 205: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6030 - auc_5: 0.6456 - f05: 0.5174 - loss: 0.5501 - precision_5: 0.6116 - precision_focused: 0.5182 - recall_5: 0.6235 - val_accuracy: 0.4992 - val_auc_5: 0.5174 - val_f05: 0.4504 - val_loss: 0.5916 - val_precision_5: 0.5337 - val_precision_focused: 0.4599 - val_recall_5: 0.3173 - learning_rate: 2.7930e-05\n",
      "Epoch 206/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6102 - auc_5: 0.6548 - f05: 0.5091 - loss: 0.5483 - precision_5: 0.6172 - precision_focused: 0.5089 - recall_5: 0.6113\n",
      "Epoch 206: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 206: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6102 - auc_5: 0.6547 - f05: 0.5091 - loss: 0.5484 - precision_5: 0.6172 - precision_focused: 0.5089 - recall_5: 0.6112 - val_accuracy: 0.5043 - val_auc_5: 0.5172 - val_f05: 0.4799 - val_loss: 0.5901 - val_precision_5: 0.5336 - val_precision_focused: 0.4801 - val_recall_5: 0.3969 - learning_rate: 2.7930e-05\n",
      "Epoch 207/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6058 - auc_5: 0.6467 - f05: 0.5181 - loss: 0.5503 - precision_5: 0.6106 - precision_focused: 0.5189 - recall_5: 0.6278\n",
      "Epoch 207: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 207: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 2.2343681484926493e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6058 - auc_5: 0.6467 - f05: 0.5181 - loss: 0.5503 - precision_5: 0.6106 - precision_focused: 0.5189 - recall_5: 0.6277 - val_accuracy: 0.4989 - val_auc_5: 0.5174 - val_f05: 0.4550 - val_loss: 0.5921 - val_precision_5: 0.5322 - val_precision_focused: 0.4628 - val_recall_5: 0.3265 - learning_rate: 2.7930e-05\n",
      "Epoch 208/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6021 - auc_5: 0.6435 - f05: 0.5152 - loss: 0.5510 - precision_5: 0.6109 - precision_focused: 0.5156 - recall_5: 0.6183\n",
      "Epoch 208: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 208: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6021 - auc_5: 0.6436 - f05: 0.5152 - loss: 0.5510 - precision_5: 0.6109 - precision_focused: 0.5156 - recall_5: 0.6183 - val_accuracy: 0.5014 - val_auc_5: 0.5179 - val_f05: 0.4555 - val_loss: 0.5925 - val_precision_5: 0.5362 - val_precision_focused: 0.4633 - val_recall_5: 0.3290 - learning_rate: 2.2344e-05\n",
      "Epoch 209/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6128 - auc_5: 0.6538 - f05: 0.5134 - loss: 0.5486 - precision_5: 0.6174 - precision_focused: 0.5138 - recall_5: 0.6267\n",
      "Epoch 209: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 209: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6128 - auc_5: 0.6537 - f05: 0.5133 - loss: 0.5487 - precision_5: 0.6173 - precision_focused: 0.5137 - recall_5: 0.6267 - val_accuracy: 0.5021 - val_auc_5: 0.5182 - val_f05: 0.4645 - val_loss: 0.5923 - val_precision_5: 0.5346 - val_precision_focused: 0.4688 - val_recall_5: 0.3531 - learning_rate: 2.2344e-05\n",
      "Epoch 210/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6058 - auc_5: 0.6432 - f05: 0.5157 - loss: 0.5533 - precision_5: 0.6135 - precision_focused: 0.5161 - recall_5: 0.6206\n",
      "Epoch 210: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 210: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6057 - auc_5: 0.6432 - f05: 0.5157 - loss: 0.5533 - precision_5: 0.6134 - precision_focused: 0.5160 - recall_5: 0.6205 - val_accuracy: 0.5011 - val_auc_5: 0.5174 - val_f05: 0.4611 - val_loss: 0.5923 - val_precision_5: 0.5340 - val_precision_focused: 0.4665 - val_recall_5: 0.3438 - learning_rate: 2.2344e-05\n",
      "Epoch 211/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6078 - auc_5: 0.6492 - f05: 0.5189 - loss: 0.5506 - precision_5: 0.6133 - precision_focused: 0.5196 - recall_5: 0.6290\n",
      "Epoch 211: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 211: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6078 - auc_5: 0.6492 - f05: 0.5188 - loss: 0.5506 - precision_5: 0.6133 - precision_focused: 0.5196 - recall_5: 0.6289 - val_accuracy: 0.5011 - val_auc_5: 0.5179 - val_f05: 0.4627 - val_loss: 0.5920 - val_precision_5: 0.5336 - val_precision_focused: 0.4676 - val_recall_5: 0.3475 - learning_rate: 2.2344e-05\n",
      "Epoch 212/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6010 - auc_5: 0.6396 - f05: 0.5154 - loss: 0.5546 - precision_5: 0.6029 - precision_focused: 0.5166 - recall_5: 0.6270\n",
      "Epoch 212: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 212: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6010 - auc_5: 0.6396 - f05: 0.5154 - loss: 0.5546 - precision_5: 0.6029 - precision_focused: 0.5165 - recall_5: 0.6270 - val_accuracy: 0.5021 - val_auc_5: 0.5185 - val_f05: 0.4687 - val_loss: 0.5909 - val_precision_5: 0.5333 - val_precision_focused: 0.4715 - val_recall_5: 0.3654 - learning_rate: 2.2344e-05\n",
      "Epoch 213/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6058 - auc_5: 0.6508 - f05: 0.5163 - loss: 0.5498 - precision_5: 0.6119 - precision_focused: 0.5169 - recall_5: 0.6257\n",
      "Epoch 213: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 213: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6058 - auc_5: 0.6508 - f05: 0.5162 - loss: 0.5498 - precision_5: 0.6118 - precision_focused: 0.5169 - recall_5: 0.6257 - val_accuracy: 0.5008 - val_auc_5: 0.5189 - val_f05: 0.4596 - val_loss: 0.5924 - val_precision_5: 0.5335 - val_precision_focused: 0.4649 - val_recall_5: 0.3438 - learning_rate: 2.2344e-05\n",
      "Epoch 214/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6040 - auc_5: 0.6482 - f05: 0.5148 - loss: 0.5503 - precision_5: 0.6122 - precision_focused: 0.5149 - recall_5: 0.6126\n",
      "Epoch 214: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 214: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6040 - auc_5: 0.6482 - f05: 0.5148 - loss: 0.5503 - precision_5: 0.6122 - precision_focused: 0.5149 - recall_5: 0.6126 - val_accuracy: 0.5063 - val_auc_5: 0.5181 - val_f05: 0.4697 - val_loss: 0.5912 - val_precision_5: 0.5389 - val_precision_focused: 0.4722 - val_recall_5: 0.3716 - learning_rate: 2.2344e-05\n",
      "Epoch 215/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5981 - auc_5: 0.6437 - f05: 0.5148 - loss: 0.5524 - precision_5: 0.6002 - precision_focused: 0.5161 - recall_5: 0.6248\n",
      "Epoch 215: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 215: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5982 - auc_5: 0.6438 - f05: 0.5148 - loss: 0.5523 - precision_5: 0.6003 - precision_focused: 0.5160 - recall_5: 0.6248 - val_accuracy: 0.5024 - val_auc_5: 0.5181 - val_f05: 0.4642 - val_loss: 0.5925 - val_precision_5: 0.5349 - val_precision_focused: 0.4683 - val_recall_5: 0.3543 - learning_rate: 2.2344e-05\n",
      "Epoch 216/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6108 - auc_5: 0.6521 - f05: 0.5094 - loss: 0.5504 - precision_5: 0.6135 - precision_focused: 0.5096 - recall_5: 0.6184\n",
      "Epoch 216: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 216: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6108 - auc_5: 0.6521 - f05: 0.5094 - loss: 0.5504 - precision_5: 0.6135 - precision_focused: 0.5096 - recall_5: 0.6184 - val_accuracy: 0.5018 - val_auc_5: 0.5169 - val_f05: 0.4674 - val_loss: 0.5920 - val_precision_5: 0.5335 - val_precision_focused: 0.4710 - val_recall_5: 0.3586 - learning_rate: 2.2344e-05\n",
      "Epoch 217/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5990 - auc_5: 0.6371 - f05: 0.5222 - loss: 0.5540 - precision_5: 0.6051 - precision_focused: 0.5238 - recall_5: 0.6380\n",
      "Epoch 217: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 217: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5990 - auc_5: 0.6371 - f05: 0.5221 - loss: 0.5540 - precision_5: 0.6051 - precision_focused: 0.5238 - recall_5: 0.6380 - val_accuracy: 0.5008 - val_auc_5: 0.5175 - val_f05: 0.4546 - val_loss: 0.5934 - val_precision_5: 0.5349 - val_precision_focused: 0.4620 - val_recall_5: 0.3309 - learning_rate: 2.2344e-05\n",
      "Epoch 218/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6037 - auc_5: 0.6427 - f05: 0.5129 - loss: 0.5528 - precision_5: 0.6093 - precision_focused: 0.5133 - recall_5: 0.6176\n",
      "Epoch 218: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 218: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6037 - auc_5: 0.6428 - f05: 0.5129 - loss: 0.5528 - precision_5: 0.6094 - precision_focused: 0.5133 - recall_5: 0.6176 - val_accuracy: 0.5011 - val_auc_5: 0.5183 - val_f05: 0.4577 - val_loss: 0.5927 - val_precision_5: 0.5349 - val_precision_focused: 0.4643 - val_recall_5: 0.3358 - learning_rate: 2.2344e-05\n",
      "Epoch 219/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6161 - auc_5: 0.6626 - f05: 0.5181 - loss: 0.5444 - precision_5: 0.6230 - precision_focused: 0.5187 - recall_5: 0.6348\n",
      "Epoch 219: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 219: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6161 - auc_5: 0.6625 - f05: 0.5181 - loss: 0.5444 - precision_5: 0.6230 - precision_focused: 0.5187 - recall_5: 0.6347 - val_accuracy: 0.5031 - val_auc_5: 0.5177 - val_f05: 0.4586 - val_loss: 0.5935 - val_precision_5: 0.5373 - val_precision_focused: 0.4645 - val_recall_5: 0.3426 - learning_rate: 2.2344e-05\n",
      "Epoch 220/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6064 - auc_5: 0.6484 - f05: 0.5093 - loss: 0.5510 - precision_5: 0.6099 - precision_focused: 0.5096 - recall_5: 0.6143\n",
      "Epoch 220: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 220: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6064 - auc_5: 0.6484 - f05: 0.5093 - loss: 0.5510 - precision_5: 0.6099 - precision_focused: 0.5096 - recall_5: 0.6143 - val_accuracy: 0.4992 - val_auc_5: 0.5182 - val_f05: 0.4531 - val_loss: 0.5934 - val_precision_5: 0.5329 - val_precision_focused: 0.4613 - val_recall_5: 0.3247 - learning_rate: 2.2344e-05\n",
      "Epoch 221/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5990 - auc_5: 0.6436 - f05: 0.5209 - loss: 0.5510 - precision_5: 0.6067 - precision_focused: 0.5219 - recall_5: 0.6271\n",
      "Epoch 221: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 221: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.5991 - auc_5: 0.6436 - f05: 0.5208 - loss: 0.5510 - precision_5: 0.6067 - precision_focused: 0.5218 - recall_5: 0.6270 - val_accuracy: 0.4995 - val_auc_5: 0.5182 - val_f05: 0.4297 - val_loss: 0.5953 - val_precision_5: 0.5396 - val_precision_focused: 0.4487 - val_recall_5: 0.2778 - learning_rate: 2.2344e-05\n",
      "Epoch 222/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6033 - auc_5: 0.6424 - f05: 0.5098 - loss: 0.5536 - precision_5: 0.6095 - precision_focused: 0.5098 - recall_5: 0.6104\n",
      "Epoch 222: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 222: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6033 - auc_5: 0.6424 - f05: 0.5098 - loss: 0.5536 - precision_5: 0.6095 - precision_focused: 0.5098 - recall_5: 0.6105 - val_accuracy: 0.4982 - val_auc_5: 0.5185 - val_f05: 0.4400 - val_loss: 0.5940 - val_precision_5: 0.5348 - val_precision_focused: 0.4543 - val_recall_5: 0.2944 - learning_rate: 2.2344e-05\n",
      "Epoch 223/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6258 - auc_5: 0.6681 - f05: 0.5099 - loss: 0.5439 - precision_5: 0.6336 - precision_focused: 0.5095 - recall_5: 0.6232\n",
      "Epoch 223: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 223: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6257 - auc_5: 0.6681 - f05: 0.5099 - loss: 0.5440 - precision_5: 0.6336 - precision_focused: 0.5095 - recall_5: 0.6232 - val_accuracy: 0.4992 - val_auc_5: 0.5175 - val_f05: 0.4526 - val_loss: 0.5941 - val_precision_5: 0.5332 - val_precision_focused: 0.4613 - val_recall_5: 0.3222 - learning_rate: 2.2344e-05\n",
      "Epoch 224/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6068 - auc_5: 0.6551 - f05: 0.5182 - loss: 0.5468 - precision_5: 0.6158 - precision_focused: 0.5187 - recall_5: 0.6256\n",
      "Epoch 224: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 224: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6068 - auc_5: 0.6551 - f05: 0.5182 - loss: 0.5468 - precision_5: 0.6158 - precision_focused: 0.5187 - recall_5: 0.6256 - val_accuracy: 0.5031 - val_auc_5: 0.5189 - val_f05: 0.4481 - val_loss: 0.5943 - val_precision_5: 0.5405 - val_precision_focused: 0.4584 - val_recall_5: 0.3173 - learning_rate: 2.2344e-05\n",
      "Epoch 225/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6145 - auc_5: 0.6543 - f05: 0.5128 - loss: 0.5487 - precision_5: 0.6194 - precision_focused: 0.5130 - recall_5: 0.6230\n",
      "Epoch 225: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 225: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6145 - auc_5: 0.6543 - f05: 0.5128 - loss: 0.5487 - precision_5: 0.6195 - precision_focused: 0.5130 - recall_5: 0.6230 - val_accuracy: 0.5027 - val_auc_5: 0.5189 - val_f05: 0.4584 - val_loss: 0.5936 - val_precision_5: 0.5376 - val_precision_focused: 0.4654 - val_recall_5: 0.3352 - learning_rate: 2.2344e-05\n",
      "Epoch 226/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6090 - auc_5: 0.6554 - f05: 0.5116 - loss: 0.5494 - precision_5: 0.6095 - precision_focused: 0.5127 - recall_5: 0.6327\n",
      "Epoch 226: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 226: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6089 - auc_5: 0.6554 - f05: 0.5116 - loss: 0.5494 - precision_5: 0.6095 - precision_focused: 0.5127 - recall_5: 0.6326 - val_accuracy: 0.4998 - val_auc_5: 0.5185 - val_f05: 0.4510 - val_loss: 0.5937 - val_precision_5: 0.5348 - val_precision_focused: 0.4606 - val_recall_5: 0.3179 - learning_rate: 2.2344e-05\n",
      "Epoch 227/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6101 - auc_5: 0.6526 - f05: 0.5177 - loss: 0.5496 - precision_5: 0.6144 - precision_focused: 0.5186 - recall_5: 0.6315\n",
      "Epoch 227: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 227: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.6101 - auc_5: 0.6526 - f05: 0.5177 - loss: 0.5496 - precision_5: 0.6144 - precision_focused: 0.5185 - recall_5: 0.6315 - val_accuracy: 0.5002 - val_auc_5: 0.5178 - val_f05: 0.4481 - val_loss: 0.5940 - val_precision_5: 0.5357 - val_precision_focused: 0.4583 - val_recall_5: 0.3148 - learning_rate: 2.2344e-05\n",
      "Epoch 228/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6002 - auc_5: 0.6446 - f05: 0.5142 - loss: 0.5511 - precision_5: 0.6054 - precision_focused: 0.5148 - recall_5: 0.6177\n",
      "Epoch 228: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 228: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6002 - auc_5: 0.6447 - f05: 0.5142 - loss: 0.5511 - precision_5: 0.6054 - precision_focused: 0.5148 - recall_5: 0.6176 - val_accuracy: 0.5040 - val_auc_5: 0.5165 - val_f05: 0.4554 - val_loss: 0.5952 - val_precision_5: 0.5395 - val_precision_focused: 0.4623 - val_recall_5: 0.3370 - learning_rate: 2.2344e-05\n",
      "Epoch 229/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6155 - auc_5: 0.6583 - f05: 0.5173 - loss: 0.5460 - precision_5: 0.6201 - precision_focused: 0.5181 - recall_5: 0.6372\n",
      "Epoch 229: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 229: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6155 - auc_5: 0.6582 - f05: 0.5172 - loss: 0.5460 - precision_5: 0.6200 - precision_focused: 0.5181 - recall_5: 0.6371 - val_accuracy: 0.5043 - val_auc_5: 0.5165 - val_f05: 0.4550 - val_loss: 0.5951 - val_precision_5: 0.5404 - val_precision_focused: 0.4623 - val_recall_5: 0.3346 - learning_rate: 2.2344e-05\n",
      "Epoch 230/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6134 - auc_5: 0.6561 - f05: 0.5148 - loss: 0.5467 - precision_5: 0.6201 - precision_focused: 0.5153 - recall_5: 0.6286\n",
      "Epoch 230: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 230: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6133 - auc_5: 0.6561 - f05: 0.5148 - loss: 0.5467 - precision_5: 0.6201 - precision_focused: 0.5152 - recall_5: 0.6285 - val_accuracy: 0.4986 - val_auc_5: 0.5177 - val_f05: 0.4488 - val_loss: 0.5945 - val_precision_5: 0.5328 - val_precision_focused: 0.4584 - val_recall_5: 0.3160 - learning_rate: 2.2344e-05\n",
      "Epoch 231/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6089 - auc_5: 0.6528 - f05: 0.5174 - loss: 0.5494 - precision_5: 0.6159 - precision_focused: 0.5180 - recall_5: 0.6270\n",
      "Epoch 231: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 231: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6089 - auc_5: 0.6528 - f05: 0.5174 - loss: 0.5494 - precision_5: 0.6159 - precision_focused: 0.5179 - recall_5: 0.6270 - val_accuracy: 0.5037 - val_auc_5: 0.5178 - val_f05: 0.4546 - val_loss: 0.5940 - val_precision_5: 0.5395 - val_precision_focused: 0.4620 - val_recall_5: 0.3333 - learning_rate: 2.2344e-05\n",
      "Epoch 232/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6085 - auc_5: 0.6488 - f05: 0.5071 - loss: 0.5520 - precision_5: 0.6117 - precision_focused: 0.5069 - recall_5: 0.6067\n",
      "Epoch 232: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 232: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6085 - auc_5: 0.6488 - f05: 0.5071 - loss: 0.5520 - precision_5: 0.6118 - precision_focused: 0.5069 - recall_5: 0.6067 - val_accuracy: 0.5034 - val_auc_5: 0.5175 - val_f05: 0.4733 - val_loss: 0.5918 - val_precision_5: 0.5342 - val_precision_focused: 0.4751 - val_recall_5: 0.3765 - learning_rate: 2.2344e-05\n",
      "Epoch 233/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6119 - auc_5: 0.6552 - f05: 0.5118 - loss: 0.5481 - precision_5: 0.6153 - precision_focused: 0.5125 - recall_5: 0.6303\n",
      "Epoch 233: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 233: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6118 - auc_5: 0.6551 - f05: 0.5118 - loss: 0.5482 - precision_5: 0.6153 - precision_focused: 0.5125 - recall_5: 0.6302 - val_accuracy: 0.5002 - val_auc_5: 0.5177 - val_f05: 0.4551 - val_loss: 0.5929 - val_precision_5: 0.5340 - val_precision_focused: 0.4626 - val_recall_5: 0.3296 - learning_rate: 2.2344e-05\n",
      "Epoch 234/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6156 - auc_5: 0.6591 - f05: 0.5193 - loss: 0.5452 - precision_5: 0.6238 - precision_focused: 0.5198 - recall_5: 0.6351\n",
      "Epoch 234: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 234: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6156 - auc_5: 0.6591 - f05: 0.5193 - loss: 0.5452 - precision_5: 0.6238 - precision_focused: 0.5198 - recall_5: 0.6351 - val_accuracy: 0.4986 - val_auc_5: 0.5177 - val_f05: 0.4445 - val_loss: 0.5945 - val_precision_5: 0.5338 - val_precision_focused: 0.4561 - val_recall_5: 0.3068 - learning_rate: 2.2344e-05\n",
      "Epoch 235/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6178 - auc_5: 0.6622 - f05: 0.5094 - loss: 0.5469 - precision_5: 0.6220 - precision_focused: 0.5094 - recall_5: 0.6207\n",
      "Epoch 235: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 235: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6178 - auc_5: 0.6622 - f05: 0.5094 - loss: 0.5470 - precision_5: 0.6220 - precision_focused: 0.5094 - recall_5: 0.6207 - val_accuracy: 0.4966 - val_auc_5: 0.5172 - val_f05: 0.4506 - val_loss: 0.5944 - val_precision_5: 0.5294 - val_precision_focused: 0.4597 - val_recall_5: 0.3167 - learning_rate: 2.2344e-05\n",
      "Epoch 236/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6178 - auc_5: 0.6623 - f05: 0.5135 - loss: 0.5465 - precision_5: 0.6231 - precision_focused: 0.5138 - recall_5: 0.6285\n",
      "Epoch 236: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 236: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6178 - auc_5: 0.6622 - f05: 0.5135 - loss: 0.5465 - precision_5: 0.6230 - precision_focused: 0.5138 - recall_5: 0.6284 - val_accuracy: 0.5002 - val_auc_5: 0.5170 - val_f05: 0.4546 - val_loss: 0.5939 - val_precision_5: 0.5343 - val_precision_focused: 0.4626 - val_recall_5: 0.3265 - learning_rate: 2.2344e-05\n",
      "Epoch 237/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6043 - auc_5: 0.6465 - f05: 0.5217 - loss: 0.5490 - precision_5: 0.6126 - precision_focused: 0.5227 - recall_5: 0.6332\n",
      "Epoch 237: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 237: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6043 - auc_5: 0.6465 - f05: 0.5217 - loss: 0.5490 - precision_5: 0.6126 - precision_focused: 0.5227 - recall_5: 0.6332 - val_accuracy: 0.5002 - val_auc_5: 0.5173 - val_f05: 0.4535 - val_loss: 0.5934 - val_precision_5: 0.5341 - val_precision_focused: 0.4612 - val_recall_5: 0.3284 - learning_rate: 2.2344e-05\n",
      "Epoch 238/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6194 - auc_5: 0.6647 - f05: 0.5152 - loss: 0.5449 - precision_5: 0.6281 - precision_focused: 0.5153 - recall_5: 0.6282\n",
      "Epoch 238: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 238: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6193 - auc_5: 0.6647 - f05: 0.5152 - loss: 0.5449 - precision_5: 0.6281 - precision_focused: 0.5153 - recall_5: 0.6282 - val_accuracy: 0.5002 - val_auc_5: 0.5161 - val_f05: 0.4641 - val_loss: 0.5930 - val_precision_5: 0.5315 - val_precision_focused: 0.4680 - val_recall_5: 0.3537 - learning_rate: 2.2344e-05\n",
      "Epoch 239/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6074 - auc_5: 0.6520 - f05: 0.5168 - loss: 0.5481 - precision_5: 0.6135 - precision_focused: 0.5176 - recall_5: 0.6317\n",
      "Epoch 239: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 239: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6075 - auc_5: 0.6520 - f05: 0.5167 - loss: 0.5481 - precision_5: 0.6135 - precision_focused: 0.5176 - recall_5: 0.6316 - val_accuracy: 0.4989 - val_auc_5: 0.5168 - val_f05: 0.4484 - val_loss: 0.5946 - val_precision_5: 0.5337 - val_precision_focused: 0.4588 - val_recall_5: 0.3130 - learning_rate: 2.2344e-05\n",
      "Epoch 240/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6133 - auc_5: 0.6536 - f05: 0.5087 - loss: 0.5498 - precision_5: 0.6205 - precision_focused: 0.5085 - recall_5: 0.6133\n",
      "Epoch 240: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 240: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6133 - auc_5: 0.6536 - f05: 0.5088 - loss: 0.5498 - precision_5: 0.6204 - precision_focused: 0.5085 - recall_5: 0.6133 - val_accuracy: 0.5056 - val_auc_5: 0.5159 - val_f05: 0.4685 - val_loss: 0.5938 - val_precision_5: 0.5387 - val_precision_focused: 0.4716 - val_recall_5: 0.3654 - learning_rate: 2.2344e-05\n",
      "Epoch 241/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6092 - auc_5: 0.6512 - f05: 0.5064 - loss: 0.5516 - precision_5: 0.6106 - precision_focused: 0.5066 - recall_5: 0.6167\n",
      "Epoch 241: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 241: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6092 - auc_5: 0.6512 - f05: 0.5064 - loss: 0.5516 - precision_5: 0.6106 - precision_focused: 0.5067 - recall_5: 0.6167 - val_accuracy: 0.5021 - val_auc_5: 0.5170 - val_f05: 0.4546 - val_loss: 0.5944 - val_precision_5: 0.5366 - val_precision_focused: 0.4615 - val_recall_5: 0.3352 - learning_rate: 2.2344e-05\n",
      "Epoch 242/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6027 - auc_5: 0.6454 - f05: 0.5218 - loss: 0.5498 - precision_5: 0.6109 - precision_focused: 0.5229 - recall_5: 0.6315\n",
      "Epoch 242: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 242: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6027 - auc_5: 0.6454 - f05: 0.5217 - loss: 0.5498 - precision_5: 0.6109 - precision_focused: 0.5228 - recall_5: 0.6314 - val_accuracy: 0.5014 - val_auc_5: 0.5162 - val_f05: 0.4636 - val_loss: 0.5937 - val_precision_5: 0.5337 - val_precision_focused: 0.4680 - val_recall_5: 0.3519 - learning_rate: 2.2344e-05\n",
      "Epoch 243/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6072 - auc_5: 0.6482 - f05: 0.5220 - loss: 0.5484 - precision_5: 0.6189 - precision_focused: 0.5222 - recall_5: 0.6243\n",
      "Epoch 243: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 243: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6072 - auc_5: 0.6483 - f05: 0.5219 - loss: 0.5484 - precision_5: 0.6189 - precision_focused: 0.5222 - recall_5: 0.6242 - val_accuracy: 0.5050 - val_auc_5: 0.5161 - val_f05: 0.4555 - val_loss: 0.5953 - val_precision_5: 0.5409 - val_precision_focused: 0.4623 - val_recall_5: 0.3389 - learning_rate: 2.2344e-05\n",
      "Epoch 244/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6134 - auc_5: 0.6616 - f05: 0.5062 - loss: 0.5449 - precision_5: 0.6170 - precision_focused: 0.5062 - recall_5: 0.6175\n",
      "Epoch 244: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 244: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6134 - auc_5: 0.6616 - f05: 0.5062 - loss: 0.5449 - precision_5: 0.6170 - precision_focused: 0.5062 - recall_5: 0.6175 - val_accuracy: 0.5053 - val_auc_5: 0.5152 - val_f05: 0.4756 - val_loss: 0.5936 - val_precision_5: 0.5359 - val_precision_focused: 0.4766 - val_recall_5: 0.3870 - learning_rate: 2.2344e-05\n",
      "Epoch 245/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6160 - auc_5: 0.6595 - f05: 0.5166 - loss: 0.5461 - precision_5: 0.6179 - precision_focused: 0.5179 - recall_5: 0.6459\n",
      "Epoch 245: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 245: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6160 - auc_5: 0.6595 - f05: 0.5166 - loss: 0.5462 - precision_5: 0.6178 - precision_focused: 0.5179 - recall_5: 0.6458 - val_accuracy: 0.5031 - val_auc_5: 0.5166 - val_f05: 0.4653 - val_loss: 0.5936 - val_precision_5: 0.5357 - val_precision_focused: 0.4693 - val_recall_5: 0.3562 - learning_rate: 2.2344e-05\n",
      "Epoch 246/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6091 - auc_5: 0.6515 - f05: 0.5213 - loss: 0.5471 - precision_5: 0.6192 - precision_focused: 0.5219 - recall_5: 0.6299\n",
      "Epoch 246: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 246: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6092 - auc_5: 0.6515 - f05: 0.5212 - loss: 0.5471 - precision_5: 0.6191 - precision_focused: 0.5218 - recall_5: 0.6299 - val_accuracy: 0.5053 - val_auc_5: 0.5179 - val_f05: 0.4364 - val_loss: 0.5962 - val_precision_5: 0.5477 - val_precision_focused: 0.4516 - val_recall_5: 0.2975 - learning_rate: 2.2344e-05\n",
      "Epoch 247/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6091 - auc_5: 0.6485 - f05: 0.5155 - loss: 0.5500 - precision_5: 0.6214 - precision_focused: 0.5151 - recall_5: 0.6091\n",
      "Epoch 247: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 247: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6091 - auc_5: 0.6486 - f05: 0.5155 - loss: 0.5500 - precision_5: 0.6214 - precision_focused: 0.5150 - recall_5: 0.6092 - val_accuracy: 0.5050 - val_auc_5: 0.5171 - val_f05: 0.4580 - val_loss: 0.5946 - val_precision_5: 0.5408 - val_precision_focused: 0.4646 - val_recall_5: 0.3395 - learning_rate: 2.2344e-05\n",
      "Epoch 248/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6097 - auc_5: 0.6565 - f05: 0.5155 - loss: 0.5464 - precision_5: 0.6135 - precision_focused: 0.5166 - recall_5: 0.6365\n",
      "Epoch 248: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 248: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6097 - auc_5: 0.6565 - f05: 0.5155 - loss: 0.5464 - precision_5: 0.6135 - precision_focused: 0.5166 - recall_5: 0.6365 - val_accuracy: 0.5037 - val_auc_5: 0.5171 - val_f05: 0.4464 - val_loss: 0.5965 - val_precision_5: 0.5426 - val_precision_focused: 0.4582 - val_recall_5: 0.3105 - learning_rate: 2.2344e-05\n",
      "Epoch 249/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6240 - auc_5: 0.6690 - f05: 0.5141 - loss: 0.5423 - precision_5: 0.6284 - precision_focused: 0.5147 - recall_5: 0.6406\n",
      "Epoch 249: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 249: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6240 - auc_5: 0.6690 - f05: 0.5141 - loss: 0.5423 - precision_5: 0.6284 - precision_focused: 0.5147 - recall_5: 0.6405 - val_accuracy: 0.5037 - val_auc_5: 0.5188 - val_f05: 0.4405 - val_loss: 0.5959 - val_precision_5: 0.5443 - val_precision_focused: 0.4549 - val_recall_5: 0.2994 - learning_rate: 2.2344e-05\n",
      "Epoch 250/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6102 - auc_5: 0.6464 - f05: 0.5129 - loss: 0.5516 - precision_5: 0.6139 - precision_focused: 0.5135 - recall_5: 0.6259\n",
      "Epoch 250: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 250: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6102 - auc_5: 0.6465 - f05: 0.5129 - loss: 0.5516 - precision_5: 0.6139 - precision_focused: 0.5135 - recall_5: 0.6259 - val_accuracy: 0.5047 - val_auc_5: 0.5187 - val_f05: 0.4562 - val_loss: 0.5943 - val_precision_5: 0.5414 - val_precision_focused: 0.4643 - val_recall_5: 0.3309 - learning_rate: 2.2344e-05\n",
      "Epoch 251/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6108 - auc_5: 0.6541 - f05: 0.5238 - loss: 0.5454 - precision_5: 0.6202 - precision_focused: 0.5245 - recall_5: 0.6365\n",
      "Epoch 251: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 251: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6109 - auc_5: 0.6541 - f05: 0.5237 - loss: 0.5454 - precision_5: 0.6202 - precision_focused: 0.5245 - recall_5: 0.6365 - val_accuracy: 0.5060 - val_auc_5: 0.5186 - val_f05: 0.4446 - val_loss: 0.5954 - val_precision_5: 0.5469 - val_precision_focused: 0.4572 - val_recall_5: 0.3093 - learning_rate: 2.2344e-05\n",
      "Epoch 252/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6181 - auc_5: 0.6644 - f05: 0.5122 - loss: 0.5440 - precision_5: 0.6303 - precision_focused: 0.5116 - recall_5: 0.6124\n",
      "Epoch 252: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 252: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6181 - auc_5: 0.6644 - f05: 0.5122 - loss: 0.5440 - precision_5: 0.6302 - precision_focused: 0.5116 - recall_5: 0.6125 - val_accuracy: 0.5021 - val_auc_5: 0.5174 - val_f05: 0.4546 - val_loss: 0.5949 - val_precision_5: 0.5377 - val_precision_focused: 0.4631 - val_recall_5: 0.3259 - learning_rate: 2.2344e-05\n",
      "Epoch 253/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6165 - auc_5: 0.6585 - f05: 0.5124 - loss: 0.5483 - precision_5: 0.6216 - precision_focused: 0.5126 - recall_5: 0.6238\n",
      "Epoch 253: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 253: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6165 - auc_5: 0.6585 - f05: 0.5124 - loss: 0.5483 - precision_5: 0.6215 - precision_focused: 0.5126 - recall_5: 0.6238 - val_accuracy: 0.5056 - val_auc_5: 0.5175 - val_f05: 0.4439 - val_loss: 0.5970 - val_precision_5: 0.5468 - val_precision_focused: 0.4570 - val_recall_5: 0.3068 - learning_rate: 2.2344e-05\n",
      "Epoch 254/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6193 - auc_5: 0.6629 - f05: 0.5183 - loss: 0.5437 - precision_5: 0.6295 - precision_focused: 0.5185 - recall_5: 0.6324\n",
      "Epoch 254: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 254: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6193 - auc_5: 0.6629 - f05: 0.5183 - loss: 0.5437 - precision_5: 0.6294 - precision_focused: 0.5184 - recall_5: 0.6323 - val_accuracy: 0.5060 - val_auc_5: 0.5178 - val_f05: 0.4430 - val_loss: 0.5967 - val_precision_5: 0.5475 - val_precision_focused: 0.4563 - val_recall_5: 0.3062 - learning_rate: 2.2344e-05\n",
      "Epoch 255/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6086 - auc_5: 0.6569 - f05: 0.5159 - loss: 0.5460 - precision_5: 0.6204 - precision_focused: 0.5156 - recall_5: 0.6121\n",
      "Epoch 255: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 255: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6086 - auc_5: 0.6569 - f05: 0.5159 - loss: 0.5460 - precision_5: 0.6204 - precision_focused: 0.5156 - recall_5: 0.6121 - val_accuracy: 0.5053 - val_auc_5: 0.5180 - val_f05: 0.4572 - val_loss: 0.5957 - val_precision_5: 0.5422 - val_precision_focused: 0.4650 - val_recall_5: 0.3333 - learning_rate: 2.2344e-05\n",
      "Epoch 256/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6188 - auc_5: 0.6587 - f05: 0.5071 - loss: 0.5479 - precision_5: 0.6207 - precision_focused: 0.5074 - recall_5: 0.6262\n",
      "Epoch 256: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 256: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6187 - auc_5: 0.6587 - f05: 0.5071 - loss: 0.5480 - precision_5: 0.6207 - precision_focused: 0.5074 - recall_5: 0.6262 - val_accuracy: 0.5047 - val_auc_5: 0.5176 - val_f05: 0.4470 - val_loss: 0.5960 - val_precision_5: 0.5444 - val_precision_focused: 0.4591 - val_recall_5: 0.3105 - learning_rate: 2.2344e-05\n",
      "Epoch 257/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6143 - auc_5: 0.6512 - f05: 0.5141 - loss: 0.5495 - precision_5: 0.6214 - precision_focused: 0.5144 - recall_5: 0.6271\n",
      "Epoch 257: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 257: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 257: ReduceLROnPlateau reducing learning rate to 1.78749454789795e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6143 - auc_5: 0.6513 - f05: 0.5140 - loss: 0.5495 - precision_5: 0.6214 - precision_focused: 0.5143 - recall_5: 0.6270 - val_accuracy: 0.5014 - val_auc_5: 0.5176 - val_f05: 0.4574 - val_loss: 0.5946 - val_precision_5: 0.5365 - val_precision_focused: 0.4657 - val_recall_5: 0.3265 - learning_rate: 2.2344e-05\n",
      "Epoch 258/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6238 - auc_5: 0.6638 - f05: 0.5053 - loss: 0.5456 - precision_5: 0.6222 - precision_focused: 0.5059 - recall_5: 0.6347\n",
      "Epoch 258: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 258: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6238 - auc_5: 0.6637 - f05: 0.5053 - loss: 0.5457 - precision_5: 0.6222 - precision_focused: 0.5059 - recall_5: 0.6347 - val_accuracy: 0.5043 - val_auc_5: 0.5175 - val_f05: 0.4460 - val_loss: 0.5960 - val_precision_5: 0.5445 - val_precision_focused: 0.4590 - val_recall_5: 0.3062 - learning_rate: 1.7875e-05\n",
      "Epoch 259/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6216 - auc_5: 0.6588 - f05: 0.5083 - loss: 0.5479 - precision_5: 0.6274 - precision_focused: 0.5082 - recall_5: 0.6235\n",
      "Epoch 259: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 259: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6215 - auc_5: 0.6587 - f05: 0.5083 - loss: 0.5479 - precision_5: 0.6273 - precision_focused: 0.5082 - recall_5: 0.6235 - val_accuracy: 0.5053 - val_auc_5: 0.5172 - val_f05: 0.4408 - val_loss: 0.5965 - val_precision_5: 0.5474 - val_precision_focused: 0.4556 - val_recall_5: 0.2994 - learning_rate: 1.7875e-05\n",
      "Epoch 260/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6125 - auc_5: 0.6561 - f05: 0.5122 - loss: 0.5481 - precision_5: 0.6190 - precision_focused: 0.5123 - recall_5: 0.6186\n",
      "Epoch 260: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 260: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6125 - auc_5: 0.6561 - f05: 0.5122 - loss: 0.5481 - precision_5: 0.6190 - precision_focused: 0.5123 - recall_5: 0.6186 - val_accuracy: 0.5024 - val_auc_5: 0.5168 - val_f05: 0.4504 - val_loss: 0.5952 - val_precision_5: 0.5398 - val_precision_focused: 0.4612 - val_recall_5: 0.3142 - learning_rate: 1.7875e-05\n",
      "Epoch 261/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6214 - auc_5: 0.6673 - f05: 0.5223 - loss: 0.5413 - precision_5: 0.6295 - precision_focused: 0.5232 - recall_5: 0.6487\n",
      "Epoch 261: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 261: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6214 - auc_5: 0.6673 - f05: 0.5222 - loss: 0.5413 - precision_5: 0.6295 - precision_focused: 0.5231 - recall_5: 0.6487 - val_accuracy: 0.5043 - val_auc_5: 0.5174 - val_f05: 0.4430 - val_loss: 0.5963 - val_precision_5: 0.5447 - val_precision_focused: 0.4562 - val_recall_5: 0.3049 - learning_rate: 1.7875e-05\n",
      "Epoch 262/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6105 - auc_5: 0.6615 - f05: 0.5082 - loss: 0.5465 - precision_5: 0.6148 - precision_focused: 0.5081 - recall_5: 0.6100\n",
      "Epoch 262: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 262: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6105 - auc_5: 0.6615 - f05: 0.5082 - loss: 0.5465 - precision_5: 0.6148 - precision_focused: 0.5081 - recall_5: 0.6100 - val_accuracy: 0.5040 - val_auc_5: 0.5169 - val_f05: 0.4620 - val_loss: 0.5950 - val_precision_5: 0.5388 - val_precision_focused: 0.4679 - val_recall_5: 0.3432 - learning_rate: 1.7875e-05\n",
      "Epoch 263/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6096 - auc_5: 0.6552 - f05: 0.5103 - loss: 0.5498 - precision_5: 0.6107 - precision_focused: 0.5110 - recall_5: 0.6231\n",
      "Epoch 263: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 263: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6095 - auc_5: 0.6552 - f05: 0.5103 - loss: 0.5498 - precision_5: 0.6107 - precision_focused: 0.5110 - recall_5: 0.6230 - val_accuracy: 0.5037 - val_auc_5: 0.5179 - val_f05: 0.4611 - val_loss: 0.5941 - val_precision_5: 0.5384 - val_precision_focused: 0.4672 - val_recall_5: 0.3420 - learning_rate: 1.7875e-05\n",
      "Epoch 264/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6229 - auc_5: 0.6637 - f05: 0.5154 - loss: 0.5441 - precision_5: 0.6309 - precision_focused: 0.5156 - recall_5: 0.6344\n",
      "Epoch 264: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 264: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6229 - auc_5: 0.6636 - f05: 0.5154 - loss: 0.5441 - precision_5: 0.6309 - precision_focused: 0.5156 - recall_5: 0.6344 - val_accuracy: 0.5034 - val_auc_5: 0.5176 - val_f05: 0.4644 - val_loss: 0.5946 - val_precision_5: 0.5367 - val_precision_focused: 0.4689 - val_recall_5: 0.3525 - learning_rate: 1.7875e-05\n",
      "Epoch 265/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6103 - auc_5: 0.6588 - f05: 0.5015 - loss: 0.5487 - precision_5: 0.6100 - precision_focused: 0.5014 - recall_5: 0.6071\n",
      "Epoch 265: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 265: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6104 - auc_5: 0.6588 - f05: 0.5015 - loss: 0.5487 - precision_5: 0.6101 - precision_focused: 0.5014 - recall_5: 0.6071 - val_accuracy: 0.5011 - val_auc_5: 0.5178 - val_f05: 0.4591 - val_loss: 0.5952 - val_precision_5: 0.5353 - val_precision_focused: 0.4663 - val_recall_5: 0.3327 - learning_rate: 1.7875e-05\n",
      "Epoch 266/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6060 - auc_5: 0.6503 - f05: 0.5102 - loss: 0.5506 - precision_5: 0.6077 - precision_focused: 0.5112 - recall_5: 0.6278\n",
      "Epoch 266: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 266: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6060 - auc_5: 0.6503 - f05: 0.5102 - loss: 0.5506 - precision_5: 0.6077 - precision_focused: 0.5112 - recall_5: 0.6278 - val_accuracy: 0.5024 - val_auc_5: 0.5179 - val_f05: 0.4488 - val_loss: 0.5956 - val_precision_5: 0.5401 - val_precision_focused: 0.4601 - val_recall_5: 0.3117 - learning_rate: 1.7875e-05\n",
      "Epoch 267/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6153 - auc_5: 0.6607 - f05: 0.5109 - loss: 0.5468 - precision_5: 0.6190 - precision_focused: 0.5114 - recall_5: 0.6289\n",
      "Epoch 267: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 267: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6153 - auc_5: 0.6607 - f05: 0.5109 - loss: 0.5468 - precision_5: 0.6190 - precision_focused: 0.5114 - recall_5: 0.6289 - val_accuracy: 0.5031 - val_auc_5: 0.5173 - val_f05: 0.4570 - val_loss: 0.5958 - val_precision_5: 0.5385 - val_precision_focused: 0.4645 - val_recall_5: 0.3321 - learning_rate: 1.7875e-05\n",
      "Epoch 268/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6183 - auc_5: 0.6616 - f05: 0.5176 - loss: 0.5452 - precision_5: 0.6245 - precision_focused: 0.5183 - recall_5: 0.6383\n",
      "Epoch 268: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 268: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6184 - auc_5: 0.6616 - f05: 0.5176 - loss: 0.5452 - precision_5: 0.6245 - precision_focused: 0.5182 - recall_5: 0.6383 - val_accuracy: 0.5082 - val_auc_5: 0.5172 - val_f05: 0.4595 - val_loss: 0.5965 - val_precision_5: 0.5455 - val_precision_focused: 0.4659 - val_recall_5: 0.3444 - learning_rate: 1.7875e-05\n",
      "Epoch 269/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6145 - auc_5: 0.6592 - f05: 0.5222 - loss: 0.5444 - precision_5: 0.6209 - precision_focused: 0.5232 - recall_5: 0.6413\n",
      "Epoch 269: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 269: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6145 - auc_5: 0.6593 - f05: 0.5221 - loss: 0.5444 - precision_5: 0.6209 - precision_focused: 0.5232 - recall_5: 0.6413 - val_accuracy: 0.5031 - val_auc_5: 0.5172 - val_f05: 0.4624 - val_loss: 0.5955 - val_precision_5: 0.5372 - val_precision_focused: 0.4682 - val_recall_5: 0.3432 - learning_rate: 1.7875e-05\n",
      "Epoch 270/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6139 - auc_5: 0.6556 - f05: 0.5122 - loss: 0.5481 - precision_5: 0.6133 - precision_focused: 0.5137 - recall_5: 0.6424\n",
      "Epoch 270: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 270: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6139 - auc_5: 0.6556 - f05: 0.5122 - loss: 0.5481 - precision_5: 0.6133 - precision_focused: 0.5137 - recall_5: 0.6424 - val_accuracy: 0.5040 - val_auc_5: 0.5176 - val_f05: 0.4541 - val_loss: 0.5957 - val_precision_5: 0.5414 - val_precision_focused: 0.4636 - val_recall_5: 0.3228 - learning_rate: 1.7875e-05\n",
      "Epoch 271/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6124 - auc_5: 0.6514 - f05: 0.5210 - loss: 0.5489 - precision_5: 0.6183 - precision_focused: 0.5221 - recall_5: 0.6417\n",
      "Epoch 271: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 271: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6124 - auc_5: 0.6514 - f05: 0.5209 - loss: 0.5489 - precision_5: 0.6183 - precision_focused: 0.5220 - recall_5: 0.6416 - val_accuracy: 0.5076 - val_auc_5: 0.5173 - val_f05: 0.4584 - val_loss: 0.5957 - val_precision_5: 0.5454 - val_precision_focused: 0.4658 - val_recall_5: 0.3377 - learning_rate: 1.7875e-05\n",
      "Epoch 272/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6175 - auc_5: 0.6639 - f05: 0.5197 - loss: 0.5439 - precision_5: 0.6261 - precision_focused: 0.5200 - recall_5: 0.6337\n",
      "Epoch 272: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 272: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6175 - auc_5: 0.6639 - f05: 0.5196 - loss: 0.5439 - precision_5: 0.6261 - precision_focused: 0.5199 - recall_5: 0.6335 - val_accuracy: 0.5085 - val_auc_5: 0.5168 - val_f05: 0.4652 - val_loss: 0.5953 - val_precision_5: 0.5444 - val_precision_focused: 0.4700 - val_recall_5: 0.3556 - learning_rate: 1.7875e-05\n",
      "Epoch 273/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6089 - auc_5: 0.6515 - f05: 0.5145 - loss: 0.5482 - precision_5: 0.6155 - precision_focused: 0.5148 - recall_5: 0.6240\n",
      "Epoch 273: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 273: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6089 - auc_5: 0.6516 - f05: 0.5145 - loss: 0.5481 - precision_5: 0.6156 - precision_focused: 0.5148 - recall_5: 0.6241 - val_accuracy: 0.5063 - val_auc_5: 0.5173 - val_f05: 0.4520 - val_loss: 0.5966 - val_precision_5: 0.5450 - val_precision_focused: 0.4614 - val_recall_5: 0.3253 - learning_rate: 1.7875e-05\n",
      "Epoch 274/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6227 - auc_5: 0.6691 - f05: 0.5115 - loss: 0.5428 - precision_5: 0.6272 - precision_focused: 0.5116 - recall_5: 0.6290\n",
      "Epoch 274: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 274: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6227 - auc_5: 0.6690 - f05: 0.5115 - loss: 0.5428 - precision_5: 0.6272 - precision_focused: 0.5116 - recall_5: 0.6289 - val_accuracy: 0.5069 - val_auc_5: 0.5171 - val_f05: 0.4591 - val_loss: 0.5961 - val_precision_5: 0.5441 - val_precision_focused: 0.4662 - val_recall_5: 0.3389 - learning_rate: 1.7875e-05\n",
      "Epoch 275/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6065 - auc_5: 0.6492 - f05: 0.5149 - loss: 0.5504 - precision_5: 0.6116 - precision_focused: 0.5153 - recall_5: 0.6186\n",
      "Epoch 275: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 275: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6066 - auc_5: 0.6492 - f05: 0.5148 - loss: 0.5503 - precision_5: 0.6116 - precision_focused: 0.5152 - recall_5: 0.6186 - val_accuracy: 0.5047 - val_auc_5: 0.5179 - val_f05: 0.4484 - val_loss: 0.5965 - val_precision_5: 0.5436 - val_precision_focused: 0.4595 - val_recall_5: 0.3154 - learning_rate: 1.7875e-05\n",
      "Epoch 276/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6120 - auc_5: 0.6621 - f05: 0.5045 - loss: 0.5460 - precision_5: 0.6119 - precision_focused: 0.5045 - recall_5: 0.6112\n",
      "Epoch 276: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 276: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6120 - auc_5: 0.6621 - f05: 0.5045 - loss: 0.5460 - precision_5: 0.6119 - precision_focused: 0.5045 - recall_5: 0.6112 - val_accuracy: 0.5053 - val_auc_5: 0.5170 - val_f05: 0.4585 - val_loss: 0.5959 - val_precision_5: 0.5419 - val_precision_focused: 0.4659 - val_recall_5: 0.3352 - learning_rate: 1.7875e-05\n",
      "Epoch 277/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6096 - auc_5: 0.6511 - f05: 0.5175 - loss: 0.5488 - precision_5: 0.6171 - precision_focused: 0.5180 - recall_5: 0.6263\n",
      "Epoch 277: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 277: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.6096 - auc_5: 0.6512 - f05: 0.5175 - loss: 0.5488 - precision_5: 0.6171 - precision_focused: 0.5179 - recall_5: 0.6262 - val_accuracy: 0.5053 - val_auc_5: 0.5175 - val_f05: 0.4570 - val_loss: 0.5959 - val_precision_5: 0.5423 - val_precision_focused: 0.4649 - val_recall_5: 0.3321 - learning_rate: 1.7875e-05\n",
      "Epoch 278/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6172 - auc_5: 0.6712 - f05: 0.5185 - loss: 0.5391 - precision_5: 0.6252 - precision_focused: 0.5190 - recall_5: 0.6352\n",
      "Epoch 278: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 278: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6172 - auc_5: 0.6711 - f05: 0.5185 - loss: 0.5391 - precision_5: 0.6252 - precision_focused: 0.5189 - recall_5: 0.6352 - val_accuracy: 0.5031 - val_auc_5: 0.5170 - val_f05: 0.4465 - val_loss: 0.5987 - val_precision_5: 0.5414 - val_precision_focused: 0.4581 - val_recall_5: 0.3111 - learning_rate: 1.7875e-05\n",
      "Epoch 279/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6161 - auc_5: 0.6634 - f05: 0.5139 - loss: 0.5431 - precision_5: 0.6265 - precision_focused: 0.5137 - recall_5: 0.6189\n",
      "Epoch 279: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 279: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6161 - auc_5: 0.6634 - f05: 0.5139 - loss: 0.5431 - precision_5: 0.6265 - precision_focused: 0.5136 - recall_5: 0.6189 - val_accuracy: 0.5040 - val_auc_5: 0.5173 - val_f05: 0.4555 - val_loss: 0.5968 - val_precision_5: 0.5407 - val_precision_focused: 0.4641 - val_recall_5: 0.3278 - learning_rate: 1.7875e-05\n",
      "Epoch 280/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6207 - auc_5: 0.6664 - f05: 0.5204 - loss: 0.5436 - precision_5: 0.6275 - precision_focused: 0.5212 - recall_5: 0.6438\n",
      "Epoch 280: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 280: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6207 - auc_5: 0.6664 - f05: 0.5203 - loss: 0.5436 - precision_5: 0.6275 - precision_focused: 0.5211 - recall_5: 0.6437 - val_accuracy: 0.5082 - val_auc_5: 0.5167 - val_f05: 0.4666 - val_loss: 0.5966 - val_precision_5: 0.5435 - val_precision_focused: 0.4709 - val_recall_5: 0.3586 - learning_rate: 1.7875e-05\n",
      "Epoch 281/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6139 - auc_5: 0.6618 - f05: 0.5225 - loss: 0.5449 - precision_5: 0.6212 - precision_focused: 0.5236 - recall_5: 0.6424\n",
      "Epoch 281: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 281: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6139 - auc_5: 0.6618 - f05: 0.5224 - loss: 0.5450 - precision_5: 0.6211 - precision_focused: 0.5235 - recall_5: 0.6423 - val_accuracy: 0.5027 - val_auc_5: 0.5178 - val_f05: 0.4379 - val_loss: 0.5996 - val_precision_5: 0.5436 - val_precision_focused: 0.4539 - val_recall_5: 0.2926 - learning_rate: 1.7875e-05\n",
      "Epoch 282/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6079 - auc_5: 0.6557 - f05: 0.5166 - loss: 0.5459 - precision_5: 0.6202 - precision_focused: 0.5163 - recall_5: 0.6117\n",
      "Epoch 282: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 282: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6079 - auc_5: 0.6558 - f05: 0.5166 - loss: 0.5459 - precision_5: 0.6202 - precision_focused: 0.5163 - recall_5: 0.6118 - val_accuracy: 0.5024 - val_auc_5: 0.5177 - val_f05: 0.4503 - val_loss: 0.5985 - val_precision_5: 0.5392 - val_precision_focused: 0.4602 - val_recall_5: 0.3185 - learning_rate: 1.7875e-05\n",
      "Epoch 283/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6187 - auc_5: 0.6621 - f05: 0.5135 - loss: 0.5457 - precision_5: 0.6237 - precision_focused: 0.5137 - recall_5: 0.6274\n",
      "Epoch 283: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 283: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6186 - auc_5: 0.6621 - f05: 0.5135 - loss: 0.5457 - precision_5: 0.6236 - precision_focused: 0.5137 - recall_5: 0.6273 - val_accuracy: 0.5027 - val_auc_5: 0.5171 - val_f05: 0.4474 - val_loss: 0.5986 - val_precision_5: 0.5408 - val_precision_focused: 0.4589 - val_recall_5: 0.3111 - learning_rate: 1.7875e-05\n",
      "Epoch 284/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6105 - auc_5: 0.6587 - f05: 0.5065 - loss: 0.5463 - precision_5: 0.6164 - precision_focused: 0.5061 - recall_5: 0.6058\n",
      "Epoch 284: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 284: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6105 - auc_5: 0.6587 - f05: 0.5065 - loss: 0.5463 - precision_5: 0.6165 - precision_focused: 0.5062 - recall_5: 0.6059 - val_accuracy: 0.5079 - val_auc_5: 0.5168 - val_f05: 0.4603 - val_loss: 0.5980 - val_precision_5: 0.5447 - val_precision_focused: 0.4663 - val_recall_5: 0.3463 - learning_rate: 1.7875e-05\n",
      "Epoch 285/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6196 - auc_5: 0.6651 - f05: 0.5197 - loss: 0.5435 - precision_5: 0.6267 - precision_focused: 0.5205 - recall_5: 0.6427\n",
      "Epoch 285: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 285: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6196 - auc_5: 0.6650 - f05: 0.5197 - loss: 0.5436 - precision_5: 0.6266 - precision_focused: 0.5204 - recall_5: 0.6426 - val_accuracy: 0.5024 - val_auc_5: 0.5169 - val_f05: 0.4541 - val_loss: 0.5981 - val_precision_5: 0.5385 - val_precision_focused: 0.4630 - val_recall_5: 0.3241 - learning_rate: 1.7875e-05\n",
      "Epoch 286/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6161 - auc_5: 0.6598 - f05: 0.5145 - loss: 0.5464 - precision_5: 0.6182 - precision_focused: 0.5154 - recall_5: 0.6368\n",
      "Epoch 286: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 286: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6161 - auc_5: 0.6598 - f05: 0.5145 - loss: 0.5464 - precision_5: 0.6182 - precision_focused: 0.5154 - recall_5: 0.6367 - val_accuracy: 0.5050 - val_auc_5: 0.5179 - val_f05: 0.4558 - val_loss: 0.5980 - val_precision_5: 0.5422 - val_precision_focused: 0.4643 - val_recall_5: 0.3290 - learning_rate: 1.7875e-05\n",
      "Epoch 287/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6154 - auc_5: 0.6666 - f05: 0.5150 - loss: 0.5426 - precision_5: 0.6227 - precision_focused: 0.5152 - recall_5: 0.6273\n",
      "Epoch 287: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 287: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6154 - auc_5: 0.6666 - f05: 0.5149 - loss: 0.5426 - precision_5: 0.6227 - precision_focused: 0.5151 - recall_5: 0.6273 - val_accuracy: 0.5014 - val_auc_5: 0.5189 - val_f05: 0.4451 - val_loss: 0.5974 - val_precision_5: 0.5393 - val_precision_focused: 0.4578 - val_recall_5: 0.3049 - learning_rate: 1.7875e-05\n",
      "Epoch 288/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6255 - auc_5: 0.6705 - f05: 0.5051 - loss: 0.5433 - precision_5: 0.6307 - precision_focused: 0.5047 - recall_5: 0.6179\n",
      "Epoch 288: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 288: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6254 - auc_5: 0.6704 - f05: 0.5052 - loss: 0.5433 - precision_5: 0.6306 - precision_focused: 0.5048 - recall_5: 0.6180 - val_accuracy: 0.5053 - val_auc_5: 0.5184 - val_f05: 0.4479 - val_loss: 0.5974 - val_precision_5: 0.5448 - val_precision_focused: 0.4591 - val_recall_5: 0.3154 - learning_rate: 1.7875e-05\n",
      "Epoch 289/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6283 - auc_5: 0.6725 - f05: 0.5104 - loss: 0.5409 - precision_5: 0.6351 - precision_focused: 0.5103 - recall_5: 0.6314\n",
      "Epoch 289: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 289: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6283 - auc_5: 0.6725 - f05: 0.5104 - loss: 0.5409 - precision_5: 0.6351 - precision_focused: 0.5103 - recall_5: 0.6314 - val_accuracy: 0.4998 - val_auc_5: 0.5184 - val_f05: 0.4428 - val_loss: 0.5986 - val_precision_5: 0.5370 - val_precision_focused: 0.4562 - val_recall_5: 0.3000 - learning_rate: 1.7875e-05\n",
      "Epoch 290/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6287 - auc_5: 0.6692 - f05: 0.5006 - loss: 0.5444 - precision_5: 0.6369 - precision_focused: 0.4996 - recall_5: 0.6073\n",
      "Epoch 290: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 290: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6286 - auc_5: 0.6691 - f05: 0.5007 - loss: 0.5445 - precision_5: 0.6367 - precision_focused: 0.4997 - recall_5: 0.6073 - val_accuracy: 0.5031 - val_auc_5: 0.5174 - val_f05: 0.4543 - val_loss: 0.5972 - val_precision_5: 0.5392 - val_precision_focused: 0.4628 - val_recall_5: 0.3272 - learning_rate: 1.7875e-05\n",
      "Epoch 291/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6208 - auc_5: 0.6673 - f05: 0.5200 - loss: 0.5422 - precision_5: 0.6287 - precision_focused: 0.5205 - recall_5: 0.6395\n",
      "Epoch 291: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 291: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6208 - auc_5: 0.6673 - f05: 0.5199 - loss: 0.5422 - precision_5: 0.6287 - precision_focused: 0.5204 - recall_5: 0.6394 - val_accuracy: 0.5047 - val_auc_5: 0.5165 - val_f05: 0.4548 - val_loss: 0.5983 - val_precision_5: 0.5418 - val_precision_focused: 0.4633 - val_recall_5: 0.3284 - learning_rate: 1.7875e-05\n",
      "Epoch 292/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6105 - auc_5: 0.6561 - f05: 0.5193 - loss: 0.5454 - precision_5: 0.6186 - precision_focused: 0.5199 - recall_5: 0.6308\n",
      "Epoch 292: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 292: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6106 - auc_5: 0.6561 - f05: 0.5193 - loss: 0.5454 - precision_5: 0.6186 - precision_focused: 0.5199 - recall_5: 0.6308 - val_accuracy: 0.5027 - val_auc_5: 0.5166 - val_f05: 0.4536 - val_loss: 0.5981 - val_precision_5: 0.5388 - val_precision_focused: 0.4622 - val_recall_5: 0.3259 - learning_rate: 1.7875e-05\n",
      "Epoch 293/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6219 - auc_5: 0.6752 - f05: 0.5194 - loss: 0.5369 - precision_5: 0.6355 - precision_focused: 0.5194 - recall_5: 0.6324\n",
      "Epoch 293: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 293: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6219 - auc_5: 0.6752 - f05: 0.5194 - loss: 0.5369 - precision_5: 0.6354 - precision_focused: 0.5194 - recall_5: 0.6324 - val_accuracy: 0.5005 - val_auc_5: 0.5165 - val_f05: 0.4491 - val_loss: 0.5985 - val_precision_5: 0.5363 - val_precision_focused: 0.4594 - val_recall_5: 0.3148 - learning_rate: 1.7875e-05\n",
      "Epoch 294/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6251 - auc_5: 0.6643 - f05: 0.5089 - loss: 0.5452 - precision_5: 0.6338 - precision_focused: 0.5084 - recall_5: 0.6195\n",
      "Epoch 294: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 294: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6251 - auc_5: 0.6643 - f05: 0.5089 - loss: 0.5452 - precision_5: 0.6338 - precision_focused: 0.5084 - recall_5: 0.6195 - val_accuracy: 0.5069 - val_auc_5: 0.5165 - val_f05: 0.4605 - val_loss: 0.5972 - val_precision_5: 0.5431 - val_precision_focused: 0.4664 - val_recall_5: 0.3463 - learning_rate: 1.7875e-05\n",
      "Epoch 295/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6128 - auc_5: 0.6583 - f05: 0.5169 - loss: 0.5454 - precision_5: 0.6209 - precision_focused: 0.5172 - recall_5: 0.6276\n",
      "Epoch 295: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 295: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6128 - auc_5: 0.6583 - f05: 0.5169 - loss: 0.5454 - precision_5: 0.6209 - precision_focused: 0.5172 - recall_5: 0.6275 - val_accuracy: 0.5027 - val_auc_5: 0.5172 - val_f05: 0.4518 - val_loss: 0.5980 - val_precision_5: 0.5394 - val_precision_focused: 0.4614 - val_recall_5: 0.3210 - learning_rate: 1.7875e-05\n",
      "Epoch 296/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6206 - auc_5: 0.6661 - f05: 0.5065 - loss: 0.5444 - precision_5: 0.6205 - precision_focused: 0.5070 - recall_5: 0.6280\n",
      "Epoch 296: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 296: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6206 - auc_5: 0.6661 - f05: 0.5065 - loss: 0.5444 - precision_5: 0.6205 - precision_focused: 0.5070 - recall_5: 0.6280 - val_accuracy: 0.5027 - val_auc_5: 0.5169 - val_f05: 0.4567 - val_loss: 0.5974 - val_precision_5: 0.5378 - val_precision_focused: 0.4639 - val_recall_5: 0.3340 - learning_rate: 1.7875e-05\n",
      "Epoch 297/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6201 - auc_5: 0.6632 - f05: 0.5056 - loss: 0.5461 - precision_5: 0.6221 - precision_focused: 0.5059 - recall_5: 0.6278\n",
      "Epoch 297: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 297: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6200 - auc_5: 0.6633 - f05: 0.5056 - loss: 0.5461 - precision_5: 0.6221 - precision_focused: 0.5059 - recall_5: 0.6278 - val_accuracy: 0.5014 - val_auc_5: 0.5158 - val_f05: 0.4618 - val_loss: 0.5975 - val_precision_5: 0.5348 - val_precision_focused: 0.4676 - val_recall_5: 0.3414 - learning_rate: 1.7875e-05\n",
      "Epoch 298/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6102 - auc_5: 0.6519 - f05: 0.5144 - loss: 0.5495 - precision_5: 0.6197 - precision_focused: 0.5144 - recall_5: 0.6181\n",
      "Epoch 298: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 298: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6102 - auc_5: 0.6519 - f05: 0.5144 - loss: 0.5495 - precision_5: 0.6198 - precision_focused: 0.5144 - recall_5: 0.6181 - val_accuracy: 0.5014 - val_auc_5: 0.5160 - val_f05: 0.4537 - val_loss: 0.5980 - val_precision_5: 0.5364 - val_precision_focused: 0.4619 - val_recall_5: 0.3272 - learning_rate: 1.7875e-05\n",
      "Epoch 299/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6216 - auc_5: 0.6630 - f05: 0.5093 - loss: 0.5450 - precision_5: 0.6297 - precision_focused: 0.5089 - recall_5: 0.6187\n",
      "Epoch 299: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 299: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6216 - auc_5: 0.6630 - f05: 0.5093 - loss: 0.5450 - precision_5: 0.6296 - precision_focused: 0.5089 - recall_5: 0.6188 - val_accuracy: 0.5066 - val_auc_5: 0.5164 - val_f05: 0.4600 - val_loss: 0.5972 - val_precision_5: 0.5425 - val_precision_focused: 0.4658 - val_recall_5: 0.3469 - learning_rate: 1.7875e-05\n",
      "Epoch 300/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6184 - auc_5: 0.6645 - f05: 0.5090 - loss: 0.5439 - precision_5: 0.6272 - precision_focused: 0.5087 - recall_5: 0.6178\n",
      "Epoch 300: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 300: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6184 - auc_5: 0.6645 - f05: 0.5091 - loss: 0.5439 - precision_5: 0.6271 - precision_focused: 0.5087 - recall_5: 0.6178 - val_accuracy: 0.5043 - val_auc_5: 0.5171 - val_f05: 0.4545 - val_loss: 0.5980 - val_precision_5: 0.5405 - val_precision_focused: 0.4620 - val_recall_5: 0.3333 - learning_rate: 1.7875e-05\n",
      "Epoch 301/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6162 - auc_5: 0.6592 - f05: 0.5148 - loss: 0.5463 - precision_5: 0.6263 - precision_focused: 0.5146 - recall_5: 0.6218\n",
      "Epoch 301: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 301: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6162 - auc_5: 0.6592 - f05: 0.5148 - loss: 0.5463 - precision_5: 0.6263 - precision_focused: 0.5146 - recall_5: 0.6218 - val_accuracy: 0.5069 - val_auc_5: 0.5172 - val_f05: 0.4581 - val_loss: 0.5972 - val_precision_5: 0.5434 - val_precision_focused: 0.4643 - val_recall_5: 0.3438 - learning_rate: 1.7875e-05\n",
      "Epoch 302/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6219 - auc_5: 0.6701 - f05: 0.5180 - loss: 0.5417 - precision_5: 0.6259 - precision_focused: 0.5188 - recall_5: 0.6429\n",
      "Epoch 302: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 302: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6218 - auc_5: 0.6700 - f05: 0.5180 - loss: 0.5417 - precision_5: 0.6258 - precision_focused: 0.5188 - recall_5: 0.6428 - val_accuracy: 0.5031 - val_auc_5: 0.5173 - val_f05: 0.4516 - val_loss: 0.5978 - val_precision_5: 0.5394 - val_precision_focused: 0.4603 - val_recall_5: 0.3253 - learning_rate: 1.7875e-05\n",
      "Epoch 303/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6148 - auc_5: 0.6588 - f05: 0.5021 - loss: 0.5468 - precision_5: 0.6207 - precision_focused: 0.5014 - recall_5: 0.5993\n",
      "Epoch 303: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 303: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6148 - auc_5: 0.6588 - f05: 0.5022 - loss: 0.5468 - precision_5: 0.6207 - precision_focused: 0.5015 - recall_5: 0.5994 - val_accuracy: 0.5101 - val_auc_5: 0.5158 - val_f05: 0.4704 - val_loss: 0.5956 - val_precision_5: 0.5445 - val_precision_focused: 0.4731 - val_recall_5: 0.3741 - learning_rate: 1.7875e-05\n",
      "Epoch 304/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6264 - auc_5: 0.6681 - f05: 0.5136 - loss: 0.5432 - precision_5: 0.6265 - precision_focused: 0.5148 - recall_5: 0.6512\n",
      "Epoch 304: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 304: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6264 - auc_5: 0.6680 - f05: 0.5136 - loss: 0.5433 - precision_5: 0.6264 - precision_focused: 0.5148 - recall_5: 0.6511 - val_accuracy: 0.5063 - val_auc_5: 0.5169 - val_f05: 0.4588 - val_loss: 0.5970 - val_precision_5: 0.5424 - val_precision_focused: 0.4651 - val_recall_5: 0.3432 - learning_rate: 1.7875e-05\n",
      "Epoch 305/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6169 - auc_5: 0.6588 - f05: 0.5145 - loss: 0.5463 - precision_5: 0.6212 - precision_focused: 0.5150 - recall_5: 0.6304\n",
      "Epoch 305: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 305: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6168 - auc_5: 0.6588 - f05: 0.5145 - loss: 0.5464 - precision_5: 0.6212 - precision_focused: 0.5150 - recall_5: 0.6303 - val_accuracy: 0.5085 - val_auc_5: 0.5179 - val_f05: 0.4640 - val_loss: 0.5957 - val_precision_5: 0.5441 - val_precision_focused: 0.4685 - val_recall_5: 0.3580 - learning_rate: 1.7875e-05\n",
      "Epoch 306/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6213 - auc_5: 0.6677 - f05: 0.5141 - loss: 0.5442 - precision_5: 0.6196 - precision_focused: 0.5156 - recall_5: 0.6497\n",
      "Epoch 306: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 306: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6214 - auc_5: 0.6677 - f05: 0.5141 - loss: 0.5442 - precision_5: 0.6196 - precision_focused: 0.5156 - recall_5: 0.6496 - val_accuracy: 0.5082 - val_auc_5: 0.5170 - val_f05: 0.4608 - val_loss: 0.5970 - val_precision_5: 0.5444 - val_precision_focused: 0.4660 - val_recall_5: 0.3519 - learning_rate: 1.7875e-05\n",
      "Epoch 307/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6038 - auc_5: 0.6496 - f05: 0.5130 - loss: 0.5511 - precision_5: 0.6081 - precision_focused: 0.5136 - recall_5: 0.6206\n",
      "Epoch 307: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 307: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 307: ReduceLROnPlateau reducing learning rate to 1.4299956092145294e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6039 - auc_5: 0.6497 - f05: 0.5130 - loss: 0.5511 - precision_5: 0.6082 - precision_focused: 0.5136 - recall_5: 0.6206 - val_accuracy: 0.5069 - val_auc_5: 0.5174 - val_f05: 0.4593 - val_loss: 0.5964 - val_precision_5: 0.5430 - val_precision_focused: 0.4650 - val_recall_5: 0.3469 - learning_rate: 1.7875e-05\n",
      "Epoch 308/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6197 - auc_5: 0.6662 - f05: 0.5190 - loss: 0.5423 - precision_5: 0.6272 - precision_focused: 0.5195 - recall_5: 0.6364\n",
      "Epoch 308: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 308: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6197 - auc_5: 0.6661 - f05: 0.5190 - loss: 0.5423 - precision_5: 0.6272 - precision_focused: 0.5194 - recall_5: 0.6363 - val_accuracy: 0.5060 - val_auc_5: 0.5172 - val_f05: 0.4588 - val_loss: 0.5967 - val_precision_5: 0.5417 - val_precision_focused: 0.4648 - val_recall_5: 0.3444 - learning_rate: 1.4300e-05\n",
      "Epoch 309/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6153 - auc_5: 0.6589 - f05: 0.5074 - loss: 0.5476 - precision_5: 0.6185 - precision_focused: 0.5077 - recall_5: 0.6224\n",
      "Epoch 309: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 309: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6153 - auc_5: 0.6589 - f05: 0.5075 - loss: 0.5476 - precision_5: 0.6185 - precision_focused: 0.5077 - recall_5: 0.6224 - val_accuracy: 0.5040 - val_auc_5: 0.5171 - val_f05: 0.4555 - val_loss: 0.5963 - val_precision_5: 0.5398 - val_precision_focused: 0.4628 - val_recall_5: 0.3346 - learning_rate: 1.4300e-05\n",
      "Epoch 310/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6254 - auc_5: 0.6720 - f05: 0.5200 - loss: 0.5407 - precision_5: 0.6293 - precision_focused: 0.5209 - recall_5: 0.6493\n",
      "Epoch 310: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 310: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6253 - auc_5: 0.6720 - f05: 0.5200 - loss: 0.5407 - precision_5: 0.6293 - precision_focused: 0.5209 - recall_5: 0.6491 - val_accuracy: 0.5027 - val_auc_5: 0.5170 - val_f05: 0.4531 - val_loss: 0.5972 - val_precision_5: 0.5384 - val_precision_focused: 0.4611 - val_recall_5: 0.3290 - learning_rate: 1.4300e-05\n",
      "Epoch 311/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6205 - auc_5: 0.6659 - f05: 0.5106 - loss: 0.5437 - precision_5: 0.6245 - precision_focused: 0.5109 - recall_5: 0.6313\n",
      "Epoch 311: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 311: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6205 - auc_5: 0.6659 - f05: 0.5106 - loss: 0.5437 - precision_5: 0.6245 - precision_focused: 0.5109 - recall_5: 0.6313 - val_accuracy: 0.5053 - val_auc_5: 0.5174 - val_f05: 0.4602 - val_loss: 0.5962 - val_precision_5: 0.5405 - val_precision_focused: 0.4659 - val_recall_5: 0.3463 - learning_rate: 1.4300e-05\n",
      "Epoch 312/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6214 - auc_5: 0.6699 - f05: 0.5193 - loss: 0.5411 - precision_5: 0.6268 - precision_focused: 0.5202 - recall_5: 0.6455\n",
      "Epoch 312: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 312: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6214 - auc_5: 0.6699 - f05: 0.5193 - loss: 0.5411 - precision_5: 0.6268 - precision_focused: 0.5202 - recall_5: 0.6454 - val_accuracy: 0.5037 - val_auc_5: 0.5178 - val_f05: 0.4517 - val_loss: 0.5973 - val_precision_5: 0.5403 - val_precision_focused: 0.4603 - val_recall_5: 0.3265 - learning_rate: 1.4300e-05\n",
      "Epoch 313/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6130 - auc_5: 0.6579 - f05: 0.5113 - loss: 0.5460 - precision_5: 0.6191 - precision_focused: 0.5112 - recall_5: 0.6161\n",
      "Epoch 313: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 313: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6130 - auc_5: 0.6579 - f05: 0.5113 - loss: 0.5460 - precision_5: 0.6191 - precision_focused: 0.5112 - recall_5: 0.6161 - val_accuracy: 0.5037 - val_auc_5: 0.5175 - val_f05: 0.4477 - val_loss: 0.5980 - val_precision_5: 0.5417 - val_precision_focused: 0.4582 - val_recall_5: 0.3167 - learning_rate: 1.4300e-05\n",
      "Epoch 314/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6315 - auc_5: 0.6735 - f05: 0.5085 - loss: 0.5419 - precision_5: 0.6344 - precision_focused: 0.5084 - recall_5: 0.6292\n",
      "Epoch 314: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 314: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6314 - auc_5: 0.6734 - f05: 0.5085 - loss: 0.5419 - precision_5: 0.6344 - precision_focused: 0.5084 - recall_5: 0.6291 - val_accuracy: 0.5085 - val_auc_5: 0.5176 - val_f05: 0.4589 - val_loss: 0.5970 - val_precision_5: 0.5460 - val_precision_focused: 0.4653 - val_recall_5: 0.3444 - learning_rate: 1.4300e-05\n",
      "Epoch 315/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6237 - auc_5: 0.6721 - f05: 0.5192 - loss: 0.5405 - precision_5: 0.6259 - precision_focused: 0.5210 - recall_5: 0.6637\n",
      "Epoch 315: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 315: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6237 - auc_5: 0.6720 - f05: 0.5191 - loss: 0.5405 - precision_5: 0.6258 - precision_focused: 0.5210 - recall_5: 0.6636 - val_accuracy: 0.5079 - val_auc_5: 0.5172 - val_f05: 0.4554 - val_loss: 0.5977 - val_precision_5: 0.5462 - val_precision_focused: 0.4631 - val_recall_5: 0.3358 - learning_rate: 1.4300e-05\n",
      "Epoch 316/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6071 - auc_5: 0.6591 - f05: 0.5059 - loss: 0.5462 - precision_5: 0.6084 - precision_focused: 0.5060 - recall_5: 0.6085\n",
      "Epoch 316: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 316: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6071 - auc_5: 0.6591 - f05: 0.5059 - loss: 0.5462 - precision_5: 0.6084 - precision_focused: 0.5060 - recall_5: 0.6086 - val_accuracy: 0.5092 - val_auc_5: 0.5177 - val_f05: 0.4553 - val_loss: 0.5971 - val_precision_5: 0.5485 - val_precision_focused: 0.4633 - val_recall_5: 0.3352 - learning_rate: 1.4300e-05\n",
      "Epoch 317/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6130 - auc_5: 0.6622 - f05: 0.5221 - loss: 0.5434 - precision_5: 0.6241 - precision_focused: 0.5222 - recall_5: 0.6252\n",
      "Epoch 317: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 317: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6130 - auc_5: 0.6622 - f05: 0.5221 - loss: 0.5434 - precision_5: 0.6241 - precision_focused: 0.5222 - recall_5: 0.6252 - val_accuracy: 0.5095 - val_auc_5: 0.5169 - val_f05: 0.4541 - val_loss: 0.5981 - val_precision_5: 0.5492 - val_precision_focused: 0.4624 - val_recall_5: 0.3340 - learning_rate: 1.4300e-05\n",
      "Epoch 318/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6125 - auc_5: 0.6588 - f05: 0.5075 - loss: 0.5470 - precision_5: 0.6145 - precision_focused: 0.5078 - recall_5: 0.6183\n",
      "Epoch 318: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 318: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6125 - auc_5: 0.6588 - f05: 0.5075 - loss: 0.5470 - precision_5: 0.6145 - precision_focused: 0.5078 - recall_5: 0.6183 - val_accuracy: 0.5089 - val_auc_5: 0.5168 - val_f05: 0.4639 - val_loss: 0.5968 - val_precision_5: 0.5449 - val_precision_focused: 0.4686 - val_recall_5: 0.3562 - learning_rate: 1.4300e-05\n",
      "Epoch 319/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6256 - auc_5: 0.6714 - f05: 0.5103 - loss: 0.5414 - precision_5: 0.6255 - precision_focused: 0.5110 - recall_5: 0.6404\n",
      "Epoch 319: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 319: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6256 - auc_5: 0.6714 - f05: 0.5103 - loss: 0.5414 - precision_5: 0.6255 - precision_focused: 0.5110 - recall_5: 0.6404 - val_accuracy: 0.5060 - val_auc_5: 0.5166 - val_f05: 0.4621 - val_loss: 0.5980 - val_precision_5: 0.5407 - val_precision_focused: 0.4670 - val_recall_5: 0.3525 - learning_rate: 1.4300e-05\n",
      "Epoch 320/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6243 - auc_5: 0.6679 - f05: 0.5236 - loss: 0.5407 - precision_5: 0.6339 - precision_focused: 0.5242 - recall_5: 0.6459\n",
      "Epoch 320: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 320: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6243 - auc_5: 0.6678 - f05: 0.5236 - loss: 0.5407 - precision_5: 0.6338 - precision_focused: 0.5241 - recall_5: 0.6458 - val_accuracy: 0.5076 - val_auc_5: 0.5169 - val_f05: 0.4646 - val_loss: 0.5969 - val_precision_5: 0.5426 - val_precision_focused: 0.4689 - val_recall_5: 0.3580 - learning_rate: 1.4300e-05\n",
      "Epoch 321/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6176 - auc_5: 0.6702 - f05: 0.5001 - loss: 0.5437 - precision_5: 0.6140 - precision_focused: 0.5003 - recall_5: 0.6159\n",
      "Epoch 321: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 321: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6176 - auc_5: 0.6702 - f05: 0.5001 - loss: 0.5437 - precision_5: 0.6140 - precision_focused: 0.5003 - recall_5: 0.6159 - val_accuracy: 0.5076 - val_auc_5: 0.5166 - val_f05: 0.4727 - val_loss: 0.5962 - val_precision_5: 0.5404 - val_precision_focused: 0.4750 - val_recall_5: 0.3753 - learning_rate: 1.4300e-05\n",
      "Epoch 322/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6198 - auc_5: 0.6694 - f05: 0.5164 - loss: 0.5418 - precision_5: 0.6224 - precision_focused: 0.5178 - recall_5: 0.6494\n",
      "Epoch 322: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 322: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6198 - auc_5: 0.6694 - f05: 0.5164 - loss: 0.5418 - precision_5: 0.6224 - precision_focused: 0.5177 - recall_5: 0.6494 - val_accuracy: 0.5089 - val_auc_5: 0.5165 - val_f05: 0.4616 - val_loss: 0.5976 - val_precision_5: 0.5455 - val_precision_focused: 0.4670 - val_recall_5: 0.3512 - learning_rate: 1.4300e-05\n",
      "Epoch 323/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6187 - auc_5: 0.6646 - f05: 0.5080 - loss: 0.5444 - precision_5: 0.6213 - precision_focused: 0.5082 - recall_5: 0.6255\n",
      "Epoch 323: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 323: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6187 - auc_5: 0.6646 - f05: 0.5080 - loss: 0.5444 - precision_5: 0.6213 - precision_focused: 0.5082 - recall_5: 0.6255 - val_accuracy: 0.5098 - val_auc_5: 0.5168 - val_f05: 0.4605 - val_loss: 0.5973 - val_precision_5: 0.5477 - val_precision_focused: 0.4666 - val_recall_5: 0.3475 - learning_rate: 1.4300e-05\n",
      "Epoch 324/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6145 - auc_5: 0.6654 - f05: 0.5118 - loss: 0.5436 - precision_5: 0.6169 - precision_focused: 0.5125 - recall_5: 0.6305\n",
      "Epoch 324: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 324: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6145 - auc_5: 0.6654 - f05: 0.5118 - loss: 0.5436 - precision_5: 0.6169 - precision_focused: 0.5125 - recall_5: 0.6304 - val_accuracy: 0.5060 - val_auc_5: 0.5172 - val_f05: 0.4578 - val_loss: 0.5974 - val_precision_5: 0.5426 - val_precision_focused: 0.4648 - val_recall_5: 0.3383 - learning_rate: 1.4300e-05\n",
      "Epoch 325/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6134 - auc_5: 0.6638 - f05: 0.5225 - loss: 0.5421 - precision_5: 0.6253 - precision_focused: 0.5229 - recall_5: 0.6339\n",
      "Epoch 325: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 325: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6134 - auc_5: 0.6638 - f05: 0.5225 - loss: 0.5421 - precision_5: 0.6253 - precision_focused: 0.5229 - recall_5: 0.6339 - val_accuracy: 0.5063 - val_auc_5: 0.5169 - val_f05: 0.4524 - val_loss: 0.5996 - val_precision_5: 0.5451 - val_precision_focused: 0.4618 - val_recall_5: 0.3247 - learning_rate: 1.4300e-05\n",
      "Epoch 326/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6184 - auc_5: 0.6616 - f05: 0.5122 - loss: 0.5462 - precision_5: 0.6247 - precision_focused: 0.5122 - recall_5: 0.6239\n",
      "Epoch 326: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 326: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6184 - auc_5: 0.6616 - f05: 0.5122 - loss: 0.5461 - precision_5: 0.6247 - precision_focused: 0.5122 - recall_5: 0.6238 - val_accuracy: 0.5066 - val_auc_5: 0.5170 - val_f05: 0.4554 - val_loss: 0.5984 - val_precision_5: 0.5441 - val_precision_focused: 0.4630 - val_recall_5: 0.3352 - learning_rate: 1.4300e-05\n",
      "Epoch 327/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6194 - auc_5: 0.6649 - f05: 0.5196 - loss: 0.5410 - precision_5: 0.6288 - precision_focused: 0.5202 - recall_5: 0.6420\n",
      "Epoch 327: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 327: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.6194 - auc_5: 0.6649 - f05: 0.5196 - loss: 0.5410 - precision_5: 0.6288 - precision_focused: 0.5202 - recall_5: 0.6420 - val_accuracy: 0.5072 - val_auc_5: 0.5168 - val_f05: 0.4571 - val_loss: 0.5986 - val_precision_5: 0.5445 - val_precision_focused: 0.4640 - val_recall_5: 0.3401 - learning_rate: 1.4300e-05\n",
      "Epoch 328/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6232 - auc_5: 0.6711 - f05: 0.5108 - loss: 0.5411 - precision_5: 0.6285 - precision_focused: 0.5108 - recall_5: 0.6279\n",
      "Epoch 328: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 328: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6231 - auc_5: 0.6710 - f05: 0.5108 - loss: 0.5412 - precision_5: 0.6284 - precision_focused: 0.5108 - recall_5: 0.6279 - val_accuracy: 0.5098 - val_auc_5: 0.5169 - val_f05: 0.4675 - val_loss: 0.5974 - val_precision_5: 0.5452 - val_precision_focused: 0.4713 - val_recall_5: 0.3648 - learning_rate: 1.4300e-05\n",
      "Epoch 329/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6182 - auc_5: 0.6655 - f05: 0.5086 - loss: 0.5448 - precision_5: 0.6197 - precision_focused: 0.5091 - recall_5: 0.6303\n",
      "Epoch 329: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 329: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6182 - auc_5: 0.6655 - f05: 0.5086 - loss: 0.5448 - precision_5: 0.6197 - precision_focused: 0.5091 - recall_5: 0.6303 - val_accuracy: 0.5079 - val_auc_5: 0.5164 - val_f05: 0.4635 - val_loss: 0.5985 - val_precision_5: 0.5435 - val_precision_focused: 0.4682 - val_recall_5: 0.3549 - learning_rate: 1.4300e-05\n",
      "Epoch 330/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6146 - auc_5: 0.6586 - f05: 0.5196 - loss: 0.5468 - precision_5: 0.6187 - precision_focused: 0.5208 - recall_5: 0.6435\n",
      "Epoch 330: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 330: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6147 - auc_5: 0.6586 - f05: 0.5196 - loss: 0.5468 - precision_5: 0.6187 - precision_focused: 0.5208 - recall_5: 0.6435 - val_accuracy: 0.5069 - val_auc_5: 0.5169 - val_f05: 0.4593 - val_loss: 0.5981 - val_precision_5: 0.5433 - val_precision_focused: 0.4655 - val_recall_5: 0.3444 - learning_rate: 1.4300e-05\n",
      "Epoch 331/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6266 - auc_5: 0.6747 - f05: 0.5114 - loss: 0.5397 - precision_5: 0.6293 - precision_focused: 0.5119 - recall_5: 0.6392\n",
      "Epoch 331: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 331: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6265 - auc_5: 0.6747 - f05: 0.5114 - loss: 0.5397 - precision_5: 0.6293 - precision_focused: 0.5119 - recall_5: 0.6392 - val_accuracy: 0.5066 - val_auc_5: 0.5166 - val_f05: 0.4602 - val_loss: 0.5983 - val_precision_5: 0.5422 - val_precision_focused: 0.4657 - val_recall_5: 0.3488 - learning_rate: 1.4300e-05\n",
      "Epoch 332/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6230 - auc_5: 0.6729 - f05: 0.5086 - loss: 0.5411 - precision_5: 0.6269 - precision_focused: 0.5087 - recall_5: 0.6282\n",
      "Epoch 332: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 332: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6229 - auc_5: 0.6729 - f05: 0.5086 - loss: 0.5411 - precision_5: 0.6269 - precision_focused: 0.5087 - recall_5: 0.6282 - val_accuracy: 0.5076 - val_auc_5: 0.5169 - val_f05: 0.4590 - val_loss: 0.5986 - val_precision_5: 0.5445 - val_precision_focused: 0.4653 - val_recall_5: 0.3438 - learning_rate: 1.4300e-05\n",
      "Epoch 333/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6303 - auc_5: 0.6753 - f05: 0.5201 - loss: 0.5392 - precision_5: 0.6364 - precision_focused: 0.5209 - recall_5: 0.6530\n",
      "Epoch 333: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 333: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6303 - auc_5: 0.6753 - f05: 0.5201 - loss: 0.5392 - precision_5: 0.6364 - precision_focused: 0.5208 - recall_5: 0.6530 - val_accuracy: 0.5034 - val_auc_5: 0.5174 - val_f05: 0.4561 - val_loss: 0.5993 - val_precision_5: 0.5389 - val_precision_focused: 0.4635 - val_recall_5: 0.3333 - learning_rate: 1.4300e-05\n",
      "Epoch 334/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6185 - auc_5: 0.6663 - f05: 0.5181 - loss: 0.5416 - precision_5: 0.6259 - precision_focused: 0.5187 - recall_5: 0.6391\n",
      "Epoch 334: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 334: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6185 - auc_5: 0.6662 - f05: 0.5180 - loss: 0.5417 - precision_5: 0.6259 - precision_focused: 0.5186 - recall_5: 0.6391 - val_accuracy: 0.5043 - val_auc_5: 0.5178 - val_f05: 0.4534 - val_loss: 0.5985 - val_precision_5: 0.5414 - val_precision_focused: 0.4620 - val_recall_5: 0.3272 - learning_rate: 1.4300e-05\n",
      "Epoch 335/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6132 - auc_5: 0.6651 - f05: 0.5181 - loss: 0.5423 - precision_5: 0.6191 - precision_focused: 0.5188 - recall_5: 0.6340\n",
      "Epoch 335: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 335: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6132 - auc_5: 0.6651 - f05: 0.5181 - loss: 0.5423 - precision_5: 0.6191 - precision_focused: 0.5188 - recall_5: 0.6340 - val_accuracy: 0.5040 - val_auc_5: 0.5173 - val_f05: 0.4612 - val_loss: 0.5972 - val_precision_5: 0.5383 - val_precision_focused: 0.4666 - val_recall_5: 0.3469 - learning_rate: 1.4300e-05\n",
      "Epoch 336/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6228 - auc_5: 0.6715 - f05: 0.5111 - loss: 0.5418 - precision_5: 0.6300 - precision_focused: 0.5109 - recall_5: 0.6255\n",
      "Epoch 336: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 336: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6227 - auc_5: 0.6714 - f05: 0.5111 - loss: 0.5419 - precision_5: 0.6300 - precision_focused: 0.5109 - recall_5: 0.6255 - val_accuracy: 0.5024 - val_auc_5: 0.5169 - val_f05: 0.4575 - val_loss: 0.5980 - val_precision_5: 0.5372 - val_precision_focused: 0.4645 - val_recall_5: 0.3346 - learning_rate: 1.4300e-05\n",
      "Epoch 337/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6169 - auc_5: 0.6635 - f05: 0.5104 - loss: 0.5438 - precision_5: 0.6221 - precision_focused: 0.5105 - recall_5: 0.6253\n",
      "Epoch 337: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 337: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6169 - auc_5: 0.6635 - f05: 0.5104 - loss: 0.5438 - precision_5: 0.6221 - precision_focused: 0.5105 - recall_5: 0.6253 - val_accuracy: 0.5014 - val_auc_5: 0.5171 - val_f05: 0.4562 - val_loss: 0.5975 - val_precision_5: 0.5361 - val_precision_focused: 0.4640 - val_recall_5: 0.3296 - learning_rate: 1.4300e-05\n",
      "Epoch 338/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6191 - auc_5: 0.6701 - f05: 0.5066 - loss: 0.5412 - precision_5: 0.6228 - precision_focused: 0.5063 - recall_5: 0.6141\n",
      "Epoch 338: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 338: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6191 - auc_5: 0.6701 - f05: 0.5066 - loss: 0.5412 - precision_5: 0.6228 - precision_focused: 0.5063 - recall_5: 0.6141 - val_accuracy: 0.5034 - val_auc_5: 0.5174 - val_f05: 0.4618 - val_loss: 0.5976 - val_precision_5: 0.5378 - val_precision_focused: 0.4678 - val_recall_5: 0.3426 - learning_rate: 1.4300e-05\n",
      "Epoch 339/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6190 - auc_5: 0.6631 - f05: 0.5140 - loss: 0.5445 - precision_5: 0.6253 - precision_focused: 0.5144 - recall_5: 0.6327\n",
      "Epoch 339: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 339: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6190 - auc_5: 0.6631 - f05: 0.5140 - loss: 0.5445 - precision_5: 0.6253 - precision_focused: 0.5143 - recall_5: 0.6326 - val_accuracy: 0.5050 - val_auc_5: 0.5173 - val_f05: 0.4651 - val_loss: 0.5967 - val_precision_5: 0.5391 - val_precision_focused: 0.4698 - val_recall_5: 0.3531 - learning_rate: 1.4300e-05\n",
      "Epoch 340/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6183 - auc_5: 0.6673 - f05: 0.5097 - loss: 0.5437 - precision_5: 0.6185 - precision_focused: 0.5103 - recall_5: 0.6307\n",
      "Epoch 340: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 340: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6184 - auc_5: 0.6673 - f05: 0.5097 - loss: 0.5437 - precision_5: 0.6186 - precision_focused: 0.5103 - recall_5: 0.6308 - val_accuracy: 0.5014 - val_auc_5: 0.5168 - val_f05: 0.4565 - val_loss: 0.5986 - val_precision_5: 0.5358 - val_precision_focused: 0.4637 - val_recall_5: 0.3327 - learning_rate: 1.4300e-05\n",
      "Epoch 341/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6263 - auc_5: 0.6709 - f05: 0.5185 - loss: 0.5400 - precision_5: 0.6356 - precision_focused: 0.5187 - recall_5: 0.6403\n",
      "Epoch 341: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 341: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6263 - auc_5: 0.6708 - f05: 0.5184 - loss: 0.5400 - precision_5: 0.6355 - precision_focused: 0.5186 - recall_5: 0.6401 - val_accuracy: 0.5005 - val_auc_5: 0.5174 - val_f05: 0.4454 - val_loss: 0.5998 - val_precision_5: 0.5375 - val_precision_focused: 0.4576 - val_recall_5: 0.3056 - learning_rate: 1.4300e-05\n",
      "Epoch 342/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6087 - auc_5: 0.6559 - f05: 0.5156 - loss: 0.5462 - precision_5: 0.6212 - precision_focused: 0.5153 - recall_5: 0.6125\n",
      "Epoch 342: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 342: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6089 - auc_5: 0.6560 - f05: 0.5156 - loss: 0.5462 - precision_5: 0.6213 - precision_focused: 0.5153 - recall_5: 0.6127 - val_accuracy: 0.4998 - val_auc_5: 0.5170 - val_f05: 0.4443 - val_loss: 0.5996 - val_precision_5: 0.5365 - val_precision_focused: 0.4569 - val_recall_5: 0.3037 - learning_rate: 1.4300e-05\n",
      "Epoch 343/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6202 - auc_5: 0.6674 - f05: 0.5149 - loss: 0.5409 - precision_5: 0.6338 - precision_focused: 0.5142 - recall_5: 0.6160\n",
      "Epoch 343: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 343: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6202 - auc_5: 0.6674 - f05: 0.5148 - loss: 0.5409 - precision_5: 0.6338 - precision_focused: 0.5142 - recall_5: 0.6160 - val_accuracy: 0.5014 - val_auc_5: 0.5165 - val_f05: 0.4499 - val_loss: 0.5992 - val_precision_5: 0.5373 - val_precision_focused: 0.4594 - val_recall_5: 0.3198 - learning_rate: 1.4300e-05\n",
      "Epoch 344/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6310 - auc_5: 0.6772 - f05: 0.5075 - loss: 0.5398 - precision_5: 0.6339 - precision_focused: 0.5074 - recall_5: 0.6328\n",
      "Epoch 344: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 344: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6309 - auc_5: 0.6772 - f05: 0.5075 - loss: 0.5399 - precision_5: 0.6339 - precision_focused: 0.5074 - recall_5: 0.6328 - val_accuracy: 0.5018 - val_auc_5: 0.5170 - val_f05: 0.4525 - val_loss: 0.5983 - val_precision_5: 0.5374 - val_precision_focused: 0.4614 - val_recall_5: 0.3235 - learning_rate: 1.4300e-05\n",
      "Epoch 345/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6264 - auc_5: 0.6655 - f05: 0.5177 - loss: 0.5434 - precision_5: 0.6319 - precision_focused: 0.5182 - recall_5: 0.6447\n",
      "Epoch 345: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 345: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6264 - auc_5: 0.6655 - f05: 0.5176 - loss: 0.5434 - precision_5: 0.6319 - precision_focused: 0.5182 - recall_5: 0.6447 - val_accuracy: 0.5002 - val_auc_5: 0.5167 - val_f05: 0.4589 - val_loss: 0.5980 - val_precision_5: 0.5331 - val_precision_focused: 0.4650 - val_recall_5: 0.3383 - learning_rate: 1.4300e-05\n",
      "Epoch 346/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6105 - auc_5: 0.6624 - f05: 0.5105 - loss: 0.5439 - precision_5: 0.6183 - precision_focused: 0.5103 - recall_5: 0.6138\n",
      "Epoch 346: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 346: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6105 - auc_5: 0.6624 - f05: 0.5105 - loss: 0.5439 - precision_5: 0.6183 - precision_focused: 0.5103 - recall_5: 0.6139 - val_accuracy: 0.5027 - val_auc_5: 0.5169 - val_f05: 0.4565 - val_loss: 0.5980 - val_precision_5: 0.5376 - val_precision_focused: 0.4634 - val_recall_5: 0.3352 - learning_rate: 1.4300e-05\n",
      "Epoch 347/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6279 - auc_5: 0.6689 - f05: 0.5154 - loss: 0.5408 - precision_5: 0.6362 - precision_focused: 0.5155 - recall_5: 0.6394\n",
      "Epoch 347: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 347: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6279 - auc_5: 0.6689 - f05: 0.5154 - loss: 0.5408 - precision_5: 0.6361 - precision_focused: 0.5155 - recall_5: 0.6394 - val_accuracy: 0.5014 - val_auc_5: 0.5168 - val_f05: 0.4495 - val_loss: 0.6001 - val_precision_5: 0.5376 - val_precision_focused: 0.4594 - val_recall_5: 0.3179 - learning_rate: 1.4300e-05\n",
      "Epoch 348/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6124 - auc_5: 0.6655 - f05: 0.5122 - loss: 0.5428 - precision_5: 0.6182 - precision_focused: 0.5124 - recall_5: 0.6203\n",
      "Epoch 348: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 348: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6124 - auc_5: 0.6655 - f05: 0.5122 - loss: 0.5428 - precision_5: 0.6182 - precision_focused: 0.5124 - recall_5: 0.6203 - val_accuracy: 0.5014 - val_auc_5: 0.5166 - val_f05: 0.4606 - val_loss: 0.5991 - val_precision_5: 0.5347 - val_precision_focused: 0.4662 - val_recall_5: 0.3426 - learning_rate: 1.4300e-05\n",
      "Epoch 349/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6201 - auc_5: 0.6638 - f05: 0.5117 - loss: 0.5446 - precision_5: 0.6208 - precision_focused: 0.5126 - recall_5: 0.6397\n",
      "Epoch 349: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 349: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6201 - auc_5: 0.6638 - f05: 0.5117 - loss: 0.5446 - precision_5: 0.6208 - precision_focused: 0.5126 - recall_5: 0.6398 - val_accuracy: 0.5024 - val_auc_5: 0.5165 - val_f05: 0.4524 - val_loss: 0.6006 - val_precision_5: 0.5383 - val_precision_focused: 0.4611 - val_recall_5: 0.3253 - learning_rate: 1.4300e-05\n",
      "Epoch 350/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6237 - auc_5: 0.6719 - f05: 0.5204 - loss: 0.5386 - precision_5: 0.6372 - precision_focused: 0.5203 - recall_5: 0.6334\n",
      "Epoch 350: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 350: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6237 - auc_5: 0.6719 - f05: 0.5204 - loss: 0.5386 - precision_5: 0.6372 - precision_focused: 0.5203 - recall_5: 0.6334 - val_accuracy: 0.5027 - val_auc_5: 0.5159 - val_f05: 0.4497 - val_loss: 0.6005 - val_precision_5: 0.5397 - val_precision_focused: 0.4596 - val_recall_5: 0.3191 - learning_rate: 1.4300e-05\n",
      "Epoch 351/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6195 - auc_5: 0.6695 - f05: 0.5087 - loss: 0.5422 - precision_5: 0.6242 - precision_focused: 0.5086 - recall_5: 0.6213\n",
      "Epoch 351: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 351: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6195 - auc_5: 0.6694 - f05: 0.5087 - loss: 0.5422 - precision_5: 0.6241 - precision_focused: 0.5087 - recall_5: 0.6212 - val_accuracy: 0.4989 - val_auc_5: 0.5166 - val_f05: 0.4446 - val_loss: 0.6003 - val_precision_5: 0.5352 - val_precision_focused: 0.4577 - val_recall_5: 0.3006 - learning_rate: 1.4300e-05\n",
      "Epoch 352/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6257 - auc_5: 0.6711 - f05: 0.5157 - loss: 0.5396 - precision_5: 0.6349 - precision_focused: 0.5159 - recall_5: 0.6371\n",
      "Epoch 352: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 352: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6257 - auc_5: 0.6711 - f05: 0.5157 - loss: 0.5396 - precision_5: 0.6349 - precision_focused: 0.5158 - recall_5: 0.6371 - val_accuracy: 0.5005 - val_auc_5: 0.5159 - val_f05: 0.4456 - val_loss: 0.6007 - val_precision_5: 0.5375 - val_precision_focused: 0.4579 - val_recall_5: 0.3056 - learning_rate: 1.4300e-05\n",
      "Epoch 353/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6228 - auc_5: 0.6694 - f05: 0.5166 - loss: 0.5418 - precision_5: 0.6307 - precision_focused: 0.5168 - recall_5: 0.6344\n",
      "Epoch 353: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 353: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6228 - auc_5: 0.6694 - f05: 0.5166 - loss: 0.5419 - precision_5: 0.6306 - precision_focused: 0.5168 - recall_5: 0.6343 - val_accuracy: 0.5031 - val_auc_5: 0.5157 - val_f05: 0.4584 - val_loss: 0.5991 - val_precision_5: 0.5382 - val_precision_focused: 0.4654 - val_recall_5: 0.3352 - learning_rate: 1.4300e-05\n",
      "Epoch 354/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - auc_5: 0.6768 - f05: 0.5174 - loss: 0.5376 - precision_5: 0.6375 - precision_focused: 0.5178 - recall_5: 0.6461\n",
      "Epoch 354: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 354: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6301 - auc_5: 0.6768 - f05: 0.5173 - loss: 0.5376 - precision_5: 0.6374 - precision_focused: 0.5177 - recall_5: 0.6461 - val_accuracy: 0.5021 - val_auc_5: 0.5166 - val_f05: 0.4537 - val_loss: 0.5996 - val_precision_5: 0.5377 - val_precision_focused: 0.4622 - val_recall_5: 0.3259 - learning_rate: 1.4300e-05\n",
      "Epoch 355/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6217 - auc_5: 0.6743 - f05: 0.5096 - loss: 0.5406 - precision_5: 0.6245 - precision_focused: 0.5098 - recall_5: 0.6271\n",
      "Epoch 355: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 355: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6217 - auc_5: 0.6744 - f05: 0.5096 - loss: 0.5406 - precision_5: 0.6246 - precision_focused: 0.5098 - recall_5: 0.6271 - val_accuracy: 0.5072 - val_auc_5: 0.5160 - val_f05: 0.4641 - val_loss: 0.5992 - val_precision_5: 0.5426 - val_precision_focused: 0.4690 - val_recall_5: 0.3537 - learning_rate: 1.4300e-05\n",
      "Epoch 356/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6186 - auc_5: 0.6618 - f05: 0.5153 - loss: 0.5451 - precision_5: 0.6234 - precision_focused: 0.5159 - recall_5: 0.6357\n",
      "Epoch 356: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 356: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6186 - auc_5: 0.6619 - f05: 0.5152 - loss: 0.5450 - precision_5: 0.6235 - precision_focused: 0.5158 - recall_5: 0.6357 - val_accuracy: 0.5043 - val_auc_5: 0.5160 - val_f05: 0.4506 - val_loss: 0.6006 - val_precision_5: 0.5420 - val_precision_focused: 0.4601 - val_recall_5: 0.3228 - learning_rate: 1.4300e-05\n",
      "Epoch 357/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6274 - auc_5: 0.6708 - f05: 0.5113 - loss: 0.5420 - precision_5: 0.6337 - precision_focused: 0.5115 - recall_5: 0.6360\n",
      "Epoch 357: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 357: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 357: ReduceLROnPlateau reducing learning rate to 1.1439964873716235e-05.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6274 - auc_5: 0.6708 - f05: 0.5113 - loss: 0.5420 - precision_5: 0.6336 - precision_focused: 0.5115 - recall_5: 0.6360 - val_accuracy: 0.5043 - val_auc_5: 0.5162 - val_f05: 0.4552 - val_loss: 0.5997 - val_precision_5: 0.5410 - val_precision_focused: 0.4634 - val_recall_5: 0.3296 - learning_rate: 1.4300e-05\n",
      "Epoch 358/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6238 - auc_5: 0.6688 - f05: 0.5092 - loss: 0.5449 - precision_5: 0.6251 - precision_focused: 0.5099 - recall_5: 0.6391\n",
      "Epoch 358: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 358: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6238 - auc_5: 0.6688 - f05: 0.5092 - loss: 0.5449 - precision_5: 0.6251 - precision_focused: 0.5099 - recall_5: 0.6390 - val_accuracy: 0.5037 - val_auc_5: 0.5163 - val_f05: 0.4562 - val_loss: 0.5999 - val_precision_5: 0.5395 - val_precision_focused: 0.4638 - val_recall_5: 0.3327 - learning_rate: 1.1440e-05\n",
      "Epoch 359/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6212 - auc_5: 0.6751 - f05: 0.5152 - loss: 0.5381 - precision_5: 0.6283 - precision_focused: 0.5155 - recall_5: 0.6342\n",
      "Epoch 359: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 359: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6212 - auc_5: 0.6750 - f05: 0.5151 - loss: 0.5382 - precision_5: 0.6283 - precision_focused: 0.5155 - recall_5: 0.6342 - val_accuracy: 0.5034 - val_auc_5: 0.5162 - val_f05: 0.4533 - val_loss: 0.5997 - val_precision_5: 0.5398 - val_precision_focused: 0.4619 - val_recall_5: 0.3265 - learning_rate: 1.1440e-05\n",
      "Epoch 360/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6355 - auc_5: 0.6824 - f05: 0.5094 - loss: 0.5375 - precision_5: 0.6374 - precision_focused: 0.5098 - recall_5: 0.6459\n",
      "Epoch 360: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 360: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6354 - auc_5: 0.6823 - f05: 0.5095 - loss: 0.5375 - precision_5: 0.6373 - precision_focused: 0.5099 - recall_5: 0.6458 - val_accuracy: 0.5050 - val_auc_5: 0.5165 - val_f05: 0.4505 - val_loss: 0.6004 - val_precision_5: 0.5431 - val_precision_focused: 0.4600 - val_recall_5: 0.3228 - learning_rate: 1.1440e-05\n",
      "Epoch 361/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6157 - auc_5: 0.6566 - f05: 0.5116 - loss: 0.5468 - precision_5: 0.6230 - precision_focused: 0.5116 - recall_5: 0.6226\n",
      "Epoch 361: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 361: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6157 - auc_5: 0.6566 - f05: 0.5115 - loss: 0.5468 - precision_5: 0.6231 - precision_focused: 0.5115 - recall_5: 0.6226 - val_accuracy: 0.5040 - val_auc_5: 0.5165 - val_f05: 0.4497 - val_loss: 0.6003 - val_precision_5: 0.5420 - val_precision_focused: 0.4599 - val_recall_5: 0.3185 - learning_rate: 1.1440e-05\n",
      "Epoch 362/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6260 - auc_5: 0.6725 - f05: 0.5122 - loss: 0.5399 - precision_5: 0.6289 - precision_focused: 0.5128 - recall_5: 0.6413\n",
      "Epoch 362: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 362: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6260 - auc_5: 0.6725 - f05: 0.5122 - loss: 0.5399 - precision_5: 0.6289 - precision_focused: 0.5128 - recall_5: 0.6412 - val_accuracy: 0.5060 - val_auc_5: 0.5165 - val_f05: 0.4552 - val_loss: 0.5997 - val_precision_5: 0.5431 - val_precision_focused: 0.4628 - val_recall_5: 0.3346 - learning_rate: 1.1440e-05\n",
      "Epoch 363/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6221 - auc_5: 0.6730 - f05: 0.5220 - loss: 0.5379 - precision_5: 0.6318 - precision_focused: 0.5225 - recall_5: 0.6440\n",
      "Epoch 363: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 363: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6220 - auc_5: 0.6729 - f05: 0.5219 - loss: 0.5379 - precision_5: 0.6317 - precision_focused: 0.5225 - recall_5: 0.6439 - val_accuracy: 0.5056 - val_auc_5: 0.5167 - val_f05: 0.4508 - val_loss: 0.5997 - val_precision_5: 0.5440 - val_precision_focused: 0.4603 - val_recall_5: 0.3241 - learning_rate: 1.1440e-05\n",
      "Epoch 364/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6289 - auc_5: 0.6812 - f05: 0.5159 - loss: 0.5369 - precision_5: 0.6323 - precision_focused: 0.5164 - recall_5: 0.6446\n",
      "Epoch 364: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 364: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6289 - auc_5: 0.6812 - f05: 0.5159 - loss: 0.5369 - precision_5: 0.6323 - precision_focused: 0.5164 - recall_5: 0.6446 - val_accuracy: 0.5021 - val_auc_5: 0.5169 - val_f05: 0.4367 - val_loss: 0.6016 - val_precision_5: 0.5426 - val_precision_focused: 0.4530 - val_recall_5: 0.2907 - learning_rate: 1.1440e-05\n",
      "Epoch 365/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6266 - auc_5: 0.6685 - f05: 0.5093 - loss: 0.5428 - precision_5: 0.6323 - precision_focused: 0.5090 - recall_5: 0.6245\n",
      "Epoch 365: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 365: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6266 - auc_5: 0.6685 - f05: 0.5093 - loss: 0.5428 - precision_5: 0.6323 - precision_focused: 0.5090 - recall_5: 0.6245 - val_accuracy: 0.5072 - val_auc_5: 0.5164 - val_f05: 0.4584 - val_loss: 0.5991 - val_precision_5: 0.5443 - val_precision_focused: 0.4651 - val_recall_5: 0.3414 - learning_rate: 1.1440e-05\n",
      "Epoch 366/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6197 - auc_5: 0.6660 - f05: 0.5100 - loss: 0.5436 - precision_5: 0.6192 - precision_focused: 0.5109 - recall_5: 0.6368\n",
      "Epoch 366: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 366: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6197 - auc_5: 0.6660 - f05: 0.5100 - loss: 0.5436 - precision_5: 0.6192 - precision_focused: 0.5109 - recall_5: 0.6368 - val_accuracy: 0.5079 - val_auc_5: 0.5156 - val_f05: 0.4644 - val_loss: 0.5985 - val_precision_5: 0.5436 - val_precision_focused: 0.4693 - val_recall_5: 0.3543 - learning_rate: 1.1440e-05\n",
      "Epoch 367/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6247 - auc_5: 0.6681 - f05: 0.5176 - loss: 0.5404 - precision_5: 0.6300 - precision_focused: 0.5183 - recall_5: 0.6453\n",
      "Epoch 367: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 367: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6247 - auc_5: 0.6681 - f05: 0.5176 - loss: 0.5404 - precision_5: 0.6299 - precision_focused: 0.5183 - recall_5: 0.6452 - val_accuracy: 0.5072 - val_auc_5: 0.5164 - val_f05: 0.4553 - val_loss: 0.6000 - val_precision_5: 0.5452 - val_precision_focused: 0.4630 - val_recall_5: 0.3352 - learning_rate: 1.1440e-05\n",
      "Epoch 368/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6284 - auc_5: 0.6773 - f05: 0.5129 - loss: 0.5397 - precision_5: 0.6321 - precision_focused: 0.5133 - recall_5: 0.6390\n",
      "Epoch 368: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 368: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6285 - auc_5: 0.6773 - f05: 0.5129 - loss: 0.5397 - precision_5: 0.6322 - precision_focused: 0.5133 - recall_5: 0.6391 - val_accuracy: 0.5069 - val_auc_5: 0.5162 - val_f05: 0.4669 - val_loss: 0.5993 - val_precision_5: 0.5412 - val_precision_focused: 0.4708 - val_recall_5: 0.3605 - learning_rate: 1.1440e-05\n",
      "Epoch 369/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6246 - auc_5: 0.6743 - f05: 0.5137 - loss: 0.5388 - precision_5: 0.6250 - precision_focused: 0.5148 - recall_5: 0.6491\n",
      "Epoch 369: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 369: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6246 - auc_5: 0.6744 - f05: 0.5137 - loss: 0.5388 - precision_5: 0.6250 - precision_focused: 0.5148 - recall_5: 0.6490 - val_accuracy: 0.5092 - val_auc_5: 0.5161 - val_f05: 0.4560 - val_loss: 0.6004 - val_precision_5: 0.5485 - val_precision_focused: 0.4641 - val_recall_5: 0.3352 - learning_rate: 1.1440e-05\n",
      "Epoch 370/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6265 - auc_5: 0.6727 - f05: 0.5127 - loss: 0.5394 - precision_5: 0.6319 - precision_focused: 0.5129 - recall_5: 0.6370\n",
      "Epoch 370: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 370: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6265 - auc_5: 0.6727 - f05: 0.5127 - loss: 0.5394 - precision_5: 0.6319 - precision_focused: 0.5129 - recall_5: 0.6370 - val_accuracy: 0.5072 - val_auc_5: 0.5169 - val_f05: 0.4532 - val_loss: 0.6005 - val_precision_5: 0.5461 - val_precision_focused: 0.4620 - val_recall_5: 0.3290 - learning_rate: 1.1440e-05\n",
      "Epoch 371/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6239 - auc_5: 0.6722 - f05: 0.5051 - loss: 0.5439 - precision_5: 0.6201 - precision_focused: 0.5060 - recall_5: 0.6364\n",
      "Epoch 371: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 371: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6239 - auc_5: 0.6722 - f05: 0.5051 - loss: 0.5439 - precision_5: 0.6201 - precision_focused: 0.5060 - recall_5: 0.6363 - val_accuracy: 0.5060 - val_auc_5: 0.5170 - val_f05: 0.4499 - val_loss: 0.6000 - val_precision_5: 0.5452 - val_precision_focused: 0.4601 - val_recall_5: 0.3204 - learning_rate: 1.1440e-05\n",
      "Epoch 372/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6274 - auc_5: 0.6709 - f05: 0.5120 - loss: 0.5423 - precision_5: 0.6318 - precision_focused: 0.5121 - recall_5: 0.6341\n",
      "Epoch 372: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 372: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6274 - auc_5: 0.6710 - f05: 0.5120 - loss: 0.5423 - precision_5: 0.6318 - precision_focused: 0.5121 - recall_5: 0.6341 - val_accuracy: 0.5060 - val_auc_5: 0.5167 - val_f05: 0.4530 - val_loss: 0.5999 - val_precision_5: 0.5442 - val_precision_focused: 0.4621 - val_recall_5: 0.3265 - learning_rate: 1.1440e-05\n",
      "Epoch 373/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6272 - auc_5: 0.6807 - f05: 0.5129 - loss: 0.5371 - precision_5: 0.6313 - precision_focused: 0.5135 - recall_5: 0.6409\n",
      "Epoch 373: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 373: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6272 - auc_5: 0.6807 - f05: 0.5129 - loss: 0.5371 - precision_5: 0.6313 - precision_focused: 0.5135 - recall_5: 0.6408 - val_accuracy: 0.5069 - val_auc_5: 0.5161 - val_f05: 0.4565 - val_loss: 0.6004 - val_precision_5: 0.5442 - val_precision_focused: 0.4636 - val_recall_5: 0.3383 - learning_rate: 1.1440e-05\n",
      "Epoch 374/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6348 - auc_5: 0.6870 - f05: 0.5145 - loss: 0.5327 - precision_5: 0.6467 - precision_focused: 0.5142 - recall_5: 0.6390\n",
      "Epoch 374: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 374: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6347 - auc_5: 0.6869 - f05: 0.5144 - loss: 0.5327 - precision_5: 0.6466 - precision_focused: 0.5142 - recall_5: 0.6389 - val_accuracy: 0.5079 - val_auc_5: 0.5161 - val_f05: 0.4583 - val_loss: 0.6001 - val_precision_5: 0.5454 - val_precision_focused: 0.4651 - val_recall_5: 0.3414 - learning_rate: 1.1440e-05\n",
      "Epoch 375/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6233 - auc_5: 0.6753 - f05: 0.5250 - loss: 0.5358 - precision_5: 0.6367 - precision_focused: 0.5252 - recall_5: 0.6399\n",
      "Epoch 375: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 375: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6233 - auc_5: 0.6753 - f05: 0.5250 - loss: 0.5358 - precision_5: 0.6366 - precision_focused: 0.5252 - recall_5: 0.6399 - val_accuracy: 0.5040 - val_auc_5: 0.5166 - val_f05: 0.4484 - val_loss: 0.6007 - val_precision_5: 0.5422 - val_precision_focused: 0.4588 - val_recall_5: 0.3173 - learning_rate: 1.1440e-05\n",
      "Epoch 376/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6048 - auc_5: 0.6572 - f05: 0.5132 - loss: 0.5456 - precision_5: 0.6139 - precision_focused: 0.5131 - recall_5: 0.6101\n",
      "Epoch 376: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 376: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6049 - auc_5: 0.6572 - f05: 0.5132 - loss: 0.5456 - precision_5: 0.6139 - precision_focused: 0.5131 - recall_5: 0.6101 - val_accuracy: 0.5031 - val_auc_5: 0.5159 - val_f05: 0.4541 - val_loss: 0.6004 - val_precision_5: 0.5391 - val_precision_focused: 0.4624 - val_recall_5: 0.3278 - learning_rate: 1.1440e-05\n",
      "Epoch 377/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6198 - auc_5: 0.6665 - f05: 0.5123 - loss: 0.5430 - precision_5: 0.6261 - precision_focused: 0.5123 - recall_5: 0.6267\n",
      "Epoch 377: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 377: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6198 - auc_5: 0.6665 - f05: 0.5123 - loss: 0.5430 - precision_5: 0.6262 - precision_focused: 0.5123 - recall_5: 0.6268 - val_accuracy: 0.5034 - val_auc_5: 0.5161 - val_f05: 0.4507 - val_loss: 0.6011 - val_precision_5: 0.5405 - val_precision_focused: 0.4603 - val_recall_5: 0.3210 - learning_rate: 1.1440e-05\n",
      "Epoch 378/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6210 - auc_5: 0.6689 - f05: 0.5130 - loss: 0.5407 - precision_5: 0.6246 - precision_focused: 0.5135 - recall_5: 0.6352\n",
      "Epoch 378: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 378: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6211 - auc_5: 0.6689 - f05: 0.5130 - loss: 0.5407 - precision_5: 0.6246 - precision_focused: 0.5135 - recall_5: 0.6352 - val_accuracy: 0.5056 - val_auc_5: 0.5157 - val_f05: 0.4533 - val_loss: 0.6011 - val_precision_5: 0.5436 - val_precision_focused: 0.4622 - val_recall_5: 0.3272 - learning_rate: 1.1440e-05\n",
      "Epoch 379/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6179 - auc_5: 0.6682 - f05: 0.5165 - loss: 0.5408 - precision_5: 0.6260 - precision_focused: 0.5167 - recall_5: 0.6303\n",
      "Epoch 379: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 379: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6179 - auc_5: 0.6682 - f05: 0.5164 - loss: 0.5408 - precision_5: 0.6260 - precision_focused: 0.5167 - recall_5: 0.6303 - val_accuracy: 0.5002 - val_auc_5: 0.5165 - val_f05: 0.4435 - val_loss: 0.6015 - val_precision_5: 0.5372 - val_precision_focused: 0.4562 - val_recall_5: 0.3031 - learning_rate: 1.1440e-05\n",
      "Epoch 380/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6320 - auc_5: 0.6795 - f05: 0.5072 - loss: 0.5390 - precision_5: 0.6350 - precision_focused: 0.5071 - recall_5: 0.6323\n",
      "Epoch 380: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 380: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6320 - auc_5: 0.6794 - f05: 0.5072 - loss: 0.5390 - precision_5: 0.6350 - precision_focused: 0.5072 - recall_5: 0.6322 - val_accuracy: 0.5031 - val_auc_5: 0.5161 - val_f05: 0.4511 - val_loss: 0.6011 - val_precision_5: 0.5399 - val_precision_focused: 0.4606 - val_recall_5: 0.3216 - learning_rate: 1.1440e-05\n",
      "Epoch 381/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6301 - auc_5: 0.6762 - f05: 0.5166 - loss: 0.5384 - precision_5: 0.6370 - precision_focused: 0.5168 - recall_5: 0.6415\n",
      "Epoch 381: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 381: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6301 - auc_5: 0.6762 - f05: 0.5165 - loss: 0.5384 - precision_5: 0.6369 - precision_focused: 0.5167 - recall_5: 0.6414 - val_accuracy: 0.5056 - val_auc_5: 0.5156 - val_f05: 0.4547 - val_loss: 0.6019 - val_precision_5: 0.5429 - val_precision_focused: 0.4627 - val_recall_5: 0.3321 - learning_rate: 1.1440e-05\n",
      "Epoch 382/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6304 - auc_5: 0.6794 - f05: 0.5148 - loss: 0.5366 - precision_5: 0.6398 - precision_focused: 0.5147 - recall_5: 0.6348\n",
      "Epoch 382: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 382: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6304 - auc_5: 0.6794 - f05: 0.5148 - loss: 0.5367 - precision_5: 0.6397 - precision_focused: 0.5146 - recall_5: 0.6348 - val_accuracy: 0.5072 - val_auc_5: 0.5155 - val_f05: 0.4506 - val_loss: 0.6022 - val_precision_5: 0.5469 - val_precision_focused: 0.4604 - val_recall_5: 0.3241 - learning_rate: 1.1440e-05\n",
      "Epoch 383/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6386 - auc_5: 0.6846 - f05: 0.5083 - loss: 0.5365 - precision_5: 0.6429 - precision_focused: 0.5081 - recall_5: 0.6364\n",
      "Epoch 383: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 383: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6386 - auc_5: 0.6846 - f05: 0.5083 - loss: 0.5365 - precision_5: 0.6429 - precision_focused: 0.5081 - recall_5: 0.6364 - val_accuracy: 0.5053 - val_auc_5: 0.5158 - val_f05: 0.4563 - val_loss: 0.6006 - val_precision_5: 0.5423 - val_precision_focused: 0.4642 - val_recall_5: 0.3321 - learning_rate: 1.1440e-05\n",
      "Epoch 384/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6198 - auc_5: 0.6702 - f05: 0.5167 - loss: 0.5400 - precision_5: 0.6302 - precision_focused: 0.5167 - recall_5: 0.6293\n",
      "Epoch 384: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 384: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6198 - auc_5: 0.6703 - f05: 0.5166 - loss: 0.5400 - precision_5: 0.6302 - precision_focused: 0.5166 - recall_5: 0.6293 - val_accuracy: 0.5024 - val_auc_5: 0.5154 - val_f05: 0.4511 - val_loss: 0.6011 - val_precision_5: 0.5393 - val_precision_focused: 0.4611 - val_recall_5: 0.3179 - learning_rate: 1.1440e-05\n",
      "Epoch 385/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6173 - auc_5: 0.6690 - f05: 0.5173 - loss: 0.5407 - precision_5: 0.6248 - precision_focused: 0.5176 - recall_5: 0.6319\n",
      "Epoch 385: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 385: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6174 - auc_5: 0.6690 - f05: 0.5172 - loss: 0.5407 - precision_5: 0.6248 - precision_focused: 0.5176 - recall_5: 0.6319 - val_accuracy: 0.5047 - val_auc_5: 0.5148 - val_f05: 0.4556 - val_loss: 0.6016 - val_precision_5: 0.5412 - val_precision_focused: 0.4635 - val_recall_5: 0.3321 - learning_rate: 1.1440e-05\n",
      "Epoch 386/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6245 - auc_5: 0.6734 - f05: 0.5074 - loss: 0.5403 - precision_5: 0.6280 - precision_focused: 0.5075 - recall_5: 0.6291\n",
      "Epoch 386: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 386: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6245 - auc_5: 0.6734 - f05: 0.5074 - loss: 0.5403 - precision_5: 0.6280 - precision_focused: 0.5075 - recall_5: 0.6291 - val_accuracy: 0.5037 - val_auc_5: 0.5155 - val_f05: 0.4573 - val_loss: 0.6006 - val_precision_5: 0.5392 - val_precision_focused: 0.4645 - val_recall_5: 0.3352 - learning_rate: 1.1440e-05\n",
      "Epoch 387/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6229 - auc_5: 0.6692 - f05: 0.5060 - loss: 0.5423 - precision_5: 0.6249 - precision_focused: 0.5061 - recall_5: 0.6258\n",
      "Epoch 387: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 387: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6229 - auc_5: 0.6692 - f05: 0.5060 - loss: 0.5422 - precision_5: 0.6250 - precision_focused: 0.5061 - recall_5: 0.6258 - val_accuracy: 0.5047 - val_auc_5: 0.5153 - val_f05: 0.4562 - val_loss: 0.6011 - val_precision_5: 0.5410 - val_precision_focused: 0.4637 - val_recall_5: 0.3340 - learning_rate: 1.1440e-05\n",
      "Epoch 388/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6212 - auc_5: 0.6726 - f05: 0.5065 - loss: 0.5411 - precision_5: 0.6243 - precision_focused: 0.5065 - recall_5: 0.6235\n",
      "Epoch 388: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 388: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6212 - auc_5: 0.6726 - f05: 0.5065 - loss: 0.5411 - precision_5: 0.6243 - precision_focused: 0.5066 - recall_5: 0.6235 - val_accuracy: 0.5047 - val_auc_5: 0.5150 - val_f05: 0.4617 - val_loss: 0.6003 - val_precision_5: 0.5393 - val_precision_focused: 0.4671 - val_recall_5: 0.3469 - learning_rate: 1.1440e-05\n",
      "Epoch 389/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6119 - auc_5: 0.6637 - f05: 0.5134 - loss: 0.5429 - precision_5: 0.6215 - precision_focused: 0.5135 - recall_5: 0.6214\n",
      "Epoch 389: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 389: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6120 - auc_5: 0.6637 - f05: 0.5133 - loss: 0.5429 - precision_5: 0.6215 - precision_focused: 0.5135 - recall_5: 0.6214 - val_accuracy: 0.5034 - val_auc_5: 0.5161 - val_f05: 0.4487 - val_loss: 0.6004 - val_precision_5: 0.5410 - val_precision_focused: 0.4589 - val_recall_5: 0.3179 - learning_rate: 1.1440e-05\n",
      "Epoch 390/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6241 - auc_5: 0.6707 - f05: 0.5128 - loss: 0.5406 - precision_5: 0.6319 - precision_focused: 0.5128 - recall_5: 0.6311\n",
      "Epoch 390: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 390: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6241 - auc_5: 0.6707 - f05: 0.5128 - loss: 0.5406 - precision_5: 0.6319 - precision_focused: 0.5128 - recall_5: 0.6311 - val_accuracy: 0.5037 - val_auc_5: 0.5157 - val_f05: 0.4549 - val_loss: 0.6001 - val_precision_5: 0.5399 - val_precision_focused: 0.4629 - val_recall_5: 0.3302 - learning_rate: 1.1440e-05\n",
      "Epoch 391/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6164 - auc_5: 0.6697 - f05: 0.5217 - loss: 0.5404 - precision_5: 0.6272 - precision_focused: 0.5220 - recall_5: 0.6310\n",
      "Epoch 391: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 391: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6164 - auc_5: 0.6697 - f05: 0.5217 - loss: 0.5404 - precision_5: 0.6272 - precision_focused: 0.5219 - recall_5: 0.6310 - val_accuracy: 0.5031 - val_auc_5: 0.5153 - val_f05: 0.4570 - val_loss: 0.6006 - val_precision_5: 0.5382 - val_precision_focused: 0.4640 - val_recall_5: 0.3352 - learning_rate: 1.1440e-05\n",
      "Epoch 392/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6267 - auc_5: 0.6771 - f05: 0.5101 - loss: 0.5390 - precision_5: 0.6307 - precision_focused: 0.5101 - recall_5: 0.6300\n",
      "Epoch 392: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 392: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6267 - auc_5: 0.6771 - f05: 0.5101 - loss: 0.5390 - precision_5: 0.6307 - precision_focused: 0.5101 - recall_5: 0.6300 - val_accuracy: 0.5031 - val_auc_5: 0.5151 - val_f05: 0.4585 - val_loss: 0.6003 - val_precision_5: 0.5377 - val_precision_focused: 0.4649 - val_recall_5: 0.3389 - learning_rate: 1.1440e-05\n",
      "Epoch 393/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6235 - auc_5: 0.6728 - f05: 0.5038 - loss: 0.5431 - precision_5: 0.6205 - precision_focused: 0.5043 - recall_5: 0.6312\n",
      "Epoch 393: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 393: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6235 - auc_5: 0.6728 - f05: 0.5038 - loss: 0.5430 - precision_5: 0.6205 - precision_focused: 0.5043 - recall_5: 0.6312 - val_accuracy: 0.5072 - val_auc_5: 0.5152 - val_f05: 0.4642 - val_loss: 0.6004 - val_precision_5: 0.5425 - val_precision_focused: 0.4689 - val_recall_5: 0.3543 - learning_rate: 1.1440e-05\n",
      "Epoch 394/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6248 - auc_5: 0.6746 - f05: 0.5078 - loss: 0.5412 - precision_5: 0.6245 - precision_focused: 0.5083 - recall_5: 0.6350\n",
      "Epoch 394: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 394: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6248 - auc_5: 0.6746 - f05: 0.5078 - loss: 0.5412 - precision_5: 0.6245 - precision_focused: 0.5083 - recall_5: 0.6350 - val_accuracy: 0.5018 - val_auc_5: 0.5160 - val_f05: 0.4552 - val_loss: 0.6014 - val_precision_5: 0.5366 - val_precision_focused: 0.4628 - val_recall_5: 0.3302 - learning_rate: 1.1440e-05\n",
      "Epoch 395/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6319 - auc_5: 0.6812 - f05: 0.5184 - loss: 0.5354 - precision_5: 0.6401 - precision_focused: 0.5186 - recall_5: 0.6422\n",
      "Epoch 395: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 395: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.6319 - auc_5: 0.6812 - f05: 0.5184 - loss: 0.5355 - precision_5: 0.6400 - precision_focused: 0.5185 - recall_5: 0.6421 - val_accuracy: 0.5040 - val_auc_5: 0.5162 - val_f05: 0.4501 - val_loss: 0.6025 - val_precision_5: 0.5416 - val_precision_focused: 0.4597 - val_recall_5: 0.3216 - learning_rate: 1.1440e-05\n",
      "Epoch 396/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6379 - auc_5: 0.6854 - f05: 0.5067 - loss: 0.5363 - precision_5: 0.6389 - precision_focused: 0.5070 - recall_5: 0.6444\n",
      "Epoch 396: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 396: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6379 - auc_5: 0.6854 - f05: 0.5067 - loss: 0.5363 - precision_5: 0.6388 - precision_focused: 0.5070 - recall_5: 0.6443 - val_accuracy: 0.5034 - val_auc_5: 0.5160 - val_f05: 0.4519 - val_loss: 0.6021 - val_precision_5: 0.5401 - val_precision_focused: 0.4610 - val_recall_5: 0.3241 - learning_rate: 1.1440e-05\n",
      "Epoch 397/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6312 - auc_5: 0.6789 - f05: 0.5154 - loss: 0.5363 - precision_5: 0.6386 - precision_focused: 0.5154 - recall_5: 0.6385\n",
      "Epoch 397: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 397: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6312 - auc_5: 0.6788 - f05: 0.5153 - loss: 0.5364 - precision_5: 0.6385 - precision_focused: 0.5154 - recall_5: 0.6385 - val_accuracy: 0.5043 - val_auc_5: 0.5163 - val_f05: 0.4446 - val_loss: 0.6030 - val_precision_5: 0.5437 - val_precision_focused: 0.4565 - val_recall_5: 0.3111 - learning_rate: 1.1440e-05\n",
      "Epoch 398/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6291 - auc_5: 0.6798 - f05: 0.5150 - loss: 0.5362 - precision_5: 0.6383 - precision_focused: 0.5150 - recall_5: 0.6377\n",
      "Epoch 398: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 398: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6291 - auc_5: 0.6797 - f05: 0.5149 - loss: 0.5362 - precision_5: 0.6383 - precision_focused: 0.5149 - recall_5: 0.6377 - val_accuracy: 0.5053 - val_auc_5: 0.5162 - val_f05: 0.4454 - val_loss: 0.6031 - val_precision_5: 0.5451 - val_precision_focused: 0.4569 - val_recall_5: 0.3136 - learning_rate: 1.1440e-05\n",
      "Epoch 399/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6166 - auc_5: 0.6649 - f05: 0.5201 - loss: 0.5418 - precision_5: 0.6337 - precision_focused: 0.5199 - recall_5: 0.6217\n",
      "Epoch 399: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 399: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6167 - auc_5: 0.6650 - f05: 0.5200 - loss: 0.5418 - precision_5: 0.6336 - precision_focused: 0.5198 - recall_5: 0.6219 - val_accuracy: 0.5063 - val_auc_5: 0.5160 - val_f05: 0.4399 - val_loss: 0.6038 - val_precision_5: 0.5488 - val_precision_focused: 0.4543 - val_recall_5: 0.3019 - learning_rate: 1.1440e-05\n",
      "Epoch 400/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6260 - auc_5: 0.6779 - f05: 0.5065 - loss: 0.5385 - precision_5: 0.6296 - precision_focused: 0.5064 - recall_5: 0.6229\n",
      "Epoch 400: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 400: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6260 - auc_5: 0.6779 - f05: 0.5066 - loss: 0.5385 - precision_5: 0.6296 - precision_focused: 0.5064 - recall_5: 0.6229 - val_accuracy: 0.5005 - val_auc_5: 0.5154 - val_f05: 0.4473 - val_loss: 0.6031 - val_precision_5: 0.5365 - val_precision_focused: 0.4580 - val_recall_5: 0.3130 - learning_rate: 1.1440e-05\n",
      "Epoch 401/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6119 - auc_5: 0.6633 - f05: 0.5175 - loss: 0.5424 - precision_5: 0.6227 - precision_focused: 0.5176 - recall_5: 0.6253\n",
      "Epoch 401: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 401: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6119 - auc_5: 0.6633 - f05: 0.5175 - loss: 0.5424 - precision_5: 0.6227 - precision_focused: 0.5176 - recall_5: 0.6253 - val_accuracy: 0.5027 - val_auc_5: 0.5155 - val_f05: 0.4557 - val_loss: 0.6009 - val_precision_5: 0.5382 - val_precision_focused: 0.4634 - val_recall_5: 0.3309 - learning_rate: 1.1440e-05\n",
      "Epoch 402/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6227 - auc_5: 0.6767 - f05: 0.5126 - loss: 0.5382 - precision_5: 0.6240 - precision_focused: 0.5134 - recall_5: 0.6392\n",
      "Epoch 402: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 402: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6227 - auc_5: 0.6767 - f05: 0.5126 - loss: 0.5382 - precision_5: 0.6240 - precision_focused: 0.5134 - recall_5: 0.6392 - val_accuracy: 0.5018 - val_auc_5: 0.5151 - val_f05: 0.4540 - val_loss: 0.6007 - val_precision_5: 0.5369 - val_precision_focused: 0.4621 - val_recall_5: 0.3278 - learning_rate: 1.1440e-05\n",
      "Epoch 403/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6348 - auc_5: 0.6861 - f05: 0.5159 - loss: 0.5333 - precision_5: 0.6420 - precision_focused: 0.5159 - recall_5: 0.6435\n",
      "Epoch 403: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 403: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6347 - auc_5: 0.6861 - f05: 0.5158 - loss: 0.5333 - precision_5: 0.6419 - precision_focused: 0.5159 - recall_5: 0.6435 - val_accuracy: 0.5034 - val_auc_5: 0.5144 - val_f05: 0.4610 - val_loss: 0.6009 - val_precision_5: 0.5374 - val_precision_focused: 0.4665 - val_recall_5: 0.3457 - learning_rate: 1.1440e-05\n",
      "Epoch 404/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6193 - auc_5: 0.6687 - f05: 0.5172 - loss: 0.5407 - precision_5: 0.6270 - precision_focused: 0.5175 - recall_5: 0.6338\n",
      "Epoch 404: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 404: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6193 - auc_5: 0.6687 - f05: 0.5171 - loss: 0.5407 - precision_5: 0.6270 - precision_focused: 0.5175 - recall_5: 0.6338 - val_accuracy: 0.5034 - val_auc_5: 0.5145 - val_f05: 0.4592 - val_loss: 0.6007 - val_precision_5: 0.5381 - val_precision_focused: 0.4655 - val_recall_5: 0.3401 - learning_rate: 1.1440e-05\n",
      "Epoch 405/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6311 - auc_5: 0.6792 - f05: 0.5160 - loss: 0.5366 - precision_5: 0.6393 - precision_focused: 0.5160 - recall_5: 0.6381\n",
      "Epoch 405: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 405: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6311 - auc_5: 0.6792 - f05: 0.5160 - loss: 0.5366 - precision_5: 0.6393 - precision_focused: 0.5160 - recall_5: 0.6381 - val_accuracy: 0.5024 - val_auc_5: 0.5149 - val_f05: 0.4499 - val_loss: 0.6021 - val_precision_5: 0.5391 - val_precision_focused: 0.4597 - val_recall_5: 0.3191 - learning_rate: 1.1440e-05\n",
      "Epoch 406/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6350 - auc_5: 0.6798 - f05: 0.5202 - loss: 0.5351 - precision_5: 0.6445 - precision_focused: 0.5205 - recall_5: 0.6503\n",
      "Epoch 406: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 406: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6350 - auc_5: 0.6798 - f05: 0.5202 - loss: 0.5352 - precision_5: 0.6445 - precision_focused: 0.5205 - recall_5: 0.6503 - val_accuracy: 0.5034 - val_auc_5: 0.5150 - val_f05: 0.4477 - val_loss: 0.6021 - val_precision_5: 0.5415 - val_precision_focused: 0.4587 - val_recall_5: 0.3142 - learning_rate: 1.1440e-05\n",
      "Epoch 407/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6266 - auc_5: 0.6741 - f05: 0.5146 - loss: 0.5400 - precision_5: 0.6322 - precision_focused: 0.5150 - recall_5: 0.6414\n",
      "Epoch 407: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 407: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 407: ReduceLROnPlateau reducing learning rate to 9.151971607934685e-06.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6266 - auc_5: 0.6741 - f05: 0.5146 - loss: 0.5400 - precision_5: 0.6322 - precision_focused: 0.5150 - recall_5: 0.6414 - val_accuracy: 0.5043 - val_auc_5: 0.5150 - val_f05: 0.4474 - val_loss: 0.6018 - val_precision_5: 0.5431 - val_precision_focused: 0.4584 - val_recall_5: 0.3148 - learning_rate: 1.1440e-05\n",
      "Epoch 408/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6299 - auc_5: 0.6819 - f05: 0.5077 - loss: 0.5356 - precision_5: 0.6350 - precision_focused: 0.5075 - recall_5: 0.6292\n",
      "Epoch 408: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 408: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6299 - auc_5: 0.6819 - f05: 0.5077 - loss: 0.5356 - precision_5: 0.6350 - precision_focused: 0.5075 - recall_5: 0.6291 - val_accuracy: 0.5021 - val_auc_5: 0.5145 - val_f05: 0.4581 - val_loss: 0.6012 - val_precision_5: 0.5364 - val_precision_focused: 0.4648 - val_recall_5: 0.3364 - learning_rate: 9.1520e-06\n",
      "Epoch 409/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6201 - auc_5: 0.6687 - f05: 0.5115 - loss: 0.5425 - precision_5: 0.6249 - precision_focused: 0.5117 - recall_5: 0.6284\n",
      "Epoch 409: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 409: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6202 - auc_5: 0.6687 - f05: 0.5115 - loss: 0.5425 - precision_5: 0.6249 - precision_focused: 0.5117 - recall_5: 0.6284 - val_accuracy: 0.5037 - val_auc_5: 0.5143 - val_f05: 0.4497 - val_loss: 0.6013 - val_precision_5: 0.5414 - val_precision_focused: 0.4599 - val_recall_5: 0.3185 - learning_rate: 9.1520e-06\n",
      "Epoch 410/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6188 - auc_5: 0.6741 - f05: 0.5035 - loss: 0.5400 - precision_5: 0.6200 - precision_focused: 0.5034 - recall_5: 0.6165\n",
      "Epoch 410: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 410: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6188 - auc_5: 0.6741 - f05: 0.5035 - loss: 0.5400 - precision_5: 0.6200 - precision_focused: 0.5034 - recall_5: 0.6165 - val_accuracy: 0.5031 - val_auc_5: 0.5146 - val_f05: 0.4558 - val_loss: 0.6001 - val_precision_5: 0.5388 - val_precision_focused: 0.4637 - val_recall_5: 0.3302 - learning_rate: 9.1520e-06\n",
      "Epoch 411/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6273 - auc_5: 0.6751 - f05: 0.5136 - loss: 0.5399 - precision_5: 0.6317 - precision_focused: 0.5139 - recall_5: 0.6386\n",
      "Epoch 411: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 411: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6272 - auc_5: 0.6751 - f05: 0.5136 - loss: 0.5399 - precision_5: 0.6317 - precision_focused: 0.5139 - recall_5: 0.6386 - val_accuracy: 0.5047 - val_auc_5: 0.5150 - val_f05: 0.4561 - val_loss: 0.6005 - val_precision_5: 0.5407 - val_precision_focused: 0.4632 - val_recall_5: 0.3364 - learning_rate: 9.1520e-06\n",
      "Epoch 412/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6287 - auc_5: 0.6749 - f05: 0.5160 - loss: 0.5393 - precision_5: 0.6338 - precision_focused: 0.5163 - recall_5: 0.6414\n",
      "Epoch 412: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 412: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6287 - auc_5: 0.6749 - f05: 0.5159 - loss: 0.5393 - precision_5: 0.6338 - precision_focused: 0.5163 - recall_5: 0.6414 - val_accuracy: 0.5053 - val_auc_5: 0.5144 - val_f05: 0.4604 - val_loss: 0.6013 - val_precision_5: 0.5405 - val_precision_focused: 0.4661 - val_recall_5: 0.3457 - learning_rate: 9.1520e-06\n",
      "Epoch 413/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6301 - auc_5: 0.6786 - f05: 0.5205 - loss: 0.5365 - precision_5: 0.6375 - precision_focused: 0.5210 - recall_5: 0.6486\n",
      "Epoch 413: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 413: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6301 - auc_5: 0.6786 - f05: 0.5205 - loss: 0.5365 - precision_5: 0.6375 - precision_focused: 0.5209 - recall_5: 0.6486 - val_accuracy: 0.5037 - val_auc_5: 0.5142 - val_f05: 0.4586 - val_loss: 0.6019 - val_precision_5: 0.5386 - val_precision_focused: 0.4649 - val_recall_5: 0.3401 - learning_rate: 9.1520e-06\n",
      "Epoch 414/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6261 - auc_5: 0.6732 - f05: 0.5103 - loss: 0.5395 - precision_5: 0.6319 - precision_focused: 0.5103 - recall_5: 0.6301\n",
      "Epoch 414: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 414: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6261 - auc_5: 0.6732 - f05: 0.5103 - loss: 0.5395 - precision_5: 0.6319 - precision_focused: 0.5103 - recall_5: 0.6301 - val_accuracy: 0.5037 - val_auc_5: 0.5144 - val_f05: 0.4530 - val_loss: 0.6020 - val_precision_5: 0.5401 - val_precision_focused: 0.4612 - val_recall_5: 0.3284 - learning_rate: 9.1520e-06\n",
      "Epoch 415/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6241 - auc_5: 0.6752 - f05: 0.5123 - loss: 0.5389 - precision_5: 0.6316 - precision_focused: 0.5123 - recall_5: 0.6309\n",
      "Epoch 415: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 415: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6242 - auc_5: 0.6752 - f05: 0.5123 - loss: 0.5389 - precision_5: 0.6316 - precision_focused: 0.5123 - recall_5: 0.6309 - val_accuracy: 0.5031 - val_auc_5: 0.5145 - val_f05: 0.4574 - val_loss: 0.6016 - val_precision_5: 0.5381 - val_precision_focused: 0.4644 - val_recall_5: 0.3358 - learning_rate: 9.1520e-06\n",
      "Epoch 416/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6323 - auc_5: 0.6757 - f05: 0.5123 - loss: 0.5385 - precision_5: 0.6356 - precision_focused: 0.5127 - recall_5: 0.6451\n",
      "Epoch 416: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 416: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6323 - auc_5: 0.6757 - f05: 0.5123 - loss: 0.5385 - precision_5: 0.6356 - precision_focused: 0.5127 - recall_5: 0.6451 - val_accuracy: 0.5024 - val_auc_5: 0.5152 - val_f05: 0.4561 - val_loss: 0.6019 - val_precision_5: 0.5373 - val_precision_focused: 0.4633 - val_recall_5: 0.3333 - learning_rate: 9.1520e-06\n",
      "Epoch 417/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6125 - auc_5: 0.6646 - f05: 0.5137 - loss: 0.5437 - precision_5: 0.6211 - precision_focused: 0.5136 - recall_5: 0.6184\n",
      "Epoch 417: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 417: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6126 - auc_5: 0.6646 - f05: 0.5137 - loss: 0.5437 - precision_5: 0.6211 - precision_focused: 0.5136 - recall_5: 0.6184 - val_accuracy: 0.5053 - val_auc_5: 0.5152 - val_f05: 0.4554 - val_loss: 0.6013 - val_precision_5: 0.5418 - val_precision_focused: 0.4627 - val_recall_5: 0.3358 - learning_rate: 9.1520e-06\n",
      "Epoch 418/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6350 - auc_5: 0.6810 - f05: 0.5151 - loss: 0.5360 - precision_5: 0.6423 - precision_focused: 0.5154 - recall_5: 0.6470\n",
      "Epoch 418: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 418: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6349 - auc_5: 0.6809 - f05: 0.5151 - loss: 0.5360 - precision_5: 0.6422 - precision_focused: 0.5153 - recall_5: 0.6468 - val_accuracy: 0.5079 - val_auc_5: 0.5150 - val_f05: 0.4588 - val_loss: 0.6017 - val_precision_5: 0.5447 - val_precision_focused: 0.4649 - val_recall_5: 0.3457 - learning_rate: 9.1520e-06\n",
      "Epoch 419/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6274 - auc_5: 0.6807 - f05: 0.5179 - loss: 0.5362 - precision_5: 0.6313 - precision_focused: 0.5187 - recall_5: 0.6485\n",
      "Epoch 419: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 419: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6273 - auc_5: 0.6806 - f05: 0.5179 - loss: 0.5362 - precision_5: 0.6313 - precision_focused: 0.5187 - recall_5: 0.6484 - val_accuracy: 0.5066 - val_auc_5: 0.5152 - val_f05: 0.4538 - val_loss: 0.6021 - val_precision_5: 0.5443 - val_precision_focused: 0.4616 - val_recall_5: 0.3340 - learning_rate: 9.1520e-06\n",
      "Epoch 420/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6302 - auc_5: 0.6773 - f05: 0.5171 - loss: 0.5368 - precision_5: 0.6396 - precision_focused: 0.5169 - recall_5: 0.6353\n",
      "Epoch 420: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 420: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6302 - auc_5: 0.6772 - f05: 0.5170 - loss: 0.5369 - precision_5: 0.6396 - precision_focused: 0.5169 - recall_5: 0.6353 - val_accuracy: 0.5063 - val_auc_5: 0.5152 - val_f05: 0.4538 - val_loss: 0.6021 - val_precision_5: 0.5439 - val_precision_focused: 0.4617 - val_recall_5: 0.3327 - learning_rate: 9.1520e-06\n",
      "Epoch 421/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6262 - auc_5: 0.6800 - f05: 0.5127 - loss: 0.5362 - precision_5: 0.6332 - precision_focused: 0.5129 - recall_5: 0.6373\n",
      "Epoch 421: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 421: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6261 - auc_5: 0.6799 - f05: 0.5126 - loss: 0.5363 - precision_5: 0.6332 - precision_focused: 0.5129 - recall_5: 0.6373 - val_accuracy: 0.5047 - val_auc_5: 0.5151 - val_f05: 0.4518 - val_loss: 0.6023 - val_precision_5: 0.5423 - val_precision_focused: 0.4610 - val_recall_5: 0.3247 - learning_rate: 9.1520e-06\n",
      "Epoch 422/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6389 - auc_5: 0.6925 - f05: 0.5068 - loss: 0.5316 - precision_5: 0.6425 - precision_focused: 0.5065 - recall_5: 0.6357\n",
      "Epoch 422: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 422: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6388 - auc_5: 0.6924 - f05: 0.5068 - loss: 0.5316 - precision_5: 0.6425 - precision_focused: 0.5066 - recall_5: 0.6356 - val_accuracy: 0.5050 - val_auc_5: 0.5152 - val_f05: 0.4585 - val_loss: 0.6015 - val_precision_5: 0.5406 - val_precision_focused: 0.4649 - val_recall_5: 0.3407 - learning_rate: 9.1520e-06\n",
      "Epoch 423/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6239 - auc_5: 0.6780 - f05: 0.5159 - loss: 0.5369 - precision_5: 0.6251 - precision_focused: 0.5171 - recall_5: 0.6481\n",
      "Epoch 423: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 423: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.6239 - auc_5: 0.6780 - f05: 0.5159 - loss: 0.5370 - precision_5: 0.6251 - precision_focused: 0.5170 - recall_5: 0.6481 - val_accuracy: 0.5069 - val_auc_5: 0.5149 - val_f05: 0.4579 - val_loss: 0.6016 - val_precision_5: 0.5438 - val_precision_focused: 0.4645 - val_recall_5: 0.3414 - learning_rate: 9.1520e-06\n",
      "Epoch 424/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6335 - auc_5: 0.6804 - f05: 0.5059 - loss: 0.5391 - precision_5: 0.6329 - precision_focused: 0.5063 - recall_5: 0.6418\n",
      "Epoch 424: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 424: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6335 - auc_5: 0.6804 - f05: 0.5059 - loss: 0.5391 - precision_5: 0.6329 - precision_focused: 0.5063 - recall_5: 0.6418 - val_accuracy: 0.5063 - val_auc_5: 0.5151 - val_f05: 0.4639 - val_loss: 0.6010 - val_precision_5: 0.5411 - val_precision_focused: 0.4687 - val_recall_5: 0.3537 - learning_rate: 9.1520e-06\n",
      "Epoch 425/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6195 - auc_5: 0.6662 - f05: 0.5170 - loss: 0.5431 - precision_5: 0.6221 - precision_focused: 0.5183 - recall_5: 0.6478\n",
      "Epoch 425: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 425: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6195 - auc_5: 0.6663 - f05: 0.5169 - loss: 0.5431 - precision_5: 0.6222 - precision_focused: 0.5182 - recall_5: 0.6476 - val_accuracy: 0.5060 - val_auc_5: 0.5149 - val_f05: 0.4608 - val_loss: 0.6012 - val_precision_5: 0.5413 - val_precision_focused: 0.4664 - val_recall_5: 0.3475 - learning_rate: 9.1520e-06\n",
      "Epoch 426/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6329 - auc_5: 0.6753 - f05: 0.5124 - loss: 0.5391 - precision_5: 0.6411 - precision_focused: 0.5121 - recall_5: 0.6341\n",
      "Epoch 426: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 426: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6329 - auc_5: 0.6753 - f05: 0.5124 - loss: 0.5391 - precision_5: 0.6411 - precision_focused: 0.5121 - recall_5: 0.6341 - val_accuracy: 0.5101 - val_auc_5: 0.5154 - val_f05: 0.4623 - val_loss: 0.6007 - val_precision_5: 0.5473 - val_precision_focused: 0.4675 - val_recall_5: 0.3537 - learning_rate: 9.1520e-06\n",
      "Epoch 427/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6299 - auc_5: 0.6789 - f05: 0.5042 - loss: 0.5390 - precision_5: 0.6295 - precision_focused: 0.5044 - recall_5: 0.6326\n",
      "Epoch 427: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 427: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6299 - auc_5: 0.6789 - f05: 0.5042 - loss: 0.5390 - precision_5: 0.6296 - precision_focused: 0.5044 - recall_5: 0.6326 - val_accuracy: 0.5101 - val_auc_5: 0.5151 - val_f05: 0.4593 - val_loss: 0.6006 - val_precision_5: 0.5483 - val_precision_focused: 0.4656 - val_recall_5: 0.3469 - learning_rate: 9.1520e-06\n",
      "Epoch 428/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6203 - auc_5: 0.6666 - f05: 0.5178 - loss: 0.5419 - precision_5: 0.6273 - precision_focused: 0.5183 - recall_5: 0.6380\n",
      "Epoch 428: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 428: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6203 - auc_5: 0.6667 - f05: 0.5178 - loss: 0.5419 - precision_5: 0.6274 - precision_focused: 0.5183 - recall_5: 0.6380 - val_accuracy: 0.5056 - val_auc_5: 0.5158 - val_f05: 0.4492 - val_loss: 0.6014 - val_precision_5: 0.5445 - val_precision_focused: 0.4592 - val_recall_5: 0.3210 - learning_rate: 9.1520e-06\n",
      "Epoch 429/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6274 - auc_5: 0.6751 - f05: 0.5119 - loss: 0.5392 - precision_5: 0.6359 - precision_focused: 0.5117 - recall_5: 0.6289\n",
      "Epoch 429: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 429: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6274 - auc_5: 0.6751 - f05: 0.5119 - loss: 0.5392 - precision_5: 0.6358 - precision_focused: 0.5117 - recall_5: 0.6289 - val_accuracy: 0.5053 - val_auc_5: 0.5156 - val_f05: 0.4483 - val_loss: 0.6021 - val_precision_5: 0.5443 - val_precision_focused: 0.4588 - val_recall_5: 0.3185 - learning_rate: 9.1520e-06\n",
      "Epoch 430/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6308 - auc_5: 0.6779 - f05: 0.5111 - loss: 0.5384 - precision_5: 0.6374 - precision_focused: 0.5109 - recall_5: 0.6319\n",
      "Epoch 430: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 430: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6308 - auc_5: 0.6779 - f05: 0.5111 - loss: 0.5384 - precision_5: 0.6374 - precision_focused: 0.5109 - recall_5: 0.6319 - val_accuracy: 0.5069 - val_auc_5: 0.5152 - val_f05: 0.4525 - val_loss: 0.6014 - val_precision_5: 0.5454 - val_precision_focused: 0.4611 - val_recall_5: 0.3302 - learning_rate: 9.1520e-06\n",
      "Epoch 431/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6363 - auc_5: 0.6887 - f05: 0.5031 - loss: 0.5349 - precision_5: 0.6377 - precision_focused: 0.5032 - recall_5: 0.6381\n",
      "Epoch 431: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 431: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6363 - auc_5: 0.6886 - f05: 0.5032 - loss: 0.5349 - precision_5: 0.6377 - precision_focused: 0.5032 - recall_5: 0.6380 - val_accuracy: 0.5072 - val_auc_5: 0.5148 - val_f05: 0.4633 - val_loss: 0.6007 - val_precision_5: 0.5426 - val_precision_focused: 0.4682 - val_recall_5: 0.3537 - learning_rate: 9.1520e-06\n",
      "Epoch 432/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6417 - auc_5: 0.6964 - f05: 0.5257 - loss: 0.5267 - precision_5: 0.6515 - precision_focused: 0.5262 - recall_5: 0.6627\n",
      "Epoch 432: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 432: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6415 - auc_5: 0.6962 - f05: 0.5255 - loss: 0.5269 - precision_5: 0.6512 - precision_focused: 0.5260 - recall_5: 0.6623 - val_accuracy: 0.5085 - val_auc_5: 0.5152 - val_f05: 0.4547 - val_loss: 0.6020 - val_precision_5: 0.5474 - val_precision_focused: 0.4627 - val_recall_5: 0.3352 - learning_rate: 9.1520e-06\n",
      "Epoch 433/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6331 - auc_5: 0.6860 - f05: 0.5131 - loss: 0.5334 - precision_5: 0.6419 - precision_focused: 0.5128 - recall_5: 0.6337\n",
      "Epoch 433: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 433: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6330 - auc_5: 0.6860 - f05: 0.5131 - loss: 0.5335 - precision_5: 0.6418 - precision_focused: 0.5128 - recall_5: 0.6336 - val_accuracy: 0.5092 - val_auc_5: 0.5152 - val_f05: 0.4583 - val_loss: 0.6018 - val_precision_5: 0.5472 - val_precision_focused: 0.4649 - val_recall_5: 0.3438 - learning_rate: 9.1520e-06\n",
      "Epoch 434/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6175 - auc_5: 0.6660 - f05: 0.5036 - loss: 0.5455 - precision_5: 0.6182 - precision_focused: 0.5036 - recall_5: 0.6170\n",
      "Epoch 434: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 434: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6175 - auc_5: 0.6661 - f05: 0.5037 - loss: 0.5454 - precision_5: 0.6182 - precision_focused: 0.5037 - recall_5: 0.6170 - val_accuracy: 0.5098 - val_auc_5: 0.5154 - val_f05: 0.4584 - val_loss: 0.6014 - val_precision_5: 0.5482 - val_precision_focused: 0.4651 - val_recall_5: 0.3438 - learning_rate: 9.1520e-06\n",
      "Epoch 435/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6346 - auc_5: 0.6871 - f05: 0.5186 - loss: 0.5326 - precision_5: 0.6455 - precision_focused: 0.5187 - recall_5: 0.6466\n",
      "Epoch 435: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 435: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6346 - auc_5: 0.6870 - f05: 0.5186 - loss: 0.5326 - precision_5: 0.6454 - precision_focused: 0.5186 - recall_5: 0.6466 - val_accuracy: 0.5072 - val_auc_5: 0.5154 - val_f05: 0.4527 - val_loss: 0.6026 - val_precision_5: 0.5458 - val_precision_focused: 0.4612 - val_recall_5: 0.3309 - learning_rate: 9.1520e-06\n",
      "Epoch 436/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6247 - auc_5: 0.6710 - f05: 0.5178 - loss: 0.5389 - precision_5: 0.6379 - precision_focused: 0.5175 - recall_5: 0.6272\n",
      "Epoch 436: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 436: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6246 - auc_5: 0.6710 - f05: 0.5178 - loss: 0.5389 - precision_5: 0.6378 - precision_focused: 0.5174 - recall_5: 0.6271 - val_accuracy: 0.5092 - val_auc_5: 0.5154 - val_f05: 0.4597 - val_loss: 0.6016 - val_precision_5: 0.5463 - val_precision_focused: 0.4654 - val_recall_5: 0.3494 - learning_rate: 9.1520e-06\n",
      "Epoch 437/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6203 - auc_5: 0.6693 - f05: 0.5190 - loss: 0.5403 - precision_5: 0.6283 - precision_focused: 0.5194 - recall_5: 0.6363\n",
      "Epoch 437: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 437: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6203 - auc_5: 0.6694 - f05: 0.5189 - loss: 0.5403 - precision_5: 0.6282 - precision_focused: 0.5193 - recall_5: 0.6363 - val_accuracy: 0.5082 - val_auc_5: 0.5155 - val_f05: 0.4565 - val_loss: 0.6015 - val_precision_5: 0.5461 - val_precision_focused: 0.4635 - val_recall_5: 0.3401 - learning_rate: 9.1520e-06\n",
      "Epoch 438/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6299 - auc_5: 0.6818 - f05: 0.5146 - loss: 0.5365 - precision_5: 0.6371 - precision_focused: 0.5147 - recall_5: 0.6382\n",
      "Epoch 438: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 438: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6299 - auc_5: 0.6818 - f05: 0.5146 - loss: 0.5365 - precision_5: 0.6371 - precision_focused: 0.5147 - recall_5: 0.6382 - val_accuracy: 0.5069 - val_auc_5: 0.5156 - val_f05: 0.4562 - val_loss: 0.6019 - val_precision_5: 0.5441 - val_precision_focused: 0.4632 - val_recall_5: 0.3389 - learning_rate: 9.1520e-06\n",
      "Epoch 439/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6265 - auc_5: 0.6804 - f05: 0.5176 - loss: 0.5351 - precision_5: 0.6393 - precision_focused: 0.5173 - recall_5: 0.6296\n",
      "Epoch 439: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 439: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6265 - auc_5: 0.6803 - f05: 0.5176 - loss: 0.5351 - precision_5: 0.6392 - precision_focused: 0.5172 - recall_5: 0.6296 - val_accuracy: 0.5085 - val_auc_5: 0.5153 - val_f05: 0.4577 - val_loss: 0.6019 - val_precision_5: 0.5459 - val_precision_focused: 0.4640 - val_recall_5: 0.3451 - learning_rate: 9.1520e-06\n",
      "Epoch 440/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6225 - auc_5: 0.6712 - f05: 0.5082 - loss: 0.5421 - precision_5: 0.6277 - precision_focused: 0.5079 - recall_5: 0.6216\n",
      "Epoch 440: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 440: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6226 - auc_5: 0.6712 - f05: 0.5082 - loss: 0.5420 - precision_5: 0.6278 - precision_focused: 0.5080 - recall_5: 0.6216 - val_accuracy: 0.5079 - val_auc_5: 0.5155 - val_f05: 0.4589 - val_loss: 0.6017 - val_precision_5: 0.5444 - val_precision_focused: 0.4646 - val_recall_5: 0.3481 - learning_rate: 9.1520e-06\n",
      "Epoch 441/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6250 - auc_5: 0.6776 - f05: 0.5151 - loss: 0.5368 - precision_5: 0.6330 - precision_focused: 0.5152 - recall_5: 0.6358\n",
      "Epoch 441: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 441: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6251 - auc_5: 0.6777 - f05: 0.5151 - loss: 0.5368 - precision_5: 0.6330 - precision_focused: 0.5152 - recall_5: 0.6358 - val_accuracy: 0.5098 - val_auc_5: 0.5154 - val_f05: 0.4604 - val_loss: 0.6017 - val_precision_5: 0.5469 - val_precision_focused: 0.4658 - val_recall_5: 0.3525 - learning_rate: 9.1520e-06\n",
      "Epoch 442/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6297 - auc_5: 0.6751 - f05: 0.5117 - loss: 0.5399 - precision_5: 0.6351 - precision_focused: 0.5118 - recall_5: 0.6372\n",
      "Epoch 442: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 442: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6296 - auc_5: 0.6751 - f05: 0.5117 - loss: 0.5399 - precision_5: 0.6350 - precision_focused: 0.5118 - recall_5: 0.6372 - val_accuracy: 0.5101 - val_auc_5: 0.5152 - val_f05: 0.4627 - val_loss: 0.6015 - val_precision_5: 0.5471 - val_precision_focused: 0.4678 - val_recall_5: 0.3549 - learning_rate: 9.1520e-06\n",
      "Epoch 443/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6262 - auc_5: 0.6805 - f05: 0.5090 - loss: 0.5358 - precision_5: 0.6280 - precision_focused: 0.5093 - recall_5: 0.6347\n",
      "Epoch 443: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 443: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6263 - auc_5: 0.6805 - f05: 0.5090 - loss: 0.5358 - precision_5: 0.6281 - precision_focused: 0.5093 - recall_5: 0.6347 - val_accuracy: 0.5098 - val_auc_5: 0.5147 - val_f05: 0.4660 - val_loss: 0.6018 - val_precision_5: 0.5452 - val_precision_focused: 0.4697 - val_recall_5: 0.3648 - learning_rate: 9.1520e-06\n",
      "Epoch 444/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - auc_5: 0.6793 - f05: 0.5076 - loss: 0.5384 - precision_5: 0.6284 - precision_focused: 0.5085 - recall_5: 0.6480\n",
      "Epoch 444: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 444: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6302 - auc_5: 0.6792 - f05: 0.5076 - loss: 0.5384 - precision_5: 0.6284 - precision_focused: 0.5086 - recall_5: 0.6480 - val_accuracy: 0.5098 - val_auc_5: 0.5146 - val_f05: 0.4608 - val_loss: 0.6025 - val_precision_5: 0.5468 - val_precision_focused: 0.4660 - val_recall_5: 0.3537 - learning_rate: 9.1520e-06\n",
      "Epoch 445/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6259 - auc_5: 0.6790 - f05: 0.5142 - loss: 0.5361 - precision_5: 0.6320 - precision_focused: 0.5144 - recall_5: 0.6369\n",
      "Epoch 445: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 445: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6260 - auc_5: 0.6791 - f05: 0.5141 - loss: 0.5361 - precision_5: 0.6321 - precision_focused: 0.5144 - recall_5: 0.6369 - val_accuracy: 0.5108 - val_auc_5: 0.5155 - val_f05: 0.4618 - val_loss: 0.6024 - val_precision_5: 0.5479 - val_precision_focused: 0.4667 - val_recall_5: 0.3568 - learning_rate: 9.1520e-06\n",
      "Epoch 446/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6205 - auc_5: 0.6705 - f05: 0.5134 - loss: 0.5410 - precision_5: 0.6265 - precision_focused: 0.5135 - recall_5: 0.6292\n",
      "Epoch 446: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 446: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6206 - auc_5: 0.6706 - f05: 0.5134 - loss: 0.5409 - precision_5: 0.6265 - precision_focused: 0.5135 - recall_5: 0.6293 - val_accuracy: 0.5101 - val_auc_5: 0.5155 - val_f05: 0.4600 - val_loss: 0.6026 - val_precision_5: 0.5476 - val_precision_focused: 0.4655 - val_recall_5: 0.3519 - learning_rate: 9.1520e-06\n",
      "Epoch 447/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6311 - auc_5: 0.6856 - f05: 0.5103 - loss: 0.5351 - precision_5: 0.6326 - precision_focused: 0.5106 - recall_5: 0.6408\n",
      "Epoch 447: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 447: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6311 - auc_5: 0.6856 - f05: 0.5103 - loss: 0.5351 - precision_5: 0.6326 - precision_focused: 0.5106 - recall_5: 0.6408 - val_accuracy: 0.5111 - val_auc_5: 0.5157 - val_f05: 0.4599 - val_loss: 0.6022 - val_precision_5: 0.5489 - val_precision_focused: 0.4653 - val_recall_5: 0.3537 - learning_rate: 9.1520e-06\n",
      "Epoch 448/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6316 - auc_5: 0.6801 - f05: 0.5198 - loss: 0.5365 - precision_5: 0.6422 - precision_focused: 0.5200 - recall_5: 0.6465\n",
      "Epoch 448: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 448: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6315 - auc_5: 0.6800 - f05: 0.5197 - loss: 0.5365 - precision_5: 0.6421 - precision_focused: 0.5199 - recall_5: 0.6465 - val_accuracy: 0.5105 - val_auc_5: 0.5155 - val_f05: 0.4555 - val_loss: 0.6031 - val_precision_5: 0.5496 - val_precision_focused: 0.4626 - val_recall_5: 0.3420 - learning_rate: 9.1520e-06\n",
      "Epoch 449/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6250 - auc_5: 0.6689 - f05: 0.5085 - loss: 0.5420 - precision_5: 0.6336 - precision_focused: 0.5080 - recall_5: 0.6187\n",
      "Epoch 449: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 449: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6250 - auc_5: 0.6689 - f05: 0.5085 - loss: 0.5420 - precision_5: 0.6336 - precision_focused: 0.5080 - recall_5: 0.6187 - val_accuracy: 0.5092 - val_auc_5: 0.5153 - val_f05: 0.4624 - val_loss: 0.6019 - val_precision_5: 0.5453 - val_precision_focused: 0.4670 - val_recall_5: 0.3568 - learning_rate: 9.1520e-06\n",
      "Epoch 450/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6329 - auc_5: 0.6863 - f05: 0.5186 - loss: 0.5340 - precision_5: 0.6376 - precision_focused: 0.5194 - recall_5: 0.6553\n",
      "Epoch 450: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 450: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6329 - auc_5: 0.6863 - f05: 0.5186 - loss: 0.5340 - precision_5: 0.6376 - precision_focused: 0.5194 - recall_5: 0.6553 - val_accuracy: 0.5095 - val_auc_5: 0.5157 - val_f05: 0.4568 - val_loss: 0.6028 - val_precision_5: 0.5481 - val_precision_focused: 0.4639 - val_recall_5: 0.3414 - learning_rate: 9.1520e-06\n",
      "Epoch 451/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6218 - auc_5: 0.6685 - f05: 0.5068 - loss: 0.5418 - precision_5: 0.6261 - precision_focused: 0.5066 - recall_5: 0.6212\n",
      "Epoch 451: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 451: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6219 - auc_5: 0.6686 - f05: 0.5068 - loss: 0.5418 - precision_5: 0.6262 - precision_focused: 0.5067 - recall_5: 0.6213 - val_accuracy: 0.5092 - val_auc_5: 0.5150 - val_f05: 0.4595 - val_loss: 0.6028 - val_precision_5: 0.5466 - val_precision_focused: 0.4656 - val_recall_5: 0.3475 - learning_rate: 9.1520e-06\n",
      "Epoch 452/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6271 - auc_5: 0.6739 - f05: 0.5254 - loss: 0.5376 - precision_5: 0.6365 - precision_focused: 0.5261 - recall_5: 0.6523\n",
      "Epoch 452: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 452: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6271 - auc_5: 0.6739 - f05: 0.5252 - loss: 0.5376 - precision_5: 0.6364 - precision_focused: 0.5259 - recall_5: 0.6521 - val_accuracy: 0.5082 - val_auc_5: 0.5158 - val_f05: 0.4496 - val_loss: 0.6038 - val_precision_5: 0.5484 - val_precision_focused: 0.4592 - val_recall_5: 0.3253 - learning_rate: 9.1520e-06\n",
      "Epoch 453/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6278 - auc_5: 0.6767 - f05: 0.5212 - loss: 0.5367 - precision_5: 0.6395 - precision_focused: 0.5213 - recall_5: 0.6415\n",
      "Epoch 453: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 453: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6279 - auc_5: 0.6767 - f05: 0.5211 - loss: 0.5367 - precision_5: 0.6395 - precision_focused: 0.5212 - recall_5: 0.6414 - val_accuracy: 0.5098 - val_auc_5: 0.5159 - val_f05: 0.4483 - val_loss: 0.6040 - val_precision_5: 0.5516 - val_precision_focused: 0.4586 - val_recall_5: 0.3235 - learning_rate: 9.1520e-06\n",
      "Epoch 454/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6317 - auc_5: 0.6804 - f05: 0.5111 - loss: 0.5376 - precision_5: 0.6376 - precision_focused: 0.5109 - recall_5: 0.6336\n",
      "Epoch 454: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 454: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6317 - auc_5: 0.6804 - f05: 0.5111 - loss: 0.5376 - precision_5: 0.6376 - precision_focused: 0.5109 - recall_5: 0.6336 - val_accuracy: 0.5072 - val_auc_5: 0.5157 - val_f05: 0.4516 - val_loss: 0.6037 - val_precision_5: 0.5462 - val_precision_focused: 0.4605 - val_recall_5: 0.3284 - learning_rate: 9.1520e-06\n",
      "Epoch 455/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6236 - auc_5: 0.6783 - f05: 0.5145 - loss: 0.5364 - precision_5: 0.6293 - precision_focused: 0.5148 - recall_5: 0.6365\n",
      "Epoch 455: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 455: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6236 - auc_5: 0.6783 - f05: 0.5145 - loss: 0.5364 - precision_5: 0.6293 - precision_focused: 0.5148 - recall_5: 0.6365 - val_accuracy: 0.5098 - val_auc_5: 0.5157 - val_f05: 0.4529 - val_loss: 0.6028 - val_precision_5: 0.5498 - val_precision_focused: 0.4613 - val_recall_5: 0.3340 - learning_rate: 9.1520e-06\n",
      "Epoch 456/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6269 - auc_5: 0.6828 - f05: 0.5140 - loss: 0.5338 - precision_5: 0.6357 - precision_focused: 0.5138 - recall_5: 0.6293\n",
      "Epoch 456: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 456: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6269 - auc_5: 0.6827 - f05: 0.5140 - loss: 0.5338 - precision_5: 0.6357 - precision_focused: 0.5137 - recall_5: 0.6294 - val_accuracy: 0.5092 - val_auc_5: 0.5161 - val_f05: 0.4539 - val_loss: 0.6033 - val_precision_5: 0.5486 - val_precision_focused: 0.4620 - val_recall_5: 0.3346 - learning_rate: 9.1520e-06\n",
      "Epoch 457/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6246 - auc_5: 0.6761 - f05: 0.5148 - loss: 0.5372 - precision_5: 0.6336 - precision_focused: 0.5148 - recall_5: 0.6348\n",
      "Epoch 457: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 457: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 457: ReduceLROnPlateau reducing learning rate to 7.321577140828595e-06.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6246 - auc_5: 0.6760 - f05: 0.5147 - loss: 0.5372 - precision_5: 0.6336 - precision_focused: 0.5148 - recall_5: 0.6347 - val_accuracy: 0.5095 - val_auc_5: 0.5161 - val_f05: 0.4589 - val_loss: 0.6020 - val_precision_5: 0.5475 - val_precision_focused: 0.4654 - val_recall_5: 0.3451 - learning_rate: 9.1520e-06\n",
      "Epoch 458/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6397 - auc_5: 0.6935 - f05: 0.5167 - loss: 0.5295 - precision_5: 0.6460 - precision_focused: 0.5171 - recall_5: 0.6522\n",
      "Epoch 458: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 458: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6395 - auc_5: 0.6933 - f05: 0.5167 - loss: 0.5295 - precision_5: 0.6459 - precision_focused: 0.5170 - recall_5: 0.6520 - val_accuracy: 0.5089 - val_auc_5: 0.5164 - val_f05: 0.4595 - val_loss: 0.6017 - val_precision_5: 0.5463 - val_precision_focused: 0.4658 - val_recall_5: 0.3457 - learning_rate: 7.3216e-06\n",
      "Epoch 459/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6249 - auc_5: 0.6754 - f05: 0.5087 - loss: 0.5400 - precision_5: 0.6297 - precision_focused: 0.5086 - recall_5: 0.6250\n",
      "Epoch 459: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 459: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6248 - auc_5: 0.6754 - f05: 0.5087 - loss: 0.5400 - precision_5: 0.6297 - precision_focused: 0.5086 - recall_5: 0.6250 - val_accuracy: 0.5108 - val_auc_5: 0.5162 - val_f05: 0.4613 - val_loss: 0.6014 - val_precision_5: 0.5487 - val_precision_focused: 0.4670 - val_recall_5: 0.3512 - learning_rate: 7.3216e-06\n",
      "Epoch 460/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6328 - auc_5: 0.6877 - f05: 0.5151 - loss: 0.5321 - precision_5: 0.6407 - precision_focused: 0.5152 - recall_5: 0.6415\n",
      "Epoch 460: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 460: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6327 - auc_5: 0.6875 - f05: 0.5151 - loss: 0.5321 - precision_5: 0.6406 - precision_focused: 0.5152 - recall_5: 0.6414 - val_accuracy: 0.5085 - val_auc_5: 0.5162 - val_f05: 0.4574 - val_loss: 0.6020 - val_precision_5: 0.5466 - val_precision_focused: 0.4645 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 461/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6248 - auc_5: 0.6725 - f05: 0.5193 - loss: 0.5389 - precision_5: 0.6349 - precision_focused: 0.5195 - recall_5: 0.6387\n",
      "Epoch 461: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 461: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6248 - auc_5: 0.6726 - f05: 0.5192 - loss: 0.5389 - precision_5: 0.6349 - precision_focused: 0.5194 - recall_5: 0.6387 - val_accuracy: 0.5072 - val_auc_5: 0.5159 - val_f05: 0.4585 - val_loss: 0.6024 - val_precision_5: 0.5443 - val_precision_focused: 0.4652 - val_recall_5: 0.3414 - learning_rate: 7.3216e-06\n",
      "Epoch 462/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6324 - auc_5: 0.6845 - f05: 0.5169 - loss: 0.5338 - precision_5: 0.6389 - precision_focused: 0.5172 - recall_5: 0.6457\n",
      "Epoch 462: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 462: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6324 - auc_5: 0.6845 - f05: 0.5169 - loss: 0.5338 - precision_5: 0.6389 - precision_focused: 0.5172 - recall_5: 0.6457 - val_accuracy: 0.5079 - val_auc_5: 0.5157 - val_f05: 0.4505 - val_loss: 0.6038 - val_precision_5: 0.5477 - val_precision_focused: 0.4600 - val_recall_5: 0.3259 - learning_rate: 7.3216e-06\n",
      "Epoch 463/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6246 - auc_5: 0.6792 - f05: 0.5067 - loss: 0.5377 - precision_5: 0.6322 - precision_focused: 0.5062 - recall_5: 0.6172\n",
      "Epoch 463: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 463: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6246 - auc_5: 0.6792 - f05: 0.5067 - loss: 0.5377 - precision_5: 0.6322 - precision_focused: 0.5062 - recall_5: 0.6172 - val_accuracy: 0.5079 - val_auc_5: 0.5153 - val_f05: 0.4614 - val_loss: 0.6025 - val_precision_5: 0.5443 - val_precision_focused: 0.4671 - val_recall_5: 0.3488 - learning_rate: 7.3216e-06\n",
      "Epoch 464/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6295 - auc_5: 0.6777 - f05: 0.5160 - loss: 0.5381 - precision_5: 0.6346 - precision_focused: 0.5165 - recall_5: 0.6454\n",
      "Epoch 464: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 464: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6295 - auc_5: 0.6776 - f05: 0.5160 - loss: 0.5381 - precision_5: 0.6345 - precision_focused: 0.5165 - recall_5: 0.6453 - val_accuracy: 0.5076 - val_auc_5: 0.5158 - val_f05: 0.4574 - val_loss: 0.6028 - val_precision_5: 0.5451 - val_precision_focused: 0.4644 - val_recall_5: 0.3395 - learning_rate: 7.3216e-06\n",
      "Epoch 465/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6194 - auc_5: 0.6712 - f05: 0.5090 - loss: 0.5411 - precision_5: 0.6224 - precision_focused: 0.5093 - recall_5: 0.6283\n",
      "Epoch 465: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 465: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6194 - auc_5: 0.6712 - f05: 0.5090 - loss: 0.5411 - precision_5: 0.6224 - precision_focused: 0.5093 - recall_5: 0.6283 - val_accuracy: 0.5069 - val_auc_5: 0.5156 - val_f05: 0.4579 - val_loss: 0.6025 - val_precision_5: 0.5439 - val_precision_focused: 0.4647 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 466/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - auc_5: 0.6785 - f05: 0.5181 - loss: 0.5382 - precision_5: 0.6405 - precision_focused: 0.5182 - recall_5: 0.6411\n",
      "Epoch 466: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 466: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6301 - auc_5: 0.6785 - f05: 0.5181 - loss: 0.5382 - precision_5: 0.6405 - precision_focused: 0.5182 - recall_5: 0.6411 - val_accuracy: 0.5079 - val_auc_5: 0.5156 - val_f05: 0.4570 - val_loss: 0.6028 - val_precision_5: 0.5455 - val_precision_focused: 0.4640 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 467/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6293 - auc_5: 0.6860 - f05: 0.5124 - loss: 0.5332 - precision_5: 0.6358 - precision_focused: 0.5125 - recall_5: 0.6358\n",
      "Epoch 467: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 467: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6293 - auc_5: 0.6860 - f05: 0.5124 - loss: 0.5332 - precision_5: 0.6358 - precision_focused: 0.5125 - recall_5: 0.6357 - val_accuracy: 0.5076 - val_auc_5: 0.5157 - val_f05: 0.4602 - val_loss: 0.6031 - val_precision_5: 0.5441 - val_precision_focused: 0.4662 - val_recall_5: 0.3463 - learning_rate: 7.3216e-06\n",
      "Epoch 468/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6244 - auc_5: 0.6775 - f05: 0.5175 - loss: 0.5362 - precision_5: 0.6312 - precision_focused: 0.5180 - recall_5: 0.6444\n",
      "Epoch 468: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 468: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6244 - auc_5: 0.6775 - f05: 0.5174 - loss: 0.5362 - precision_5: 0.6311 - precision_focused: 0.5180 - recall_5: 0.6444 - val_accuracy: 0.5069 - val_auc_5: 0.5157 - val_f05: 0.4593 - val_loss: 0.6028 - val_precision_5: 0.5434 - val_precision_focused: 0.4656 - val_recall_5: 0.3438 - learning_rate: 7.3216e-06\n",
      "Epoch 469/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6371 - auc_5: 0.6917 - f05: 0.5127 - loss: 0.5308 - precision_5: 0.6399 - precision_focused: 0.5132 - recall_5: 0.6516\n",
      "Epoch 469: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 469: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6371 - auc_5: 0.6916 - f05: 0.5127 - loss: 0.5309 - precision_5: 0.6399 - precision_focused: 0.5132 - recall_5: 0.6515 - val_accuracy: 0.5072 - val_auc_5: 0.5153 - val_f05: 0.4597 - val_loss: 0.6031 - val_precision_5: 0.5439 - val_precision_focused: 0.4660 - val_recall_5: 0.3444 - learning_rate: 7.3216e-06\n",
      "Epoch 470/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6426 - auc_5: 0.6888 - f05: 0.5082 - loss: 0.5349 - precision_5: 0.6445 - precision_focused: 0.5085 - recall_5: 0.6504\n",
      "Epoch 470: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 470: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6425 - auc_5: 0.6888 - f05: 0.5082 - loss: 0.5349 - precision_5: 0.6444 - precision_focused: 0.5085 - recall_5: 0.6504 - val_accuracy: 0.5082 - val_auc_5: 0.5153 - val_f05: 0.4636 - val_loss: 0.6024 - val_precision_5: 0.5440 - val_precision_focused: 0.4684 - val_recall_5: 0.3549 - learning_rate: 7.3216e-06\n",
      "Epoch 471/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6328 - auc_5: 0.6862 - f05: 0.5145 - loss: 0.5330 - precision_5: 0.6388 - precision_focused: 0.5149 - recall_5: 0.6478\n",
      "Epoch 471: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 471: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6328 - auc_5: 0.6861 - f05: 0.5144 - loss: 0.5331 - precision_5: 0.6388 - precision_focused: 0.5149 - recall_5: 0.6477 - val_accuracy: 0.5092 - val_auc_5: 0.5155 - val_f05: 0.4606 - val_loss: 0.6032 - val_precision_5: 0.5463 - val_precision_focused: 0.4663 - val_recall_5: 0.3494 - learning_rate: 7.3216e-06\n",
      "Epoch 472/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6334 - auc_5: 0.6739 - f05: 0.5112 - loss: 0.5409 - precision_5: 0.6385 - precision_focused: 0.5114 - recall_5: 0.6421\n",
      "Epoch 472: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 472: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6334 - auc_5: 0.6740 - f05: 0.5112 - loss: 0.5409 - precision_5: 0.6385 - precision_focused: 0.5114 - recall_5: 0.6420 - val_accuracy: 0.5043 - val_auc_5: 0.5153 - val_f05: 0.4621 - val_loss: 0.6025 - val_precision_5: 0.5388 - val_precision_focused: 0.4674 - val_recall_5: 0.3475 - learning_rate: 7.3216e-06\n",
      "Epoch 473/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - auc_5: 0.6804 - f05: 0.5109 - loss: 0.5373 - precision_5: 0.6315 - precision_focused: 0.5114 - recall_5: 0.6427\n",
      "Epoch 473: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 473: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6302 - auc_5: 0.6804 - f05: 0.5109 - loss: 0.5373 - precision_5: 0.6315 - precision_focused: 0.5114 - recall_5: 0.6426 - val_accuracy: 0.5069 - val_auc_5: 0.5149 - val_f05: 0.4640 - val_loss: 0.6023 - val_precision_5: 0.5418 - val_precision_focused: 0.4685 - val_recall_5: 0.3562 - learning_rate: 7.3216e-06\n",
      "Epoch 474/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6294 - auc_5: 0.6852 - f05: 0.5198 - loss: 0.5328 - precision_5: 0.6364 - precision_focused: 0.5205 - recall_5: 0.6508\n",
      "Epoch 474: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 474: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6294 - auc_5: 0.6852 - f05: 0.5198 - loss: 0.5328 - precision_5: 0.6364 - precision_focused: 0.5204 - recall_5: 0.6508 - val_accuracy: 0.5050 - val_auc_5: 0.5151 - val_f05: 0.4567 - val_loss: 0.6040 - val_precision_5: 0.5412 - val_precision_focused: 0.4638 - val_recall_5: 0.3364 - learning_rate: 7.3216e-06\n",
      "Epoch 475/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6315 - auc_5: 0.6812 - f05: 0.5133 - loss: 0.5363 - precision_5: 0.6391 - precision_focused: 0.5135 - recall_5: 0.6384\n",
      "Epoch 475: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 475: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6315 - auc_5: 0.6812 - f05: 0.5133 - loss: 0.5363 - precision_5: 0.6391 - precision_focused: 0.5135 - recall_5: 0.6383 - val_accuracy: 0.5076 - val_auc_5: 0.5150 - val_f05: 0.4559 - val_loss: 0.6037 - val_precision_5: 0.5455 - val_precision_focused: 0.4634 - val_recall_5: 0.3370 - learning_rate: 7.3216e-06\n",
      "Epoch 476/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6352 - auc_5: 0.6814 - f05: 0.5121 - loss: 0.5366 - precision_5: 0.6408 - precision_focused: 0.5121 - recall_5: 0.6408\n",
      "Epoch 476: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 476: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6352 - auc_5: 0.6813 - f05: 0.5121 - loss: 0.5366 - precision_5: 0.6408 - precision_focused: 0.5121 - recall_5: 0.6408 - val_accuracy: 0.5069 - val_auc_5: 0.5156 - val_f05: 0.4540 - val_loss: 0.6037 - val_precision_5: 0.5451 - val_precision_focused: 0.4622 - val_recall_5: 0.3321 - learning_rate: 7.3216e-06\n",
      "Epoch 477/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6281 - auc_5: 0.6768 - f05: 0.5138 - loss: 0.5377 - precision_5: 0.6361 - precision_focused: 0.5138 - recall_5: 0.6348\n",
      "Epoch 477: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 477: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6281 - auc_5: 0.6769 - f05: 0.5138 - loss: 0.5377 - precision_5: 0.6361 - precision_focused: 0.5137 - recall_5: 0.6348 - val_accuracy: 0.5063 - val_auc_5: 0.5150 - val_f05: 0.4605 - val_loss: 0.6035 - val_precision_5: 0.5423 - val_precision_focused: 0.4666 - val_recall_5: 0.3444 - learning_rate: 7.3216e-06\n",
      "Epoch 478/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6343 - auc_5: 0.6868 - f05: 0.5084 - loss: 0.5354 - precision_5: 0.6347 - precision_focused: 0.5089 - recall_5: 0.6437\n",
      "Epoch 478: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 478: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.6343 - auc_5: 0.6868 - f05: 0.5085 - loss: 0.5354 - precision_5: 0.6347 - precision_focused: 0.5089 - recall_5: 0.6437 - val_accuracy: 0.5047 - val_auc_5: 0.5146 - val_f05: 0.4566 - val_loss: 0.6038 - val_precision_5: 0.5409 - val_precision_focused: 0.4640 - val_recall_5: 0.3346 - learning_rate: 7.3216e-06\n",
      "Epoch 479/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6349 - auc_5: 0.6851 - f05: 0.5164 - loss: 0.5348 - precision_5: 0.6389 - precision_focused: 0.5168 - recall_5: 0.6468\n",
      "Epoch 479: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 479: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6349 - auc_5: 0.6851 - f05: 0.5164 - loss: 0.5348 - precision_5: 0.6389 - precision_focused: 0.5167 - recall_5: 0.6467 - val_accuracy: 0.5047 - val_auc_5: 0.5147 - val_f05: 0.4596 - val_loss: 0.6039 - val_precision_5: 0.5400 - val_precision_focused: 0.4659 - val_recall_5: 0.3414 - learning_rate: 7.3216e-06\n",
      "Epoch 480/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6203 - auc_5: 0.6729 - f05: 0.5107 - loss: 0.5403 - precision_5: 0.6256 - precision_focused: 0.5106 - recall_5: 0.6225\n",
      "Epoch 480: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 480: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6204 - auc_5: 0.6730 - f05: 0.5107 - loss: 0.5402 - precision_5: 0.6257 - precision_focused: 0.5106 - recall_5: 0.6226 - val_accuracy: 0.5069 - val_auc_5: 0.5148 - val_f05: 0.4625 - val_loss: 0.6030 - val_precision_5: 0.5425 - val_precision_focused: 0.4677 - val_recall_5: 0.3506 - learning_rate: 7.3216e-06\n",
      "Epoch 481/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6271 - auc_5: 0.6799 - f05: 0.5146 - loss: 0.5365 - precision_5: 0.6321 - precision_focused: 0.5151 - recall_5: 0.6423\n",
      "Epoch 481: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 481: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6271 - auc_5: 0.6798 - f05: 0.5146 - loss: 0.5365 - precision_5: 0.6321 - precision_focused: 0.5150 - recall_5: 0.6421 - val_accuracy: 0.5040 - val_auc_5: 0.5148 - val_f05: 0.4581 - val_loss: 0.6033 - val_precision_5: 0.5394 - val_precision_focused: 0.4648 - val_recall_5: 0.3383 - learning_rate: 7.3216e-06\n",
      "Epoch 482/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6292 - auc_5: 0.6786 - f05: 0.5196 - loss: 0.5374 - precision_5: 0.6408 - precision_focused: 0.5194 - recall_5: 0.6349\n",
      "Epoch 482: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 482: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6292 - auc_5: 0.6786 - f05: 0.5196 - loss: 0.5374 - precision_5: 0.6408 - precision_focused: 0.5194 - recall_5: 0.6349 - val_accuracy: 0.5060 - val_auc_5: 0.5148 - val_f05: 0.4623 - val_loss: 0.6025 - val_precision_5: 0.5412 - val_precision_focused: 0.4677 - val_recall_5: 0.3488 - learning_rate: 7.3216e-06\n",
      "Epoch 483/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6321 - auc_5: 0.6851 - f05: 0.5050 - loss: 0.5363 - precision_5: 0.6307 - precision_focused: 0.5057 - recall_5: 0.6427\n",
      "Epoch 483: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 483: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6321 - auc_5: 0.6851 - f05: 0.5050 - loss: 0.5363 - precision_5: 0.6307 - precision_focused: 0.5057 - recall_5: 0.6427 - val_accuracy: 0.5034 - val_auc_5: 0.5153 - val_f05: 0.4597 - val_loss: 0.6032 - val_precision_5: 0.5381 - val_precision_focused: 0.4660 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 484/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6303 - auc_5: 0.6810 - f05: 0.5060 - loss: 0.5378 - precision_5: 0.6310 - precision_focused: 0.5063 - recall_5: 0.6364\n",
      "Epoch 484: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 484: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6304 - auc_5: 0.6810 - f05: 0.5061 - loss: 0.5378 - precision_5: 0.6310 - precision_focused: 0.5063 - recall_5: 0.6364 - val_accuracy: 0.5047 - val_auc_5: 0.5149 - val_f05: 0.4640 - val_loss: 0.6032 - val_precision_5: 0.5387 - val_precision_focused: 0.4687 - val_recall_5: 0.3525 - learning_rate: 7.3216e-06\n",
      "Epoch 485/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6193 - auc_5: 0.6719 - f05: 0.5084 - loss: 0.5411 - precision_5: 0.6210 - precision_focused: 0.5088 - recall_5: 0.6294\n",
      "Epoch 485: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 485: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6193 - auc_5: 0.6719 - f05: 0.5084 - loss: 0.5411 - precision_5: 0.6210 - precision_focused: 0.5088 - recall_5: 0.6294 - val_accuracy: 0.5063 - val_auc_5: 0.5149 - val_f05: 0.4633 - val_loss: 0.6029 - val_precision_5: 0.5413 - val_precision_focused: 0.4682 - val_recall_5: 0.3519 - learning_rate: 7.3216e-06\n",
      "Epoch 486/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6208 - auc_5: 0.6738 - f05: 0.5070 - loss: 0.5423 - precision_5: 0.6186 - precision_focused: 0.5078 - recall_5: 0.6340\n",
      "Epoch 486: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 486: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6209 - auc_5: 0.6739 - f05: 0.5070 - loss: 0.5423 - precision_5: 0.6187 - precision_focused: 0.5078 - recall_5: 0.6339 - val_accuracy: 0.5047 - val_auc_5: 0.5148 - val_f05: 0.4608 - val_loss: 0.6033 - val_precision_5: 0.5397 - val_precision_focused: 0.4666 - val_recall_5: 0.3444 - learning_rate: 7.3216e-06\n",
      "Epoch 487/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6298 - auc_5: 0.6771 - f05: 0.5122 - loss: 0.5380 - precision_5: 0.6368 - precision_focused: 0.5123 - recall_5: 0.6351\n",
      "Epoch 487: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 487: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6298 - auc_5: 0.6771 - f05: 0.5122 - loss: 0.5380 - precision_5: 0.6368 - precision_focused: 0.5123 - recall_5: 0.6351 - val_accuracy: 0.5053 - val_auc_5: 0.5146 - val_f05: 0.4590 - val_loss: 0.6036 - val_precision_5: 0.5413 - val_precision_focused: 0.4656 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 488/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6319 - auc_5: 0.6800 - f05: 0.5101 - loss: 0.5375 - precision_5: 0.6406 - precision_focused: 0.5096 - recall_5: 0.6269\n",
      "Epoch 488: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 488: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6319 - auc_5: 0.6800 - f05: 0.5101 - loss: 0.5375 - precision_5: 0.6405 - precision_focused: 0.5096 - recall_5: 0.6269 - val_accuracy: 0.5069 - val_auc_5: 0.5150 - val_f05: 0.4595 - val_loss: 0.6036 - val_precision_5: 0.5437 - val_precision_focused: 0.4661 - val_recall_5: 0.3420 - learning_rate: 7.3216e-06\n",
      "Epoch 489/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6207 - auc_5: 0.6698 - f05: 0.5116 - loss: 0.5414 - precision_5: 0.6248 - precision_focused: 0.5118 - recall_5: 0.6283\n",
      "Epoch 489: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 489: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.6208 - auc_5: 0.6699 - f05: 0.5116 - loss: 0.5413 - precision_5: 0.6249 - precision_focused: 0.5118 - recall_5: 0.6283 - val_accuracy: 0.5056 - val_auc_5: 0.5155 - val_f05: 0.4563 - val_loss: 0.6038 - val_precision_5: 0.5425 - val_precision_focused: 0.4638 - val_recall_5: 0.3346 - learning_rate: 7.3216e-06\n",
      "Epoch 490/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6307 - auc_5: 0.6815 - f05: 0.5120 - loss: 0.5356 - precision_5: 0.6368 - precision_focused: 0.5121 - recall_5: 0.6380\n",
      "Epoch 490: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 490: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6306 - auc_5: 0.6815 - f05: 0.5120 - loss: 0.5357 - precision_5: 0.6368 - precision_focused: 0.5121 - recall_5: 0.6379 - val_accuracy: 0.5056 - val_auc_5: 0.5154 - val_f05: 0.4573 - val_loss: 0.6034 - val_precision_5: 0.5423 - val_precision_focused: 0.4645 - val_recall_5: 0.3364 - learning_rate: 7.3216e-06\n",
      "Epoch 491/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6329 - auc_5: 0.6832 - f05: 0.5082 - loss: 0.5351 - precision_5: 0.6370 - precision_focused: 0.5080 - recall_5: 0.6324\n",
      "Epoch 491: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 491: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6329 - auc_5: 0.6832 - f05: 0.5082 - loss: 0.5351 - precision_5: 0.6370 - precision_focused: 0.5080 - recall_5: 0.6324 - val_accuracy: 0.5043 - val_auc_5: 0.5152 - val_f05: 0.4594 - val_loss: 0.6032 - val_precision_5: 0.5396 - val_precision_focused: 0.4657 - val_recall_5: 0.3407 - learning_rate: 7.3216e-06\n",
      "Epoch 492/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6407 - auc_5: 0.6913 - f05: 0.5174 - loss: 0.5309 - precision_5: 0.6466 - precision_focused: 0.5177 - recall_5: 0.6544\n",
      "Epoch 492: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 492: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6406 - auc_5: 0.6912 - f05: 0.5173 - loss: 0.5310 - precision_5: 0.6465 - precision_focused: 0.5177 - recall_5: 0.6543 - val_accuracy: 0.5063 - val_auc_5: 0.5155 - val_f05: 0.4569 - val_loss: 0.6039 - val_precision_5: 0.5431 - val_precision_focused: 0.4640 - val_recall_5: 0.3383 - learning_rate: 7.3216e-06\n",
      "Epoch 493/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6269 - auc_5: 0.6822 - f05: 0.5091 - loss: 0.5352 - precision_5: 0.6328 - precision_focused: 0.5089 - recall_5: 0.6250\n",
      "Epoch 493: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 493: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6270 - auc_5: 0.6823 - f05: 0.5092 - loss: 0.5352 - precision_5: 0.6328 - precision_focused: 0.5089 - recall_5: 0.6251 - val_accuracy: 0.5060 - val_auc_5: 0.5153 - val_f05: 0.4577 - val_loss: 0.6038 - val_precision_5: 0.5423 - val_precision_focused: 0.4644 - val_recall_5: 0.3401 - learning_rate: 7.3216e-06\n",
      "Epoch 494/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6249 - auc_5: 0.6791 - f05: 0.5183 - loss: 0.5353 - precision_5: 0.6358 - precision_focused: 0.5182 - recall_5: 0.6327\n",
      "Epoch 494: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 494: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6249 - auc_5: 0.6791 - f05: 0.5183 - loss: 0.5353 - precision_5: 0.6358 - precision_focused: 0.5182 - recall_5: 0.6326 - val_accuracy: 0.5076 - val_auc_5: 0.5155 - val_f05: 0.4559 - val_loss: 0.6034 - val_precision_5: 0.5455 - val_precision_focused: 0.4634 - val_recall_5: 0.3370 - learning_rate: 7.3216e-06\n",
      "Epoch 495/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6345 - auc_5: 0.6876 - f05: 0.5116 - loss: 0.5334 - precision_5: 0.6466 - precision_focused: 0.5110 - recall_5: 0.6288\n",
      "Epoch 495: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 495: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6345 - auc_5: 0.6876 - f05: 0.5116 - loss: 0.5334 - precision_5: 0.6466 - precision_focused: 0.5110 - recall_5: 0.6288 - val_accuracy: 0.5072 - val_auc_5: 0.5151 - val_f05: 0.4625 - val_loss: 0.6030 - val_precision_5: 0.5428 - val_precision_focused: 0.4675 - val_recall_5: 0.3525 - learning_rate: 7.3216e-06\n",
      "Epoch 496/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6282 - auc_5: 0.6759 - f05: 0.5159 - loss: 0.5377 - precision_5: 0.6376 - precision_focused: 0.5160 - recall_5: 0.6383\n",
      "Epoch 496: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 496: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6282 - auc_5: 0.6759 - f05: 0.5159 - loss: 0.5377 - precision_5: 0.6375 - precision_focused: 0.5160 - recall_5: 0.6383 - val_accuracy: 0.5101 - val_auc_5: 0.5149 - val_f05: 0.4609 - val_loss: 0.6025 - val_precision_5: 0.5476 - val_precision_focused: 0.4665 - val_recall_5: 0.3512 - learning_rate: 7.3216e-06\n",
      "Epoch 497/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6333 - auc_5: 0.6797 - f05: 0.5103 - loss: 0.5381 - precision_5: 0.6387 - precision_focused: 0.5103 - recall_5: 0.6361\n",
      "Epoch 497: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 497: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6333 - auc_5: 0.6797 - f05: 0.5103 - loss: 0.5380 - precision_5: 0.6387 - precision_focused: 0.5103 - recall_5: 0.6361 - val_accuracy: 0.5089 - val_auc_5: 0.5145 - val_f05: 0.4686 - val_loss: 0.6021 - val_precision_5: 0.5435 - val_precision_focused: 0.4720 - val_recall_5: 0.3667 - learning_rate: 7.3216e-06\n",
      "Epoch 498/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6287 - auc_5: 0.6764 - f05: 0.5168 - loss: 0.5373 - precision_5: 0.6318 - precision_focused: 0.5176 - recall_5: 0.6488\n",
      "Epoch 498: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 498: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6287 - auc_5: 0.6765 - f05: 0.5168 - loss: 0.5373 - precision_5: 0.6318 - precision_focused: 0.5176 - recall_5: 0.6487 - val_accuracy: 0.5076 - val_auc_5: 0.5144 - val_f05: 0.4686 - val_loss: 0.6021 - val_precision_5: 0.5416 - val_precision_focused: 0.4720 - val_recall_5: 0.3654 - learning_rate: 7.3216e-06\n",
      "Epoch 499/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6237 - auc_5: 0.6697 - f05: 0.5139 - loss: 0.5391 - precision_5: 0.6307 - precision_focused: 0.5141 - recall_5: 0.6345\n",
      "Epoch 499: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 499: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.6237 - auc_5: 0.6698 - f05: 0.5139 - loss: 0.5391 - precision_5: 0.6308 - precision_focused: 0.5140 - recall_5: 0.6345 - val_accuracy: 0.5069 - val_auc_5: 0.5147 - val_f05: 0.4645 - val_loss: 0.6025 - val_precision_5: 0.5419 - val_precision_focused: 0.4691 - val_recall_5: 0.3549 - learning_rate: 7.3216e-06\n",
      "Epoch 500/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6285 - auc_5: 0.6770 - f05: 0.5104 - loss: 0.5385 - precision_5: 0.6307 - precision_focused: 0.5107 - recall_5: 0.6375\n",
      "Epoch 500: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 500: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6285 - auc_5: 0.6770 - f05: 0.5105 - loss: 0.5385 - precision_5: 0.6307 - precision_focused: 0.5108 - recall_5: 0.6375 - val_accuracy: 0.5085 - val_auc_5: 0.5148 - val_f05: 0.4599 - val_loss: 0.6027 - val_precision_5: 0.5457 - val_precision_focused: 0.4660 - val_recall_5: 0.3463 - learning_rate: 7.3216e-06\n",
      "Epoch 501/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6200 - auc_5: 0.6688 - f05: 0.5159 - loss: 0.5416 - precision_5: 0.6290 - precision_focused: 0.5158 - recall_5: 0.6276\n",
      "Epoch 501: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 501: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6200 - auc_5: 0.6688 - f05: 0.5158 - loss: 0.5416 - precision_5: 0.6290 - precision_focused: 0.5158 - recall_5: 0.6276 - val_accuracy: 0.5076 - val_auc_5: 0.5150 - val_f05: 0.4606 - val_loss: 0.6023 - val_precision_5: 0.5440 - val_precision_focused: 0.4664 - val_recall_5: 0.3475 - learning_rate: 7.3216e-06\n",
      "Epoch 502/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6261 - auc_5: 0.6707 - f05: 0.5054 - loss: 0.5422 - precision_5: 0.6308 - precision_focused: 0.5051 - recall_5: 0.6223\n",
      "Epoch 502: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 502: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6261 - auc_5: 0.6707 - f05: 0.5054 - loss: 0.5422 - precision_5: 0.6308 - precision_focused: 0.5051 - recall_5: 0.6223 - val_accuracy: 0.5069 - val_auc_5: 0.5147 - val_f05: 0.4598 - val_loss: 0.6020 - val_precision_5: 0.5434 - val_precision_focused: 0.4660 - val_recall_5: 0.3438 - learning_rate: 7.3216e-06\n",
      "Epoch 503/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6230 - auc_5: 0.6777 - f05: 0.5103 - loss: 0.5390 - precision_5: 0.6258 - precision_focused: 0.5106 - recall_5: 0.6316\n",
      "Epoch 503: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 503: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6230 - auc_5: 0.6777 - f05: 0.5103 - loss: 0.5390 - precision_5: 0.6258 - precision_focused: 0.5106 - recall_5: 0.6316 - val_accuracy: 0.5076 - val_auc_5: 0.5148 - val_f05: 0.4590 - val_loss: 0.6019 - val_precision_5: 0.5447 - val_precision_focused: 0.4656 - val_recall_5: 0.3426 - learning_rate: 7.3216e-06\n",
      "Epoch 504/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6360 - auc_5: 0.6839 - f05: 0.5125 - loss: 0.5356 - precision_5: 0.6399 - precision_focused: 0.5129 - recall_5: 0.6468\n",
      "Epoch 504: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 504: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6360 - auc_5: 0.6839 - f05: 0.5125 - loss: 0.5356 - precision_5: 0.6399 - precision_focused: 0.5128 - recall_5: 0.6468 - val_accuracy: 0.5072 - val_auc_5: 0.5146 - val_f05: 0.4582 - val_loss: 0.6027 - val_precision_5: 0.5443 - val_precision_focused: 0.4649 - val_recall_5: 0.3414 - learning_rate: 7.3216e-06\n",
      "Epoch 505/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6373 - auc_5: 0.6848 - f05: 0.5155 - loss: 0.5335 - precision_5: 0.6432 - precision_focused: 0.5158 - recall_5: 0.6499\n",
      "Epoch 505: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 505: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6372 - auc_5: 0.6847 - f05: 0.5155 - loss: 0.5335 - precision_5: 0.6431 - precision_focused: 0.5157 - recall_5: 0.6497 - val_accuracy: 0.5076 - val_auc_5: 0.5147 - val_f05: 0.4559 - val_loss: 0.6028 - val_precision_5: 0.5454 - val_precision_focused: 0.4633 - val_recall_5: 0.3377 - learning_rate: 7.3216e-06\n",
      "Epoch 506/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6198 - auc_5: 0.6724 - f05: 0.5162 - loss: 0.5397 - precision_5: 0.6304 - precision_focused: 0.5162 - recall_5: 0.6289\n",
      "Epoch 506: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 506: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.6198 - auc_5: 0.6724 - f05: 0.5161 - loss: 0.5396 - precision_5: 0.6304 - precision_focused: 0.5162 - recall_5: 0.6290 - val_accuracy: 0.5072 - val_auc_5: 0.5145 - val_f05: 0.4618 - val_loss: 0.6026 - val_precision_5: 0.5432 - val_precision_focused: 0.4673 - val_recall_5: 0.3494 - learning_rate: 7.3216e-06\n",
      "Epoch 507/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6266 - auc_5: 0.6776 - f05: 0.5155 - loss: 0.5366 - precision_5: 0.6356 - precision_focused: 0.5155 - recall_5: 0.6335\n",
      "Epoch 507: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 507: val_precision_focused did not improve from 0.50901\n",
      "\n",
      "Epoch 507: ReduceLROnPlateau reducing learning rate to 5.8572615671437235e-06.\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6266 - auc_5: 0.6776 - f05: 0.5155 - loss: 0.5366 - precision_5: 0.6356 - precision_focused: 0.5155 - recall_5: 0.6335 - val_accuracy: 0.5089 - val_auc_5: 0.5148 - val_f05: 0.4590 - val_loss: 0.6023 - val_precision_5: 0.5463 - val_precision_focused: 0.4651 - val_recall_5: 0.3463 - learning_rate: 7.3216e-06\n",
      "Epoch 508/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6252 - auc_5: 0.6764 - f05: 0.5099 - loss: 0.5383 - precision_5: 0.6289 - precision_focused: 0.5100 - recall_5: 0.6297\n",
      "Epoch 508: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 508: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6252 - auc_5: 0.6765 - f05: 0.5099 - loss: 0.5383 - precision_5: 0.6290 - precision_focused: 0.5100 - recall_5: 0.6297 - val_accuracy: 0.5114 - val_auc_5: 0.5148 - val_f05: 0.4576 - val_loss: 0.6027 - val_precision_5: 0.5505 - val_precision_focused: 0.4641 - val_recall_5: 0.3463 - learning_rate: 5.8573e-06\n",
      "Epoch 509/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - auc_5: 0.6781 - f05: 0.5062 - loss: 0.5391 - precision_5: 0.6336 - precision_focused: 0.5060 - recall_5: 0.6279\n",
      "Epoch 509: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 509: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6302 - auc_5: 0.6781 - f05: 0.5062 - loss: 0.5391 - precision_5: 0.6336 - precision_focused: 0.5060 - recall_5: 0.6279 - val_accuracy: 0.5118 - val_auc_5: 0.5148 - val_f05: 0.4610 - val_loss: 0.6024 - val_precision_5: 0.5499 - val_precision_focused: 0.4665 - val_recall_5: 0.3537 - learning_rate: 5.8573e-06\n",
      "Epoch 510/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6416 - auc_5: 0.6852 - f05: 0.5117 - loss: 0.5368 - precision_5: 0.6472 - precision_focused: 0.5118 - recall_5: 0.6491\n",
      "Epoch 510: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 510: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.6416 - auc_5: 0.6851 - f05: 0.5117 - loss: 0.5368 - precision_5: 0.6471 - precision_focused: 0.5118 - recall_5: 0.6490 - val_accuracy: 0.5069 - val_auc_5: 0.5153 - val_f05: 0.4540 - val_loss: 0.6035 - val_precision_5: 0.5447 - val_precision_focused: 0.4618 - val_recall_5: 0.3346 - learning_rate: 5.8573e-06\n",
      "Epoch 511/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6345 - auc_5: 0.6810 - f05: 0.5072 - loss: 0.5383 - precision_5: 0.6422 - precision_focused: 0.5067 - recall_5: 0.6260\n",
      "Epoch 511: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 511: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.6345 - auc_5: 0.6810 - f05: 0.5072 - loss: 0.5382 - precision_5: 0.6422 - precision_focused: 0.5067 - recall_5: 0.6261 - val_accuracy: 0.5092 - val_auc_5: 0.5152 - val_f05: 0.4615 - val_loss: 0.6032 - val_precision_5: 0.5461 - val_precision_focused: 0.4669 - val_recall_5: 0.3512 - learning_rate: 5.8573e-06\n",
      "Epoch 512/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6375 - auc_5: 0.6891 - f05: 0.5080 - loss: 0.5319 - precision_5: 0.6438 - precision_focused: 0.5078 - recall_5: 0.6356\n",
      "Epoch 512: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 512: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6375 - auc_5: 0.6890 - f05: 0.5081 - loss: 0.5319 - precision_5: 0.6437 - precision_focused: 0.5078 - recall_5: 0.6356 - val_accuracy: 0.5095 - val_auc_5: 0.5153 - val_f05: 0.4602 - val_loss: 0.6030 - val_precision_5: 0.5470 - val_precision_focused: 0.4661 - val_recall_5: 0.3488 - learning_rate: 5.8573e-06\n",
      "Epoch 513/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6225 - auc_5: 0.6733 - f05: 0.5207 - loss: 0.5378 - precision_5: 0.6321 - precision_focused: 0.5212 - recall_5: 0.6410\n",
      "Epoch 513: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 513: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.6225 - auc_5: 0.6733 - f05: 0.5207 - loss: 0.5378 - precision_5: 0.6321 - precision_focused: 0.5212 - recall_5: 0.6409 - val_accuracy: 0.5092 - val_auc_5: 0.5151 - val_f05: 0.4584 - val_loss: 0.6035 - val_precision_5: 0.5468 - val_precision_focused: 0.4646 - val_recall_5: 0.3463 - learning_rate: 5.8573e-06\n",
      "Epoch 514/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6322 - auc_5: 0.6794 - f05: 0.5156 - loss: 0.5365 - precision_5: 0.6411 - precision_focused: 0.5155 - recall_5: 0.6391\n",
      "Epoch 514: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 514: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.6322 - auc_5: 0.6794 - f05: 0.5155 - loss: 0.5365 - precision_5: 0.6411 - precision_focused: 0.5155 - recall_5: 0.6391 - val_accuracy: 0.5063 - val_auc_5: 0.5154 - val_f05: 0.4582 - val_loss: 0.6036 - val_precision_5: 0.5424 - val_precision_focused: 0.4644 - val_recall_5: 0.3432 - learning_rate: 5.8573e-06\n",
      "Epoch 515/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6351 - auc_5: 0.6843 - f05: 0.5094 - loss: 0.5346 - precision_5: 0.6396 - precision_focused: 0.5095 - recall_5: 0.6397\n",
      "Epoch 515: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 515: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6351 - auc_5: 0.6843 - f05: 0.5094 - loss: 0.5346 - precision_5: 0.6396 - precision_focused: 0.5095 - recall_5: 0.6397 - val_accuracy: 0.5076 - val_auc_5: 0.5152 - val_f05: 0.4592 - val_loss: 0.6033 - val_precision_5: 0.5440 - val_precision_focused: 0.4651 - val_recall_5: 0.3469 - learning_rate: 5.8573e-06\n",
      "Epoch 516/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6269 - auc_5: 0.6817 - f05: 0.5111 - loss: 0.5359 - precision_5: 0.6304 - precision_focused: 0.5114 - recall_5: 0.6347\n",
      "Epoch 516: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 516: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.6269 - auc_5: 0.6817 - f05: 0.5111 - loss: 0.5359 - precision_5: 0.6304 - precision_focused: 0.5114 - recall_5: 0.6347 - val_accuracy: 0.5069 - val_auc_5: 0.5155 - val_f05: 0.4601 - val_loss: 0.6036 - val_precision_5: 0.5427 - val_precision_focused: 0.4656 - val_recall_5: 0.3488 - learning_rate: 5.8573e-06\n",
      "Epoch 517/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6316 - auc_5: 0.6829 - f05: 0.5113 - loss: 0.5355 - precision_5: 0.6370 - precision_focused: 0.5114 - recall_5: 0.6390\n",
      "Epoch 517: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 517: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6316 - auc_5: 0.6829 - f05: 0.5113 - loss: 0.5355 - precision_5: 0.6370 - precision_focused: 0.5114 - recall_5: 0.6390 - val_accuracy: 0.5060 - val_auc_5: 0.5152 - val_f05: 0.4608 - val_loss: 0.6036 - val_precision_5: 0.5410 - val_precision_focused: 0.4660 - val_recall_5: 0.3500 - learning_rate: 5.8573e-06\n",
      "Epoch 518/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6336 - auc_5: 0.6786 - f05: 0.5084 - loss: 0.5378 - precision_5: 0.6360 - precision_focused: 0.5086 - recall_5: 0.6388\n",
      "Epoch 518: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 518: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6336 - auc_5: 0.6786 - f05: 0.5085 - loss: 0.5378 - precision_5: 0.6360 - precision_focused: 0.5086 - recall_5: 0.6388 - val_accuracy: 0.5089 - val_auc_5: 0.5153 - val_f05: 0.4643 - val_loss: 0.6033 - val_precision_5: 0.5444 - val_precision_focused: 0.4686 - val_recall_5: 0.3599 - learning_rate: 5.8573e-06\n",
      "Epoch 519/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6354 - auc_5: 0.6826 - f05: 0.5182 - loss: 0.5349 - precision_5: 0.6407 - precision_focused: 0.5186 - recall_5: 0.6498\n",
      "Epoch 519: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 519: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6354 - auc_5: 0.6826 - f05: 0.5182 - loss: 0.5349 - precision_5: 0.6406 - precision_focused: 0.5186 - recall_5: 0.6498 - val_accuracy: 0.5063 - val_auc_5: 0.5153 - val_f05: 0.4616 - val_loss: 0.6035 - val_precision_5: 0.5414 - val_precision_focused: 0.4666 - val_recall_5: 0.3512 - learning_rate: 5.8573e-06\n",
      "Epoch 520/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6229 - auc_5: 0.6744 - f05: 0.5144 - loss: 0.5383 - precision_5: 0.6306 - precision_focused: 0.5143 - recall_5: 0.6281\n",
      "Epoch 520: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 520: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6229 - auc_5: 0.6744 - f05: 0.5144 - loss: 0.5383 - precision_5: 0.6306 - precision_focused: 0.5143 - recall_5: 0.6281 - val_accuracy: 0.5066 - val_auc_5: 0.5152 - val_f05: 0.4616 - val_loss: 0.6029 - val_precision_5: 0.5418 - val_precision_focused: 0.4666 - val_recall_5: 0.3519 - learning_rate: 5.8573e-06\n",
      "Epoch 521/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6396 - auc_5: 0.6893 - f05: 0.5133 - loss: 0.5316 - precision_5: 0.6436 - precision_focused: 0.5138 - recall_5: 0.6553\n",
      "Epoch 521: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 521: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6395 - auc_5: 0.6892 - f05: 0.5133 - loss: 0.5317 - precision_5: 0.6435 - precision_focused: 0.5138 - recall_5: 0.6551 - val_accuracy: 0.5079 - val_auc_5: 0.5153 - val_f05: 0.4607 - val_loss: 0.6030 - val_precision_5: 0.5441 - val_precision_focused: 0.4661 - val_recall_5: 0.3506 - learning_rate: 5.8573e-06\n",
      "Epoch 522/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6350 - auc_5: 0.6829 - f05: 0.5160 - loss: 0.5354 - precision_5: 0.6393 - precision_focused: 0.5165 - recall_5: 0.6497\n",
      "Epoch 522: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 522: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6350 - auc_5: 0.6829 - f05: 0.5159 - loss: 0.5354 - precision_5: 0.6393 - precision_focused: 0.5164 - recall_5: 0.6497 - val_accuracy: 0.5066 - val_auc_5: 0.5150 - val_f05: 0.4615 - val_loss: 0.6033 - val_precision_5: 0.5419 - val_precision_focused: 0.4666 - val_recall_5: 0.3512 - learning_rate: 5.8573e-06\n",
      "Epoch 523/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6300 - auc_5: 0.6778 - f05: 0.5106 - loss: 0.5390 - precision_5: 0.6316 - precision_focused: 0.5110 - recall_5: 0.6375\n",
      "Epoch 523: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 523: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6299 - auc_5: 0.6778 - f05: 0.5106 - loss: 0.5390 - precision_5: 0.6317 - precision_focused: 0.5110 - recall_5: 0.6375 - val_accuracy: 0.5092 - val_auc_5: 0.5150 - val_f05: 0.4644 - val_loss: 0.6024 - val_precision_5: 0.5449 - val_precision_focused: 0.4687 - val_recall_5: 0.3599 - learning_rate: 5.8573e-06\n",
      "Epoch 524/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6274 - auc_5: 0.6845 - f05: 0.5195 - loss: 0.5350 - precision_5: 0.6308 - precision_focused: 0.5205 - recall_5: 0.6512\n",
      "Epoch 524: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 524: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6274 - auc_5: 0.6845 - f05: 0.5195 - loss: 0.5350 - precision_5: 0.6309 - precision_focused: 0.5204 - recall_5: 0.6511 - val_accuracy: 0.5082 - val_auc_5: 0.5150 - val_f05: 0.4642 - val_loss: 0.6027 - val_precision_5: 0.5434 - val_precision_focused: 0.4684 - val_recall_5: 0.3593 - learning_rate: 5.8573e-06\n",
      "Epoch 525/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6379 - auc_5: 0.6906 - f05: 0.5135 - loss: 0.5313 - precision_5: 0.6462 - precision_focused: 0.5134 - recall_5: 0.6432\n",
      "Epoch 525: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 525: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6379 - auc_5: 0.6905 - f05: 0.5135 - loss: 0.5313 - precision_5: 0.6462 - precision_focused: 0.5134 - recall_5: 0.6432 - val_accuracy: 0.5066 - val_auc_5: 0.5149 - val_f05: 0.4579 - val_loss: 0.6036 - val_precision_5: 0.5429 - val_precision_focused: 0.4641 - val_recall_5: 0.3438 - learning_rate: 5.8573e-06\n",
      "Epoch 526/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6341 - auc_5: 0.6853 - f05: 0.5159 - loss: 0.5326 - precision_5: 0.6441 - precision_focused: 0.5158 - recall_5: 0.6414\n",
      "Epoch 526: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 526: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6342 - auc_5: 0.6854 - f05: 0.5159 - loss: 0.5325 - precision_5: 0.6441 - precision_focused: 0.5158 - recall_5: 0.6415 - val_accuracy: 0.5092 - val_auc_5: 0.5146 - val_f05: 0.4635 - val_loss: 0.6035 - val_precision_5: 0.5450 - val_precision_focused: 0.4679 - val_recall_5: 0.3586 - learning_rate: 5.8573e-06\n",
      "Epoch 527/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6276 - auc_5: 0.6734 - f05: 0.5172 - loss: 0.5382 - precision_5: 0.6372 - precision_focused: 0.5171 - recall_5: 0.6358\n",
      "Epoch 527: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 527: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6276 - auc_5: 0.6734 - f05: 0.5171 - loss: 0.5382 - precision_5: 0.6372 - precision_focused: 0.5171 - recall_5: 0.6358 - val_accuracy: 0.5085 - val_auc_5: 0.5149 - val_f05: 0.4602 - val_loss: 0.6041 - val_precision_5: 0.5451 - val_precision_focused: 0.4657 - val_recall_5: 0.3506 - learning_rate: 5.8573e-06\n",
      "Epoch 528/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6321 - auc_5: 0.6814 - f05: 0.5147 - loss: 0.5352 - precision_5: 0.6406 - precision_focused: 0.5146 - recall_5: 0.6392\n",
      "Epoch 528: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 528: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6321 - auc_5: 0.6814 - f05: 0.5146 - loss: 0.5352 - precision_5: 0.6406 - precision_focused: 0.5146 - recall_5: 0.6392 - val_accuracy: 0.5063 - val_auc_5: 0.5147 - val_f05: 0.4602 - val_loss: 0.6041 - val_precision_5: 0.5419 - val_precision_focused: 0.4658 - val_recall_5: 0.3475 - learning_rate: 5.8573e-06\n",
      "Epoch 529/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6228 - auc_5: 0.6781 - f05: 0.5143 - loss: 0.5367 - precision_5: 0.6300 - precision_focused: 0.5144 - recall_5: 0.6313\n",
      "Epoch 529: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 529: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6228 - auc_5: 0.6781 - f05: 0.5143 - loss: 0.5367 - precision_5: 0.6300 - precision_focused: 0.5144 - recall_5: 0.6313 - val_accuracy: 0.5066 - val_auc_5: 0.5151 - val_f05: 0.4569 - val_loss: 0.6042 - val_precision_5: 0.5434 - val_precision_focused: 0.4636 - val_recall_5: 0.3401 - learning_rate: 5.8573e-06\n",
      "Epoch 530/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6342 - auc_5: 0.6822 - f05: 0.5199 - loss: 0.5356 - precision_5: 0.6445 - precision_focused: 0.5199 - recall_5: 0.6434\n",
      "Epoch 530: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 530: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6341 - auc_5: 0.6822 - f05: 0.5198 - loss: 0.5356 - precision_5: 0.6444 - precision_focused: 0.5198 - recall_5: 0.6433 - val_accuracy: 0.5060 - val_auc_5: 0.5154 - val_f05: 0.4540 - val_loss: 0.6049 - val_precision_5: 0.5432 - val_precision_focused: 0.4617 - val_recall_5: 0.3340 - learning_rate: 5.8573e-06\n",
      "Epoch 531/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6324 - auc_5: 0.6824 - f05: 0.5059 - loss: 0.5364 - precision_5: 0.6367 - precision_focused: 0.5056 - recall_5: 0.6273\n",
      "Epoch 531: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 531: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6324 - auc_5: 0.6824 - f05: 0.5059 - loss: 0.5364 - precision_5: 0.6367 - precision_focused: 0.5056 - recall_5: 0.6273 - val_accuracy: 0.5050 - val_auc_5: 0.5149 - val_f05: 0.4624 - val_loss: 0.6036 - val_precision_5: 0.5393 - val_precision_focused: 0.4672 - val_recall_5: 0.3519 - learning_rate: 5.8573e-06\n",
      "Epoch 532/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6367 - auc_5: 0.6850 - f05: 0.5126 - loss: 0.5357 - precision_5: 0.6384 - precision_focused: 0.5134 - recall_5: 0.6546\n",
      "Epoch 532: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 532: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6367 - auc_5: 0.6850 - f05: 0.5126 - loss: 0.5357 - precision_5: 0.6384 - precision_focused: 0.5134 - recall_5: 0.6545 - val_accuracy: 0.5053 - val_auc_5: 0.5154 - val_f05: 0.4561 - val_loss: 0.6040 - val_precision_5: 0.5416 - val_precision_focused: 0.4631 - val_recall_5: 0.3377 - learning_rate: 5.8573e-06\n",
      "Epoch 533/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6334 - auc_5: 0.6857 - f05: 0.5196 - loss: 0.5326 - precision_5: 0.6454 - precision_focused: 0.5194 - recall_5: 0.6396\n",
      "Epoch 533: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 533: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.6334 - auc_5: 0.6857 - f05: 0.5195 - loss: 0.5326 - precision_5: 0.6454 - precision_focused: 0.5194 - recall_5: 0.6395 - val_accuracy: 0.5047 - val_auc_5: 0.5155 - val_f05: 0.4585 - val_loss: 0.6038 - val_precision_5: 0.5398 - val_precision_focused: 0.4645 - val_recall_5: 0.3432 - learning_rate: 5.8573e-06\n",
      "Epoch 534/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6257 - auc_5: 0.6776 - f05: 0.5108 - loss: 0.5375 - precision_5: 0.6325 - precision_focused: 0.5106 - recall_5: 0.6289\n",
      "Epoch 534: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 534: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6258 - auc_5: 0.6776 - f05: 0.5108 - loss: 0.5375 - precision_5: 0.6326 - precision_focused: 0.5107 - recall_5: 0.6290 - val_accuracy: 0.5066 - val_auc_5: 0.5151 - val_f05: 0.4619 - val_loss: 0.6031 - val_precision_5: 0.5417 - val_precision_focused: 0.4669 - val_recall_5: 0.3525 - learning_rate: 5.8573e-06\n",
      "Epoch 535/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6200 - auc_5: 0.6744 - f05: 0.5156 - loss: 0.5361 - precision_5: 0.6259 - precision_focused: 0.5160 - recall_5: 0.6356\n",
      "Epoch 535: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 535: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6201 - auc_5: 0.6745 - f05: 0.5155 - loss: 0.5361 - precision_5: 0.6259 - precision_focused: 0.5159 - recall_5: 0.6356 - val_accuracy: 0.5066 - val_auc_5: 0.5153 - val_f05: 0.4626 - val_loss: 0.6031 - val_precision_5: 0.5415 - val_precision_focused: 0.4673 - val_recall_5: 0.3543 - learning_rate: 5.8573e-06\n",
      "Epoch 536/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6356 - auc_5: 0.6842 - f05: 0.5114 - loss: 0.5357 - precision_5: 0.6371 - precision_focused: 0.5119 - recall_5: 0.6493\n",
      "Epoch 536: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 536: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6356 - auc_5: 0.6842 - f05: 0.5114 - loss: 0.5357 - precision_5: 0.6371 - precision_focused: 0.5119 - recall_5: 0.6493 - val_accuracy: 0.5053 - val_auc_5: 0.5156 - val_f05: 0.4607 - val_loss: 0.6032 - val_precision_5: 0.5402 - val_precision_focused: 0.4661 - val_recall_5: 0.3481 - learning_rate: 5.8573e-06\n",
      "Epoch 537/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6374 - auc_5: 0.6916 - f05: 0.5103 - loss: 0.5320 - precision_5: 0.6357 - precision_focused: 0.5110 - recall_5: 0.6525\n",
      "Epoch 537: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 537: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6374 - auc_5: 0.6916 - f05: 0.5103 - loss: 0.5320 - precision_5: 0.6357 - precision_focused: 0.5110 - recall_5: 0.6524 - val_accuracy: 0.5043 - val_auc_5: 0.5157 - val_f05: 0.4586 - val_loss: 0.6034 - val_precision_5: 0.5394 - val_precision_focused: 0.4647 - val_recall_5: 0.3426 - learning_rate: 5.8573e-06\n",
      "Epoch 538/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6301 - auc_5: 0.6853 - f05: 0.5158 - loss: 0.5337 - precision_5: 0.6346 - precision_focused: 0.5165 - recall_5: 0.6493\n",
      "Epoch 538: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 538: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.6301 - auc_5: 0.6853 - f05: 0.5158 - loss: 0.5338 - precision_5: 0.6346 - precision_focused: 0.5165 - recall_5: 0.6492 - val_accuracy: 0.5072 - val_auc_5: 0.5156 - val_f05: 0.4587 - val_loss: 0.6035 - val_precision_5: 0.5437 - val_precision_focused: 0.4647 - val_recall_5: 0.3457 - learning_rate: 5.8573e-06\n",
      "Epoch 539/550\n",
      "\u001b[1m391/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6325 - auc_5: 0.6928 - f05: 0.5140 - loss: 0.5303 - precision_5: 0.6352 - precision_focused: 0.5143 - recall_5: 0.6424\n",
      "Epoch 539: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 539: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6325 - auc_5: 0.6927 - f05: 0.5139 - loss: 0.5303 - precision_5: 0.6352 - precision_focused: 0.5143 - recall_5: 0.6423 - val_accuracy: 0.5085 - val_auc_5: 0.5152 - val_f05: 0.4626 - val_loss: 0.6033 - val_precision_5: 0.5444 - val_precision_focused: 0.4674 - val_recall_5: 0.3556 - learning_rate: 5.8573e-06\n",
      "Epoch 540/550\n",
      "\u001b[1m390/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6394 - auc_5: 0.6843 - f05: 0.5139 - loss: 0.5359 - precision_5: 0.6415 - precision_focused: 0.5146 - recall_5: 0.6577\n",
      "Epoch 540: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 540: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6392 - auc_5: 0.6842 - f05: 0.5139 - loss: 0.5359 - precision_5: 0.6414 - precision_focused: 0.5146 - recall_5: 0.6575 - val_accuracy: 0.5056 - val_auc_5: 0.5160 - val_f05: 0.4567 - val_loss: 0.6037 - val_precision_5: 0.5420 - val_precision_focused: 0.4635 - val_recall_5: 0.3389 - learning_rate: 5.8573e-06\n",
      "Epoch 541/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6206 - auc_5: 0.6725 - f05: 0.5159 - loss: 0.5398 - precision_5: 0.6260 - precision_focused: 0.5163 - recall_5: 0.6331\n",
      "Epoch 541: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 541: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6207 - auc_5: 0.6727 - f05: 0.5158 - loss: 0.5397 - precision_5: 0.6261 - precision_focused: 0.5162 - recall_5: 0.6331 - val_accuracy: 0.5056 - val_auc_5: 0.5161 - val_f05: 0.4551 - val_loss: 0.6040 - val_precision_5: 0.5423 - val_precision_focused: 0.4624 - val_recall_5: 0.3364 - learning_rate: 5.8573e-06\n",
      "Epoch 542/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6210 - auc_5: 0.6723 - f05: 0.5258 - loss: 0.5363 - precision_5: 0.6382 - precision_focused: 0.5256 - recall_5: 0.6318\n",
      "Epoch 542: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 542: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6210 - auc_5: 0.6723 - f05: 0.5257 - loss: 0.5363 - precision_5: 0.6382 - precision_focused: 0.5256 - recall_5: 0.6318 - val_accuracy: 0.5079 - val_auc_5: 0.5161 - val_f05: 0.4569 - val_loss: 0.6035 - val_precision_5: 0.5454 - val_precision_focused: 0.4637 - val_recall_5: 0.3414 - learning_rate: 5.8573e-06\n",
      "Epoch 543/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6321 - auc_5: 0.6838 - f05: 0.5034 - loss: 0.5354 - precision_5: 0.6322 - precision_focused: 0.5036 - recall_5: 0.6342\n",
      "Epoch 543: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 543: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6321 - auc_5: 0.6838 - f05: 0.5035 - loss: 0.5354 - precision_5: 0.6322 - precision_focused: 0.5036 - recall_5: 0.6341 - val_accuracy: 0.5082 - val_auc_5: 0.5161 - val_f05: 0.4571 - val_loss: 0.6036 - val_precision_5: 0.5459 - val_precision_focused: 0.4639 - val_recall_5: 0.3414 - learning_rate: 5.8573e-06\n",
      "Epoch 544/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6382 - auc_5: 0.6876 - f05: 0.5177 - loss: 0.5330 - precision_5: 0.6470 - precision_focused: 0.5178 - recall_5: 0.6466\n",
      "Epoch 544: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 544: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.6382 - auc_5: 0.6876 - f05: 0.5177 - loss: 0.5330 - precision_5: 0.6469 - precision_focused: 0.5178 - recall_5: 0.6466 - val_accuracy: 0.5085 - val_auc_5: 0.5162 - val_f05: 0.4581 - val_loss: 0.6037 - val_precision_5: 0.5460 - val_precision_focused: 0.4645 - val_recall_5: 0.3444 - learning_rate: 5.8573e-06\n",
      "Epoch 545/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6351 - auc_5: 0.6789 - f05: 0.5140 - loss: 0.5371 - precision_5: 0.6440 - precision_focused: 0.5141 - recall_5: 0.6435\n",
      "Epoch 545: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 545: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6351 - auc_5: 0.6790 - f05: 0.5140 - loss: 0.5371 - precision_5: 0.6440 - precision_focused: 0.5141 - recall_5: 0.6436 - val_accuracy: 0.5082 - val_auc_5: 0.5157 - val_f05: 0.4599 - val_loss: 0.6040 - val_precision_5: 0.5449 - val_precision_focused: 0.4657 - val_recall_5: 0.3481 - learning_rate: 5.8573e-06\n",
      "Epoch 546/550\n",
      "\u001b[1m394/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6374 - auc_5: 0.6820 - f05: 0.5169 - loss: 0.5347 - precision_5: 0.6454 - precision_focused: 0.5170 - recall_5: 0.6468\n",
      "Epoch 546: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 546: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6374 - auc_5: 0.6820 - f05: 0.5169 - loss: 0.5347 - precision_5: 0.6454 - precision_focused: 0.5169 - recall_5: 0.6468 - val_accuracy: 0.5085 - val_auc_5: 0.5153 - val_f05: 0.4614 - val_loss: 0.6046 - val_precision_5: 0.5448 - val_precision_focused: 0.4666 - val_recall_5: 0.3525 - learning_rate: 5.8573e-06\n",
      "Epoch 547/550\n",
      "\u001b[1m389/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6368 - auc_5: 0.6831 - f05: 0.5259 - loss: 0.5338 - precision_5: 0.6467 - precision_focused: 0.5265 - recall_5: 0.6582\n",
      "Epoch 547: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 547: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6368 - auc_5: 0.6831 - f05: 0.5257 - loss: 0.5338 - precision_5: 0.6466 - precision_focused: 0.5262 - recall_5: 0.6581 - val_accuracy: 0.5089 - val_auc_5: 0.5154 - val_f05: 0.4595 - val_loss: 0.6049 - val_precision_5: 0.5460 - val_precision_focused: 0.4654 - val_recall_5: 0.3481 - learning_rate: 5.8573e-06\n",
      "Epoch 548/550\n",
      "\u001b[1m392/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6327 - auc_5: 0.6806 - f05: 0.5056 - loss: 0.5389 - precision_5: 0.6302 - precision_focused: 0.5060 - recall_5: 0.6396\n",
      "Epoch 548: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 548: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6327 - auc_5: 0.6806 - f05: 0.5057 - loss: 0.5389 - precision_5: 0.6303 - precision_focused: 0.5061 - recall_5: 0.6397 - val_accuracy: 0.5085 - val_auc_5: 0.5156 - val_f05: 0.4573 - val_loss: 0.6046 - val_precision_5: 0.5463 - val_precision_focused: 0.4640 - val_recall_5: 0.3426 - learning_rate: 5.8573e-06\n",
      "Epoch 549/550\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6387 - auc_5: 0.6885 - f05: 0.5159 - loss: 0.5313 - precision_5: 0.6456 - precision_focused: 0.5161 - recall_5: 0.6513\n",
      "Epoch 549: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 549: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6387 - auc_5: 0.6885 - f05: 0.5159 - loss: 0.5314 - precision_5: 0.6456 - precision_focused: 0.5161 - recall_5: 0.6512 - val_accuracy: 0.5076 - val_auc_5: 0.5157 - val_f05: 0.4575 - val_loss: 0.6047 - val_precision_5: 0.5447 - val_precision_focused: 0.4641 - val_recall_5: 0.3426 - learning_rate: 5.8573e-06\n",
      "Epoch 550/550\n",
      "\u001b[1m393/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6331 - auc_5: 0.6773 - f05: 0.5198 - loss: 0.5353 - precision_5: 0.6451 - precision_focused: 0.5197 - recall_5: 0.6432\n",
      "Epoch 550: val_f05 did not improve from 0.50772\n",
      "\n",
      "Epoch 550: val_precision_focused did not improve from 0.50901\n",
      "\u001b[1m395/395\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.6331 - auc_5: 0.6774 - f05: 0.5197 - loss: 0.5353 - precision_5: 0.6451 - precision_focused: 0.5196 - recall_5: 0.6432 - val_accuracy: 0.5069 - val_auc_5: 0.5155 - val_f05: 0.4595 - val_loss: 0.6044 - val_precision_5: 0.5430 - val_precision_focused: 0.4652 - val_recall_5: 0.3469 - learning_rate: 5.8573e-06\n",
      "ðŸ” Evaluating training set...\n",
      "ðŸ” Evaluating current validation set...\n",
      "\n",
      "ðŸ“ˆ Current Model Training Results:\n",
      "   Precision: 0.6757\n",
      "   Recall:    0.7427\n",
      "   F1:        0.7076\n",
      "   F0.5:      0.6881\n",
      "   Accuracy:  0.6885\n",
      "   AUC:       0.7598\n",
      "\n",
      "ðŸ“Š Current Model Validation Results:\n",
      "   Precision: 0.5430\n",
      "   Recall:    0.3469\n",
      "   F1:        0.4234\n",
      "   F0.5:      0.4878\n",
      "   Accuracy:  0.5069\n",
      "   AUC:       0.5154\n",
      "ðŸ” Evaluating Best Precision model...\n",
      "   ðŸ“Š Best Precision Results:\n",
      "      Precision: 0.5430\n",
      "      Recall:    0.3469\n",
      "      F1:        0.4234\n",
      "      F0.5:      0.4878\n",
      "      Accuracy:  0.5069\n",
      "      AUC:       0.5154\n",
      "ðŸ” Evaluating Best F0.5 model...\n",
      "   ðŸ“Š Best F0.5 Results:\n",
      "      Precision: 0.5227\n",
      "      Recall:    0.5463\n",
      "      F1:        0.5343\n",
      "      F0.5:      0.5273\n",
      "      Accuracy:  0.5031\n",
      "      AUC:       0.5091\n",
      "ðŸ” Evaluating Best Precision-Focused model...\n",
      "   ðŸ“Š Best Precision-Focused Results:\n",
      "      Precision: 0.5227\n",
      "      Recall:    0.5463\n",
      "      F1:        0.5343\n",
      "      F0.5:      0.5273\n",
      "      Accuracy:  0.5031\n",
      "      AUC:       0.5091\n",
      "\n",
      "ðŸ† MODELS COMPARISON:\n",
      "ðŸŽ¯ Highest Precision-Focused Score: Best F0.5\n",
      "   Score: 0.5286\n",
      "   Precision: 0.5227\n",
      "   Recall: 0.5463\n",
      "   F0.5: 0.5273\n",
      "\n",
      "ðŸ“Š For comparison:\n",
      "   Highest Precision Only: Best Precision (P:0.5430, R:0.3469)\n",
      "   Highest F0.5: Best F0.5 (P:0.5227, R:0.5463)\n",
      "   Highest Recall: Best F0.5 (P:0.5227, R:0.5463)\n",
      "\n",
      "âœ… SELECTED: Best F0.5\n",
      "   Reason: Highest precision-focused score (3Ã—P + R)/4 = 0.5286\n",
      "   This balances high precision (0.5227) with reasonable recall (0.5463)\n",
      "ðŸ“ˆ Creating training plots â€¦\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'precision_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 491\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Precision plot\u001b[39;00m\n\u001b[0;32m    490\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m    492\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_precision_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m    493\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'precision_1'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  CNNâ€“LSTM  (80-20 split)  â€“  Optimise F0Â·5  â€“  Export validation CSV\n",
    "#  NO EARLY STOPPING - UNLIMITED TRAINING VERSION\n",
    "# =============================================================================\n",
    "import os, json, warnings, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             fbeta_score, accuracy_score, roc_curve, auc)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, ReduceLROnPlateau)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# â”€â”€ Optuna-best hyper-params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BEST_PARAMS = {\n",
    "    \"arch\":        \"conv_lstm\",\n",
    "    \"window\":      66,\n",
    "    \"filters\":     32,\n",
    "    \"kernel\":      5,\n",
    "    \"units\":       32,\n",
    "    \"conv_drop\":   0.1990,\n",
    "    \"lstm_drop\":   0.0056,\n",
    "    \"dense\":       32,\n",
    "    \"dropout\":     0.3674,\n",
    "    \"l2\":          1.73e-06,\n",
    "    \"lr\":          5.455e-05,\n",
    "    \"batch\":       32,\n",
    "    \"act\":         \"relu\",\n",
    "    \"conv_blocks\": 2,\n",
    "    \"optim\":       \"nadam\",\n",
    "}\n",
    "\n",
    "# â”€â”€ paths & columns to drop (truncate if you like) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open','high','low','typical_price','EMA_21','SMA_20','vwap_24h','close_4h',\n",
    "    'bollinger_upper','bollinger_lower','resistance_level','support_level',\n",
    "    'high_low','high_close','low_close','true_range','volume_mean_20',\n",
    "    'MACD_line','MACD_signal','volatility_regime','trending_market',\n",
    "    'above_sma50','ema7_above_ema21','rsi_overbought','stoch_oversold',\n",
    "    'cci_oversold','vol_spike_1_5x','near_upper_band','near_lower_band',\n",
    "    'break_upper_band','break_lower_band','rsi_oversold','above_sma20',\n",
    "    'macd_positive','volume_breakout','volume_breakdown','bullish_scenario_1',\n",
    "    'bullish_scenario_2','bullish_scenario_3','bullish_scenario_4',\n",
    "    'bullish_scenario_5','bullish_scenario_6','bearish_scenario_1',\n",
    "    'bearish_scenario_2','bearish_scenario_3','bearish_scenario_4',\n",
    "    'bearish_scenario_6','ema_cross_up','macd_cross_up','oversold_reversal',\n",
    "    'overbought_reversal','close'\n",
    "]\n",
    "\n",
    "# â”€â”€ run parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED       = 42\n",
    "VAL_FRAC   = 0.20\n",
    "BETA       = 0.5\n",
    "THRESHOLD  = 0.5\n",
    "EPOCHS     = 550  # Much longer training\n",
    "PATIENCE   = None  # No early stopping!\n",
    "LR_PATIENCE = 50   # More patience for LR reduction\n",
    "\n",
    "# Create output directory FIRST\n",
    "OUT_DIR = Path(\"model_outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)  # Create parent directories too\n",
    "STAMP   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception: pass\n",
    "\n",
    "# â”€â”€ load & clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Loading data â€¦\")\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "\n",
    "# Filter DROP_COLS to only include columns that actually exist in df\n",
    "existing_drop_cols = [c for c in DROP_COLS if c in df.columns]\n",
    "df = df.drop(columns=existing_drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values.astype(np.float32)\n",
    "y_raw = df[\"target\"].astype(np.int8).values\n",
    "n_feat = X_raw.shape[1]\n",
    "\n",
    "print(f\"Data shape: {df.shape}, Features: {n_feat}\")\n",
    "\n",
    "# â”€â”€ 80-20 chronological split (no test slice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_start = int(len(df) * (1 - VAL_FRAC))\n",
    "train_idx = np.arange(0, val_start)\n",
    "val_idx   = np.arange(val_start, len(df))\n",
    "\n",
    "scaler = StandardScaler().fit(X_raw[train_idx])\n",
    "joblib.dump(scaler, OUT_DIR / f\"scaler_{STAMP}.pkl\")\n",
    "\n",
    "X_train = scaler.transform(X_raw[train_idx])\n",
    "X_val   = scaler.transform(X_raw[val_idx])\n",
    "y_train, y_val = y_raw[train_idx], y_raw[val_idx]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
    "\n",
    "# â”€â”€ class weighting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pos_rate = y_train.mean()\n",
    "# Stronger emphasis on precision - reduce positive class weight\n",
    "W0, W1 = 1.0, ((1 - pos_rate) / pos_rate * 0.7) if pos_rate > 0 else 1.0  # Reduced by 30%\n",
    "print(f\"Positive rate: {pos_rate:.3f}, Class weights: W0={W0:.3f}, W1={W1:.3f} (precision-focused)\")\n",
    "\n",
    "# â”€â”€ window helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, w: int) -> Tuple[np.ndarray,np.ndarray]:\n",
    "    xs, ys = [], []\n",
    "    for i in range(w, len(X)):\n",
    "        xs.append(X[i-w:i]); ys.append(y[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "WIN = BEST_PARAMS[\"window\"]\n",
    "X_tr_w, y_tr_w = make_windows(X_train, y_train, WIN)\n",
    "X_va_w, y_va_w = make_windows(X_val,   y_val,   WIN)\n",
    "\n",
    "print(f\"Windowed train shape: {X_tr_w.shape}, val shape: {X_va_w.shape}\")\n",
    "\n",
    "# â”€â”€ offline F0Â·5 helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f05_np(y_true, proba):\n",
    "    pred = (proba >= THRESHOLD)\n",
    "    return fbeta_score(y_true, pred, beta=BETA, zero_division=0)\n",
    "\n",
    "# â”€â”€ Custom Precision-Focused Metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class PrecisionFocusedMetric(keras.metrics.Metric):\n",
    "    \"\"\"Custom metric that weighs precision 3x more than recall\"\"\"\n",
    "    def __init__(self, name=\"precision_focused\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        # Weight precision 3x more than recall: 3*P + R / 4\n",
    "        return (3 * prec + rec) / 4\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "\n",
    "# â”€â”€ graph-safe F0Â·5 metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class F05Metric(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f05\", **kw):\n",
    "        super().__init__(name=name, dtype=tf.float32, **kw)\n",
    "        self.beta_sq = BETA ** 2\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_t, y_p, sample_weight=None):\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        y_p = tf.cast(y_p >= THRESHOLD, tf.float32)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, tf.float32)\n",
    "            y_t *= sw; y_p *= sw\n",
    "        self.tp.assign_add(tf.reduce_sum(y_p * y_t))\n",
    "        self.fp.assign_add(tf.reduce_sum(y_p * (1 - y_t)))\n",
    "        self.fn.assign_add(tf.reduce_sum((1 - y_p) * y_t))\n",
    "    \n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-9)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-9)\n",
    "        return (1 + self.beta_sq) * prec * rec / (self.beta_sq * prec + rec + 1e-9)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        for v in self.variables: v.assign(0)\n",
    "\n",
    "# â”€â”€ model builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(cfg):\n",
    "    l2 = regularizers.l2(cfg[\"l2\"])\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_feat))\n",
    "    x   = inp\n",
    "    \n",
    "    for _ in range(cfg[\"conv_blocks\"]):\n",
    "        x = layers.Conv1D(cfg[\"filters\"], cfg[\"kernel\"], padding=\"causal\",\n",
    "                          activation=cfg[\"act\"], kernel_regularizer=l2)(x)\n",
    "        x = layers.Dropout(cfg[\"conv_drop\"])(x)\n",
    "    \n",
    "    x = layers.LSTM(cfg[\"units\"], dropout=cfg[\"lstm_drop\"],\n",
    "                    kernel_regularizer=l2)(x)\n",
    "    x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                     kernel_regularizer=l2)(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inp, out)\n",
    "\n",
    "    # weighted BCE (dtype-safe) - create a proper closure with current W0, W1\n",
    "    w0_tf = tf.constant(W0, tf.float32)\n",
    "    w1_tf = tf.constant(W1, tf.float32)\n",
    "    \n",
    "    def custom_weighted_bce(y_t, y_p):\n",
    "        \"\"\"Custom weighted binary crossentropy with current class weights\"\"\"\n",
    "        y_t = tf.cast(y_t, tf.float32)\n",
    "        w = tf.where(tf.equal(y_t, 1.0), w1_tf, w0_tf)\n",
    "        return tf.reduce_mean(w * keras.losses.binary_crossentropy(y_t, y_p))\n",
    "\n",
    "    # Optimizer selection\n",
    "    opt_cls = {\n",
    "        \"nadam\": keras.optimizers.Nadam,\n",
    "        \"adamw\": keras.optimizers.AdamW,\n",
    "        \"rmsprop\": keras.optimizers.RMSprop\n",
    "    }.get(cfg[\"optim\"], keras.optimizers.Adam)\n",
    "    \n",
    "    if cfg[\"optim\"] == \"adamw\":\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"], weight_decay=cfg[\"l2\"])\n",
    "    else:\n",
    "        opt = opt_cls(learning_rate=cfg[\"lr\"])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=custom_weighted_bce,\n",
    "        metrics=[PrecisionFocusedMetric(),\n",
    "                F05Metric(), \n",
    "                keras.metrics.Precision(),\n",
    "                keras.metrics.Recall(), \n",
    "                keras.metrics.AUC(), \n",
    "                \"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# â”€â”€ datasets & training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ—ï¸  Building model â€¦\")\n",
    "model = build_model(BEST_PARAMS)\n",
    "print(f\"Model built with {model.count_params():,} parameters\")\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((X_tr_w, y_tr_w))\n",
    "            .shuffle(10_000, seed=SEED)\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds   = (tf.data.Dataset.from_tensor_slices((X_va_w, y_va_w))\n",
    "            .batch(BEST_PARAMS[\"batch\"])\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Create multiple checkpoints for different metrics\n",
    "best_precision_ckpt = OUT_DIR / f\"best_precision_{STAMP}.keras\"\n",
    "best_f05_ckpt = OUT_DIR / f\"best_f05_{STAMP}.keras\"\n",
    "best_prec_focused_ckpt = OUT_DIR / f\"best_precision_focused_{STAMP}.keras\"\n",
    "\n",
    "cbs  = [\n",
    "    # Save best models for different metrics\n",
    "    ModelCheckpoint(best_precision_ckpt, monitor=\"val_precision_1\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_f05_ckpt, monitor=\"val_f05\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(best_prec_focused_ckpt, monitor=\"val_precision_focused\", mode=\"max\",\n",
    "                    save_best_only=True, verbose=1),\n",
    "    \n",
    "    # NO EARLY STOPPING - Let it train the full 1000 epochs!\n",
    "    \n",
    "    # Reduce LR more gradually and with more patience\n",
    "    ReduceLROnPlateau(monitor=\"val_precision_focused\", mode=\"max\", patience=LR_PATIENCE,\n",
    "                      factor=0.8, min_lr=1e-9, verbose=1)  # Less aggressive reduction\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Starting UNLIMITED training (1000 epochs, no early stopping)â€¦\")\n",
    "hist = model.fit(train_ds, validation_data=val_ds,\n",
    "                 epochs=EPOCHS, callbacks=cbs, verbose=1)\n",
    "\n",
    "model.save(OUT_DIR / f\"final_model_{STAMP}.keras\")\n",
    "\n",
    "# â”€â”€ Evaluate all saved models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_model(model_path, name):\n",
    "    \"\"\"Evaluate a specific model and return results\"\"\"\n",
    "    if not model_path.exists():\n",
    "        print(f\"âŒ {name} model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ” Evaluating {name} model...\")\n",
    "    \n",
    "    try:\n",
    "        # Try loading with custom objects\n",
    "        temp_model = keras.models.load_model(\n",
    "            model_path, \n",
    "            compile=False  # Don't compile to avoid loss function issues\n",
    "        )\n",
    "        \n",
    "        # Recompile with current loss function\n",
    "        temp_model.compile(\n",
    "            optimizer='adam',  # Simple optimizer for evaluation\n",
    "            loss='binary_crossentropy',  # Standard loss for evaluation\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {name} model: {e}\")\n",
    "        print(f\"   Skipping {name} evaluation...\")\n",
    "        return None\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_proba = temp_model.predict(X_va_w, verbose=0).ravel()\n",
    "    val_pred = (val_proba >= THRESHOLD)\n",
    "    \n",
    "    try:\n",
    "        precision = precision_score(y_va_w, val_pred, zero_division=0)\n",
    "        recall = recall_score(y_va_w, val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_va_w, val_pred, zero_division=0)\n",
    "        f05 = f05_np(y_va_w, val_proba)\n",
    "        acc = accuracy_score(y_va_w, val_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_va_w, val_proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        results = {\n",
    "            'name': name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'f05': f05,\n",
    "            'acc': acc,\n",
    "            'auc': auc_score,\n",
    "            'proba': val_proba,\n",
    "            'pred': val_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"   ðŸ“Š {name} Results:\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall:    {recall:.4f}\")\n",
    "        print(f\"      F1:        {f1:.4f}\")\n",
    "        print(f\"      F0.5:      {f05:.4f}\")\n",
    "        print(f\"      Accuracy:  {acc:.4f}\")\n",
    "        print(f\"      AUC:       {auc_score:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error evaluating {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# â”€â”€ evaluation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(X, y, name=\"\"):\n",
    "    print(f\"ðŸ” Evaluating {name}...\")\n",
    "    proba = model.predict(X, verbose=0).ravel()\n",
    "    pred  = (proba >= THRESHOLD)\n",
    "    \n",
    "    # Handle edge cases for metrics\n",
    "    try:\n",
    "        precision = precision_score(y, pred, zero_division=0)\n",
    "        recall = recall_score(y, pred, zero_division=0)\n",
    "        f1 = f1_score(y, pred, zero_division=0)\n",
    "        f05 = f05_np(y, proba)\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # AUC calculation with error handling\n",
    "        fpr, tpr, _ = roc_curve(y, proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in metric calculation: {e}\")\n",
    "        precision = recall = f1 = f05 = acc = auc_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'proba': proba, \n",
    "        'pred': pred, \n",
    "        'y': y,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f05': f05,\n",
    "        'acc': acc,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "\n",
    "# Evaluate current model first\n",
    "train_res = evaluate(X_tr_w, y_tr_w, \"training set\")\n",
    "current_val_res = evaluate(X_va_w, y_va_w, \"current validation set\")\n",
    "\n",
    "# Print current model results\n",
    "print(f\"\\nðŸ“ˆ Current Model Training Results:\")\n",
    "print(f\"   Precision: {train_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {train_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {train_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {train_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {train_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {train_res['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Current Model Validation Results:\")\n",
    "print(f\"   Precision: {current_val_res['precision']:.4f}\")\n",
    "print(f\"   Recall:    {current_val_res['recall']:.4f}\")\n",
    "print(f\"   F1:        {current_val_res['f1']:.4f}\")\n",
    "print(f\"   F0.5:      {current_val_res['f05']:.4f}\")\n",
    "print(f\"   Accuracy:  {current_val_res['acc']:.4f}\")\n",
    "print(f\"   AUC:       {current_val_res['auc']:.4f}\")\n",
    "\n",
    "# Evaluate all model variants\n",
    "model_results = {}\n",
    "model_results['precision'] = evaluate_model(best_precision_ckpt, \"Best Precision\")\n",
    "model_results['f05'] = evaluate_model(best_f05_ckpt, \"Best F0.5\")\n",
    "model_results['precision_focused'] = evaluate_model(best_prec_focused_ckpt, \"Best Precision-Focused\")\n",
    "\n",
    "# Calculate precision-focused score for each model (3x precision + recall) / 4\n",
    "def calculate_precision_focused_score(results):\n",
    "    \"\"\"Calculate the precision-focused score: (3*precision + recall) / 4\"\"\"\n",
    "    if not results or results['precision'] == 0:\n",
    "        return 0\n",
    "    return (3 * results['precision'] + results['recall']) / 4\n",
    "\n",
    "# Find the best model based on precision-focused score\n",
    "valid_results = [r for r in model_results.values() if r and r['precision'] > 0]\n",
    "if valid_results:\n",
    "    best_precision_focused_model = max(valid_results, key=calculate_precision_focused_score)\n",
    "    \n",
    "    # Also find individual bests for comparison\n",
    "    best_precision_model = max(valid_results, key=lambda x: x['precision'])\n",
    "    best_f05_model = max(valid_results, key=lambda x: x['f05'])\n",
    "    best_recall_model = max(valid_results, key=lambda x: x['recall'])\n",
    "    \n",
    "    print(f\"\\nðŸ† MODELS COMPARISON:\")\n",
    "    print(f\"ðŸŽ¯ Highest Precision-Focused Score: {best_precision_focused_model['name']}\")\n",
    "    print(f\"   Score: {calculate_precision_focused_score(best_precision_focused_model):.4f}\")\n",
    "    print(f\"   Precision: {best_precision_focused_model['precision']:.4f}\")\n",
    "    print(f\"   Recall: {best_precision_focused_model['recall']:.4f}\")\n",
    "    print(f\"   F0.5: {best_precision_focused_model['f05']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š For comparison:\")\n",
    "    print(f\"   Highest Precision Only: {best_precision_model['name']} (P:{best_precision_model['precision']:.4f}, R:{best_precision_model['recall']:.4f})\")\n",
    "    print(f\"   Highest F0.5: {best_f05_model['name']} (P:{best_f05_model['precision']:.4f}, R:{best_f05_model['recall']:.4f})\")\n",
    "    print(f\"   Highest Recall: {best_recall_model['name']} (P:{best_recall_model['precision']:.4f}, R:{best_recall_model['recall']:.4f})\")\n",
    "    \n",
    "    # Use the precision-focused model as final choice\n",
    "    final_model_results = best_precision_focused_model\n",
    "    best_model_name = best_precision_focused_model['name']\n",
    "    final_score = calculate_precision_focused_score(best_precision_focused_model)\n",
    "    \n",
    "    print(f\"\\nâœ… SELECTED: {best_model_name}\")\n",
    "    print(f\"   Reason: Highest precision-focused score (3Ã—P + R)/4 = {final_score:.4f}\")\n",
    "    print(f\"   This balances high precision ({best_precision_focused_model['precision']:.4f}) with reasonable recall ({best_precision_focused_model['recall']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    # Fallback to current model\n",
    "    current_val_eval = evaluate(X_va_w, y_va_w, \"current model fallback\")\n",
    "    final_model_results = current_val_eval\n",
    "    best_model_name = \"Current Model (Fallback)\"\n",
    "    final_score = calculate_precision_focused_score(current_val_eval)\n",
    "    print(f\"\\nâš ï¸  Using fallback current model (precision-focused score: {final_score:.4f})\")\n",
    "\n",
    "# â”€â”€ plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“ˆ Creating training plots â€¦\")\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# F0.5 plot\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(hist.history[\"f05\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_f05\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"F0.5\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(hist.history[\"accuracy\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_accuracy\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision plot\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(hist.history[\"precision_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Recall plot\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(hist.history[\"recall_1\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_recall_1\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Recall\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "# Precision-focused metric plot\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(hist.history[\"precision_focused\"], label=\"train\", alpha=0.8)\n",
    "plt.plot(hist.history[\"val_precision_focused\"], label=\"val\", alpha=0.8)\n",
    "plt.title(\"Precision-Focused Metric\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"training_curves_{STAMP}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â”€â”€ JSON summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving results summary â€¦\")\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "train_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                for k, v in train_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "\n",
    "# Use the best model results for validation metrics\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in final_model_results.items() if k not in ['proba', 'pred', 'name']}\n",
    "    best_model_info = {\n",
    "        \"best_model\": best_model_name,\n",
    "        \"selection_criteria\": \"precision_focused_score\",\n",
    "        \"precision_focused_score\": float(final_score),\n",
    "        \"formula\": \"(3 * precision + recall) / 4\"\n",
    "    }\n",
    "else:\n",
    "    val_metrics = {k: float(v) if np.isscalar(v) else v.tolist() if hasattr(v, 'tolist') else v \n",
    "                  for k, v in current_val_res.items() if k not in ['proba', 'pred', 'y']}\n",
    "    best_model_info = {\"best_model\": \"current_model\", \"selection_criteria\": \"final_epoch\"}\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": STAMP,\n",
    "    \"best_params\": BEST_PARAMS,\n",
    "    \"training_config\": {\n",
    "        \"epochs_trained\": len(hist.history[\"loss\"]),\n",
    "        \"max_epochs\": EPOCHS,\n",
    "        \"early_stopping\": False,\n",
    "        \"lr_patience\": LR_PATIENCE,\n",
    "        \"precision_focused\": True\n",
    "    },\n",
    "    \"model_selection\": best_model_info,\n",
    "    \"data_info\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"features\": n_feat,\n",
    "        \"train_samples\": len(X_tr_w),\n",
    "        \"val_samples\": len(X_va_w)\n",
    "    },\n",
    "    \"class_weights\": {\n",
    "        \"W0\": float(W0),\n",
    "        \"W1\": float(W1),\n",
    "        \"positive_rate\": float(pos_rate),\n",
    "        \"strategy\": \"precision_focused\"\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"train\": train_metrics,\n",
    "        \"val\": val_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all model comparisons if available\n",
    "if 'model_results' in locals():\n",
    "    all_model_metrics = {}\n",
    "    for model_name, results in model_results.items():\n",
    "        if results:\n",
    "            all_model_metrics[model_name] = {k: float(v) if np.isscalar(v) else v \n",
    "                                           for k, v in results.items() \n",
    "                                           if k not in ['proba', 'pred', 'name']}\n",
    "    results_summary[\"all_models_comparison\"] = all_model_metrics\n",
    "\n",
    "with open(OUT_DIR / f\"results_{STAMP}.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# â”€â”€ validation-prediction CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Saving validation predictions â€¦\")\n",
    "\n",
    "# Calculate the correct timestamp indices\n",
    "first_val_row = val_start + WIN\n",
    "end_val_row = first_val_row + len(y_va_w)\n",
    "\n",
    "# Ensure we don't go beyond the dataframe bounds\n",
    "end_val_row = min(end_val_row, len(df))\n",
    "timestamps = df.index.to_numpy()[first_val_row:end_val_row]\n",
    "\n",
    "# Use the best model's predictions if available\n",
    "if 'final_model_results' in locals() and final_model_results and 'proba' in final_model_results:\n",
    "    best_proba = final_model_results['proba']\n",
    "    best_pred = final_model_results['pred']\n",
    "    model_used = best_model_name\n",
    "    print(f\"ðŸ“Š Using predictions from: {best_model_name}\")\n",
    "else:\n",
    "    # Fallback: generate predictions from current model\n",
    "    print(\"ðŸ“Š Using predictions from: Current Model (fallback)\")\n",
    "    best_proba = model.predict(X_va_w, verbose=0).ravel()\n",
    "    best_pred = (best_proba >= THRESHOLD).astype(int)\n",
    "    model_used = \"Current Model\"\n",
    "\n",
    "# Ensure lengths match\n",
    "min_len = min(len(timestamps), len(best_proba), len(y_va_w))\n",
    "timestamps = timestamps[:min_len]\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"prob_up\": best_proba[:min_len],\n",
    "    \"prob_down\": 1.0 - best_proba[:min_len],\n",
    "    \"winning_prob\": np.maximum(best_proba[:min_len], 1.0 - best_proba[:min_len]),\n",
    "    \"prediction\": best_pred[:min_len].astype(int),\n",
    "    \"actual\": y_va_w[:min_len],\n",
    "    \"model_used\": model_used\n",
    "})\n",
    "\n",
    "# Round probabilities\n",
    "val_df[[\"prob_up\", \"prob_down\", \"winning_prob\"]] = val_df[\n",
    "    [\"prob_up\", \"prob_down\", \"winning_prob\"]].round(6)\n",
    "\n",
    "csv_path = OUT_DIR / f\"cnn_lstm_val_preds_{STAMP}.csv\"\n",
    "val_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV saved â†’ {csv_path}  ({len(val_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nâœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ All artifacts saved in: {OUT_DIR}\")\n",
    "print(f\"ðŸ·ï¸  Timestamp: {STAMP}\")\n",
    "print(f\"ðŸŽ¯ Final model selection: {model_used}\")\n",
    "if 'final_model_results' in locals() and final_model_results:\n",
    "    print(f\"ðŸŽ¯ Final precision-focused score: {final_score:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final precision: {final_model_results['precision']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final recall: {final_model_results['recall']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Final F0.5: {final_model_results['f05']:.4f}\")\n",
    "\n",
    "# Summary of improvements made\n",
    "print(f\"\\nðŸ”§ IMPROVEMENTS IMPLEMENTED:\")\n",
    "print(f\"   âœ… UNLIMITED TRAINING: No early stopping - full 1000 epochs!\")\n",
    "print(f\"   âœ… Extended epochs: 500 â†’ 1000\") \n",
    "print(f\"   âœ… More patient LR reduction: patience 25 â†’ 50, factor 0.7 â†’ 0.8\")\n",
    "print(f\"   âœ… Precision-focused class weighting (30% reduction in positive weight)\")\n",
    "print(f\"   âœ… Smart model selection using precision-focused score: (3Ã—P + R)/4\")\n",
    "print(f\"   âœ… Multiple model checkpoints (precision, F0.5, precision-focused)\")\n",
    "print(f\"   âœ… Balanced precision-recall optimization (avoids useless high-P/low-R models)\")\n",
    "print(f\"   âœ… Comprehensive model comparison and evaluation\")\n",
    "print(f\"   ðŸš€ LET'S SEE HOW FAR WE CAN PUSH THIS MODEL!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
