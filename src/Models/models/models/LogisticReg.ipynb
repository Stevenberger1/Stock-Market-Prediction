{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5abe9e",
   "metadata": {},
   "source": [
    "# In this notebook we will create the LogisticRegression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb92670",
   "metadata": {},
   "source": [
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "             'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "             'vol_spike_1_5x',\n",
    "             'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "             'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "             'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "             'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "Dataset shape: (15855, 46)\n",
    "Target distribution: {1: 8097, 0: 7758}\n",
    "Train: (12684, 46) | Test: (3171, 46)\n",
    "\n",
    "ğŸ” Running search 1/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 1 finished in 84.7s (best CV wF0.5 = 0.564)\n",
    "\n",
    "ğŸ” Running search 2/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 2 finished in 194.9s (best CV wF0.5 = 0.566)\n",
    "\n",
    "ğŸŒŸ Overall best CV wF0.5 = 0.566\n",
    "\n",
    "ğŸŒŸ Best parameters:\n",
    "   logreg__C             : 0.00407559644007287\n",
    "   logreg__class_weight  : None\n",
    "   logreg__l1_ratio      : 0.5\n",
    "   logreg__penalty       : elasticnet\n",
    "\n",
    "ğŸ“Š HOLD-OUT METRICS\n",
    "   Accuracy    : 0.534\n",
    "   Precision   : 0.551\n",
    "   Recall      : 0.581\n",
    "   F1          : 0.566\n",
    "   wF Î²=0.5    : 0.557\n",
    "   ROC-AUC     : 0.548\n",
    "\n",
    "ğŸ… Top-15 absolute coefficients:\n",
    "buying_pressure   -0.084537\n",
    "stoch_%K          -0.038546\n",
    "bb_position       -0.025447\n",
    "MACD_histogram    -0.015845\n",
    "cci_oversold       0.012325\n",
    "obv_rising_24h    -0.002853\n",
    "above_sma20       -0.001778\n",
    "cci_overbought    -0.001013\n",
    "stoch_oversold     0.000942\n",
    "near_lower_band    0.000287\n",
    "EMA_7              0.000000\n",
    "EMA_21             0.000000\n",
    "close              0.000000\n",
    "volume             0.000000\n",
    "atr_14             0.000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe7780",
   "metadata": {},
   "source": [
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "             'MACD_line', 'MACD_signal',  'momentum_alignment',\n",
    "             'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "\n",
    "Dataset shape: (15855, 59)\n",
    "Target distribution: {1: 8097, 0: 7758}\n",
    "Train: (12684, 59) | Test: (3171, 59)\n",
    "\n",
    "ğŸ” Running search 1/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 1 finished in 101.3s (best CV wF0.5 = 0.564)\n",
    "\n",
    "ğŸ” Running search 2/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 2 finished in 164.6s (best CV wF0.5 = 0.566)\n",
    "\n",
    "ğŸŒŸ Overall best CV wF0.5 = 0.566\n",
    "\n",
    "ğŸŒŸ Best parameters:\n",
    "   logreg__C             : 0.00407559644007287\n",
    "   logreg__class_weight  : None\n",
    "   logreg__l1_ratio      : 0.5\n",
    "   logreg__penalty       : elasticnet\n",
    "\n",
    "ğŸ“Š HOLD-OUT METRICS\n",
    "   Accuracy    : 0.534\n",
    "   Precision   : 0.551\n",
    "   Recall      : 0.581\n",
    "   F1          : 0.566\n",
    "   wF Î²=0.5    : 0.557\n",
    "   ROC-AUC     : 0.548\n",
    "\n",
    "ğŸ… Top-15 absolute coefficients:\n",
    "buying_pressure   -0.084537\n",
    "stoch_%K          -0.038547\n",
    "bb_position       -0.025443\n",
    "MACD_histogram    -0.015845\n",
    "cci_oversold       0.012325\n",
    "obv_rising_24h    -0.002853\n",
    "above_sma20       -0.001780\n",
    "cci_overbought    -0.001014\n",
    "stoch_oversold     0.000941\n",
    "near_lower_band    0.000288\n",
    "EMA_7              0.000000\n",
    "bollinger_lower    0.000000\n",
    "bollinger_upper    0.000000\n",
    "CCI                0.000000\n",
    "bollinger_width    0.000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c01c18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0373baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15855, 46)\n",
      "Target distribution: {1: 8097, 0: 7758}\n",
      "Train: (12684, 46) | Test: (3171, 46)\n",
      "\n",
      "ğŸ” Running search 1/2...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Search 1 finished in 84.7s (best CV wF0.5 = 0.564)\n",
      "\n",
      "ğŸ” Running search 2/2...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Search 2 finished in 194.9s (best CV wF0.5 = 0.566)\n",
      "\n",
      "ğŸŒŸ Overall best CV wF0.5 = 0.566\n",
      "\n",
      "ğŸŒŸ Best parameters:\n",
      "   logreg__C             : 0.00407559644007287\n",
      "   logreg__class_weight  : None\n",
      "   logreg__l1_ratio      : 0.5\n",
      "   logreg__penalty       : elasticnet\n",
      "\n",
      "ğŸ“Š HOLD-OUT METRICS\n",
      "   Accuracy    : 0.534\n",
      "   Precision   : 0.551\n",
      "   Recall      : 0.581\n",
      "   F1          : 0.566\n",
      "   wF Î²=0.5    : 0.557\n",
      "   ROC-AUC     : 0.548\n",
      "\n",
      "ğŸ… Top-15 absolute coefficients:\n",
      "buying_pressure   -0.084537\n",
      "stoch_%K          -0.038546\n",
      "bb_position       -0.025447\n",
      "MACD_histogram    -0.015845\n",
      "cci_oversold       0.012325\n",
      "obv_rising_24h    -0.002853\n",
      "above_sma20       -0.001778\n",
      "cci_overbought    -0.001013\n",
      "stoch_oversold     0.000942\n",
      "near_lower_band    0.000287\n",
      "EMA_7              0.000000\n",
      "EMA_21             0.000000\n",
      "close              0.000000\n",
      "volume             0.000000\n",
      "atr_14             0.000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  LOGISTIC-REGRESSION  HYPER-TUNER  (precision-weighted FÎ²=0.5)\n",
    "# =============================================================\n",
    "import numpy as np, pandas as pd, time, sys, warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (precision_score, recall_score, make_scorer,\n",
    "                             accuracy_score, f1_score, roc_auc_score)\n",
    "from scipy.stats import loguniform\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_FILE   = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL   = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC  = 0.20\n",
    "\n",
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "             'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "             'vol_spike_1_5x',\n",
    "             'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "             'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "             'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "             'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) LOAD & VALIDATE DATA\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not CSV_FILE.exists():\n",
    "    sys.exit(f\"âŒ File not found: {CSV_FILE}\")\n",
    "\n",
    "df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "df = df.loc[START_DATE:].copy()\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    sys.exit(f\"âŒ '{TARGET_COL}' column missing!\")\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Data checks\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"âš ï¸ Missing values detected!\")\n",
    "    print(X.isnull().sum()[X.isnull().sum() > 0])\n",
    "if y.sum() / len(y) < 0.01 or y.sum() / len(y) > 0.99:\n",
    "    print(\"âš ï¸ Highly imbalanced target!\")\n",
    "\n",
    "# Chronological split\n",
    "split = int(len(df) * (1 - TEST_FRAC))\n",
    "X_tr, X_te = X.iloc[:split], X.iloc[split:]\n",
    "y_tr, y_te = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "print(f\"Train: {X_tr.shape} | Test: {X_te.shape}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) CUSTOM SCORER: PRECISION-WEIGHTED FÎ² (Î² = 0.5)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "weighted_f = make_scorer(f_beta_half, greater_is_better=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) PIPELINE AND PARAMETER SEARCHES\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=4000, solver='saga', n_jobs=1))\n",
    "])\n",
    "\n",
    "# Separate hyperparameter sets to avoid l1_ratio conflicts\n",
    "param_dist_list = [\n",
    "    {\n",
    "        \"logreg__penalty\": ['l1', 'l2', 'none'],\n",
    "        \"logreg__C\": loguniform(1e-3, 1e2),\n",
    "        \"logreg__class_weight\": [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        \"logreg__penalty\": ['elasticnet'],\n",
    "        \"logreg__C\": loguniform(1e-3, 1e2),\n",
    "        \"logreg__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        \"logreg__class_weight\": [None, 'balanced']\n",
    "    }\n",
    "]\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "best_score = -np.inf\n",
    "best_estimator = None\n",
    "best_params = None\n",
    "\n",
    "for i, param_dist in enumerate(param_dist_list):\n",
    "    print(f\"\\nğŸ” Running search {i+1}/{len(param_dist_list)}...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, param_distributions=param_dist,\n",
    "        n_iter=30, cv=cv, scoring=weighted_f,\n",
    "        random_state=42, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    search.fit(X_tr, y_tr)\n",
    "    print(f\"Search {i+1} finished in {time.time()-t0:.1f}s \"\n",
    "          f\"(best CV wF0.5 = {search.best_score_:.3f})\")\n",
    "\n",
    "    if search.best_score_ > best_score:\n",
    "        best_score = search.best_score_\n",
    "        best_estimator = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "\n",
    "print(f\"\\nğŸŒŸ Overall best CV wF0.5 = {best_score:.3f}\")\n",
    "print(\"\\nğŸŒŸ Best parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"   {k:<22}: {v}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) HOLD-OUT VALIDATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "y_pred = best_estimator.predict(X_te)\n",
    "y_prob = best_estimator.predict_proba(X_te)[:, 1]\n",
    "\n",
    "def show(name, val):\n",
    "    print(f\"   {name:<12}: {val:.3f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š HOLD-OUT METRICS\")\n",
    "show(\"Accuracy\",  accuracy_score(y_te, y_pred))\n",
    "show(\"Precision\", precision_score(y_te, y_pred, zero_division=0))\n",
    "show(\"Recall\",    recall_score   (y_te, y_pred, zero_division=0))\n",
    "show(\"F1\",        f1_score       (y_te, y_pred, zero_division=0))\n",
    "show(\"wF Î²=0.5\",  f_beta_half    (y_te, y_pred))\n",
    "show(\"ROC-AUC\",   roc_auc_score  (y_te, y_prob))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) TOP COEFFICIENTS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "coefs = best_estimator.named_steps['logreg'].coef_[0]\n",
    "coef_df = (pd.Series(coefs, index=X_tr.columns)\n",
    "             .sort_values(key=np.abs, ascending=False)\n",
    "             .head(15))\n",
    "print(\"\\nğŸ… Top-15 absolute coefficients:\")\n",
    "print(coef_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb89e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ENHANCED LOGISTIC REGRESSION HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "ğŸ“‚ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "ğŸ“Š Dataset shape: (15855, 46)\n",
      "ğŸ“ˆ Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "ğŸ¯ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "ğŸ“Š TRAIN/TEST SPLIT\n",
      "   Train: 12,684 samples (80.0%)\n",
      "   Test:  3,171 samples (20.0%)\n",
      "   Train date range: 2018-01-01 00:00:00 to 2023-10-16 12:00:00\n",
      "   Test date range:  2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "   Train positive rate: 0.508\n",
      "   Test positive rate:  0.522\n",
      "ğŸ”§ Preprocessing features...\n",
      "âš ï¸ Removed 2 low-variance features\n",
      "\n",
      "ğŸ”§ FEATURE PREPROCESSING COMPLETE\n",
      "   Final feature count: 44\n",
      "ğŸ” Starting hyperparameter search with 3 parameter sets...\n",
      "\n",
      "ğŸ” Search 1/3 - L1 regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "âœ… Search 1 completed in 489.7s\n",
      "   Best CV F-beta(0.5): 0.5639\n",
      "   Best params: {'logreg__C': np.float64(104.6123232641137), 'logreg__class_weight': {0: 1, 1: 5}, 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "ğŸ” Search 2/3 - L2 regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "âœ… Search 2 completed in 53.1s\n",
      "   Best CV F-beta(0.5): 0.5642\n",
      "   Best params: {'logreg__C': np.float64(0.00028533901052402264), 'logreg__class_weight': {0: 1, 1: 3}, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs'}\n",
      "\n",
      "ğŸ” Search 3/3 - ELASTICNET regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "âœ… Search 3 completed in 392.3s\n",
      "   Best CV F-beta(0.5): 0.5664\n",
      "   Best params: {'logreg__C': np.float64(0.0016351310838425184), 'logreg__class_weight': None, 'logreg__l1_ratio': np.float64(0.2636043819680166), 'logreg__penalty': 'elasticnet', 'logreg__solver': 'saga'}\n",
      "\n",
      "ğŸŒŸ HYPERPARAMETER SEARCH COMPLETED\n",
      "==================================================\n",
      "   Total time: 935.1 seconds\n",
      "   Best CV F-beta(0.5): 0.5664\n",
      "\n",
      "ğŸ† BEST PARAMETERS:\n",
      "   logreg__C                : 0.0016351310838425184\n",
      "   logreg__class_weight     : None\n",
      "   logreg__l1_ratio         : 0.2636043819680166\n",
      "   logreg__penalty          : elasticnet\n",
      "   logreg__solver           : saga\n",
      "\n",
      "ğŸ” CONVERGENCE CHECK:\n",
      "âœ… Model converged in 15 iterations\n",
      "\n",
      "ğŸ“Š HOLD-OUT TEST METRICS\n",
      "========================================\n",
      "   accuracy    : 0.5348\n",
      "   precision   : 0.5506\n",
      "   recall      : 0.5948\n",
      "   f1          : 0.5718\n",
      "   f_beta_0.5  : 0.5589\n",
      "   roc_auc     : 0.5479\n",
      "\n",
      "ğŸ“ˆ DETAILED CLASSIFICATION REPORT\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49      1515\n",
      "           1       0.55      0.59      0.57      1656\n",
      "\n",
      "    accuracy                           0.53      3171\n",
      "   macro avg       0.53      0.53      0.53      3171\n",
      "weighted avg       0.53      0.53      0.53      3171\n",
      "\n",
      "\n",
      "ğŸ¯ CONFUSION MATRIX\n",
      "========================================\n",
      "True Negatives:     711\n",
      "False Positives:    804\n",
      "False Negatives:    671\n",
      "True Positives:     985\n",
      "\n",
      "ğŸ… TOP-20 MOST IMPORTANT FEATURES\n",
      "============================================================\n",
      "Feature                        Coefficient  Abs Coef  \n",
      "------------------------------------------------------------\n",
      "buying_pressure                -0.0677      0.0677    \n",
      "stoch_%K                       -0.0371      0.0371    \n",
      "bb_position                    -0.0289      0.0289    \n",
      "MACD_histogram                 -0.0058      0.0058    \n",
      "cci_oversold                   0.0045       0.0045    \n",
      "near_lower_band                0.0020       0.0020    \n",
      "above_sma20                    -0.0012      0.0012    \n",
      "SMA_50                         0.0000       0.0000    \n",
      "EMA_21                         0.0000       0.0000    \n",
      "EMA_7                          0.0000       0.0000    \n",
      "close                          0.0000       0.0000    \n",
      "volume                         0.0000       0.0000    \n",
      "CCI                            0.0000       0.0000    \n",
      "bollinger_width                0.0000       0.0000    \n",
      "OBV                            0.0000       0.0000    \n",
      "RSI                            0.0000       0.0000    \n",
      "SMA_20                         0.0000       0.0000    \n",
      "volume_mean_20                 0.0000       0.0000    \n",
      "vwap_24h                       0.0000       0.0000    \n",
      "true_range                     0.0000       0.0000    \n",
      "\n",
      "ğŸ’¾ SAVING MODEL\n",
      "   Model saved to: best_logistic_model.pkl\n",
      "\n",
      "âœ… HYPERPARAMETER TUNING COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  ENHANCED LOGISTIC-REGRESSION HYPER-TUNER (precision-weighted FÎ²=0.5)\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, make_scorer, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import loguniform, uniform\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ENHANCED CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Model saving\n",
    "SAVE_MODEL = True\n",
    "MODEL_PATH = Path(\"best_logistic_model.pkl\")\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ENHANCED DATA LOADING & VALIDATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load data with comprehensive validation.\"\"\"\n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"âŒ File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"ğŸ“‚ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"âŒ '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    # Data validation\n",
    "    print(f\"ğŸ“Š Dataset shape: {X.shape}\")\n",
    "    print(f\"ğŸ“ˆ Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    target_dist = y.value_counts().to_dict()\n",
    "    print(f\"ğŸ¯ Target distribution: {target_dist}\")\n",
    "    \n",
    "    pos_rate = y.sum() / len(y)\n",
    "    if pos_rate < 0.01:\n",
    "        print(\"âš ï¸ Severely imbalanced target (< 1% positive class)!\")\n",
    "    elif pos_rate > 0.99:\n",
    "        print(\"âš ï¸ Severely imbalanced target (> 99% positive class)!\")\n",
    "    elif pos_rate < 0.05 or pos_rate > 0.95:\n",
    "        print(\"âš ï¸ Highly imbalanced target!\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(\"âš ï¸ Missing values detected:\")\n",
    "        print(missing_vals[missing_vals > 0])\n",
    "        print(\"Dropping rows with missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"ğŸ“Š Shape after dropping missing: {X.shape}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"âš ï¸ {inf_mask.sum()} rows with infinite values detected, dropping...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"ğŸ“Š Shape after dropping infinite values: {X.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) ENHANCED DATA PREPROCESSING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Enhanced feature preprocessing.\"\"\"\n",
    "    print(\"ğŸ”§ Preprocessing features...\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"âš ï¸ Removing {len(constant_cols)} constant features: {list(constant_cols)}\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"âš ï¸ Removed {n_features_before - n_features_after} low-variance features\")\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) ENHANCED CUSTOM SCORER\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Precision-weighted F-beta score with beta=0.5.\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "weighted_f_scorer = make_scorer(f_beta_half, greater_is_better=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ENHANCED PIPELINE WITH COMPREHENSIVE PARAMETER GRID\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def create_pipeline():\n",
    "    \"\"\"Create preprocessing and modeling pipeline.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(max_iter=5000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "def get_parameter_distributions():\n",
    "    \"\"\"Get comprehensive parameter distributions for different penalty types.\"\"\"\n",
    "    \n",
    "    # Custom class weights for imbalanced data\n",
    "    class_weights = [\n",
    "        None, \n",
    "        'balanced',\n",
    "        {0: 1, 1: 2},\n",
    "        {0: 1, 1: 3},\n",
    "        {0: 1, 1: 5}\n",
    "    ]\n",
    "    \n",
    "    param_distributions = [\n",
    "        # L1 (Lasso) regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['l1'],\n",
    "            \"logreg__solver\": ['liblinear', 'saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        },\n",
    "        \n",
    "        # L2 (Ridge) regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['l2'],\n",
    "            \"logreg__solver\": ['lbfgs', 'liblinear', 'newton-cg', 'saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        },\n",
    "        \n",
    "        # Elastic Net regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['elasticnet'],\n",
    "            \"logreg__solver\": ['saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__l1_ratio\": uniform(0.01, 0.98),  # Between 0.01 and 0.99\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        }\n",
    "        \n",
    "        # Note: Removed \"No regularization\" as strong regularization is clearly beneficial for this dataset\n",
    "    ]\n",
    "    \n",
    "    return param_distributions\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) ENHANCED HYPERPARAMETER SEARCH\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def perform_hyperparameter_search(X_train, y_train):\n",
    "    \"\"\"Perform comprehensive hyperparameter search.\"\"\"\n",
    "    \n",
    "    # Check if we have both classes in training set\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        sys.exit(\"âŒ Training set doesn't contain both classes!\")\n",
    "    \n",
    "    pipeline = create_pipeline()\n",
    "    param_distributions = get_parameter_distributions()\n",
    "    \n",
    "    # Enhanced time series cross-validation\n",
    "    cv = TimeSeriesSplit(n_splits=8, gap=24)  # Add gap to prevent data leakage\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_estimator = None\n",
    "    best_params = None\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"ğŸ” Starting hyperparameter search with {len(param_distributions)} parameter sets...\")\n",
    "    \n",
    "    for i, param_dist in enumerate(param_distributions):\n",
    "        penalty_type = param_dist['logreg__penalty'][0]\n",
    "        print(f\"\\nğŸ” Search {i+1}/{len(param_distributions)} - {penalty_type.upper()} regularization...\")\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, \n",
    "            param_distributions=param_dist,\n",
    "            n_iter=50,  # Increased iterations\n",
    "            cv=cv, \n",
    "            scoring=weighted_f_scorer,\n",
    "            random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, \n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        search_time = time.time() - t0\n",
    "        \n",
    "        print(f\"âœ… Search {i+1} completed in {search_time:.1f}s\")\n",
    "        print(f\"   Best CV F-beta(0.5): {search.best_score_:.4f}\")\n",
    "        print(f\"   Best params: {search.best_params_}\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'penalty': penalty_type,\n",
    "            'best_score': search.best_score_,\n",
    "            'best_params': search.best_params_,\n",
    "            'search_time': search_time\n",
    "        })\n",
    "        \n",
    "        if search.best_score_ > best_score:\n",
    "            best_score = search.best_score_\n",
    "            best_estimator = search.best_estimator_\n",
    "            best_params = search.best_params_\n",
    "    \n",
    "    return best_estimator, best_params, best_score, all_results\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7) ENHANCED CONVERGENCE CHECKING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def check_model_convergence(estimator):\n",
    "    \"\"\"Check if the logistic regression model converged.\"\"\"\n",
    "    logreg = estimator.named_steps['logreg']\n",
    "    \n",
    "    if hasattr(logreg, 'n_iter_'):\n",
    "        n_iter = logreg.n_iter_\n",
    "        if isinstance(n_iter, np.ndarray):\n",
    "            n_iter = n_iter[0]\n",
    "        \n",
    "        max_iter = logreg.max_iter\n",
    "        if n_iter >= max_iter:\n",
    "            print(f\"âš ï¸ Model may not have converged (used {n_iter}/{max_iter} iterations)\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"âœ… Model converged in {n_iter} iterations\")\n",
    "            return True\n",
    "    return True\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8) ENHANCED EVALUATION METRICS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_model(estimator, X_test, y_test, show_detailed=True):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    \n",
    "    y_pred = estimator.predict(X_test)\n",
    "    y_prob = estimator.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'f_beta_0.5': f_beta_half(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“Š HOLD-OUT TEST METRICS\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"   {metric_name:<12}: {value:.4f}\")\n",
    "    \n",
    "    if show_detailed:\n",
    "        print(f\"\\nğŸ“ˆ DETAILED CLASSIFICATION REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        print(f\"\\nğŸ¯ CONFUSION MATRIX\")\n",
    "        print(\"=\" * 40)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "        print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "        print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "        print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9) FEATURE IMPORTANCE ANALYSIS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def analyze_feature_importance(estimator, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and display feature importance.\"\"\"\n",
    "    \n",
    "    logreg = estimator.named_steps['logreg']\n",
    "    coefs = logreg.coef_[0]\n",
    "    \n",
    "    # Create coefficient DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs,\n",
    "        'abs_coefficient': np.abs(coefs)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ… TOP-{top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Feature':<30} {'Coefficient':<12} {'Abs Coef':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx, row in coef_df.head(top_n).iterrows():\n",
    "        print(f\"{row['feature']:<30} {row['coefficient']:<12.4f} {row['abs_coefficient']:<10.4f}\")\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10) MAIN EXECUTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ ENHANCED LOGISTIC REGRESSION HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load and validate data\n",
    "    X, y = load_and_validate_data()\n",
    "    \n",
    "    # Chronological split\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TRAIN/TEST SPLIT\")\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Train date range: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "    print(f\"   Test date range:  {X_test.index.min()} to {X_test.index.max()}\")\n",
    "    \n",
    "    # Check target distribution in splits\n",
    "    train_pos_rate = y_train.mean()\n",
    "    test_pos_rate = y_test.mean()\n",
    "    print(f\"   Train positive rate: {train_pos_rate:.3f}\")\n",
    "    print(f\"   Test positive rate:  {test_pos_rate:.3f}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    print(f\"\\nğŸ”§ FEATURE PREPROCESSING COMPLETE\")\n",
    "    print(f\"   Final feature count: {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    # Perform hyperparameter search\n",
    "    start_time = time.time()\n",
    "    best_estimator, best_params, best_score, all_results = perform_hyperparameter_search(\n",
    "        X_train_processed, y_train\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nğŸŒŸ HYPERPARAMETER SEARCH COMPLETED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"   Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"   Best CV F-beta(0.5): {best_score:.4f}\")\n",
    "    print(f\"\\nğŸ† BEST PARAMETERS:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   {param:<25}: {value}\")\n",
    "    \n",
    "    # Check convergence\n",
    "    print(f\"\\nğŸ” CONVERGENCE CHECK:\")\n",
    "    check_model_convergence(best_estimator)\n",
    "    \n",
    "    # Evaluate on hold-out test set\n",
    "    test_metrics = evaluate_model(best_estimator, X_test_processed, y_test)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance_df = analyze_feature_importance(\n",
    "        best_estimator, X_train_processed.columns\n",
    "    )\n",
    "    \n",
    "    # Save model if requested\n",
    "    if SAVE_MODEL:\n",
    "        print(f\"\\nğŸ’¾ SAVING MODEL\")\n",
    "        model_data = {\n",
    "            'model': best_estimator,\n",
    "            'best_params': best_params,\n",
    "            'best_cv_score': best_score,\n",
    "            'test_metrics': test_metrics,\n",
    "            'feature_names': list(X_train_processed.columns),\n",
    "            'feature_importance': feature_importance_df,\n",
    "            'training_info': {\n",
    "                'train_samples': len(X_train_processed),\n",
    "                'test_samples': len(X_test_processed),\n",
    "                'features_used': X_train_processed.shape[1],\n",
    "                'target_distribution': y_train.value_counts().to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, MODEL_PATH)\n",
    "        print(f\"   Model saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    print(f\"\\nâœ… HYPERPARAMETER TUNING COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "    return {\n",
    "        'best_estimator': best_estimator,\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'test_metrics': test_metrics,\n",
    "        'all_results': all_results\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 11) SCRIPT EXECUTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1336a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ FINAL MODEL TRAINING & CSV GENERATION\n",
      "============================================================\n",
      "ğŸ“‚ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "ğŸ“Š Dataset shape: (15855, 46)\n",
      "ğŸ“ˆ Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "ğŸŒŸ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "ğŸ“Š TRAIN/TEST SPLIT SUMMARY\n",
      "----------------------------------------\n",
      "   Train: 12,684 samples\n",
      "   Test:  3,171 samples\n",
      "ğŸ”§ Preprocessing features...\n",
      "âš ï¸ Removed 2 low-variance features\n",
      "âœ… Final feature count: 44\n",
      "\n",
      "ğŸ¯ TRAINING FINAL MODEL\n",
      "========================================\n",
      "ğŸ† Using best parameters:\n",
      "   C              : 0.0016351310838425184\n",
      "   class_weight   : None\n",
      "   l1_ratio       : 0.2636043819680166\n",
      "   penalty        : elasticnet\n",
      "   solver         : saga\n",
      "   max_iter       : 5000\n",
      "   random_state   : 42\n",
      "\n",
      "â±ï¸ Training model...\n",
      "âœ… Model trained in 0.08 seconds\n",
      "\n",
      "ğŸ’¾ GENERATING PREDICTION CSV FILES\n",
      "=============================================\n",
      "âœ… Training predictions saved to: model_outputs\\train_predictions.csv ((12684, 9))\n",
      "âœ… Test predictions saved to: model_outputs\\test_predictions.csv ((3171, 9))\n",
      "âœ… Full predictions saved to: model_outputs\\full_predictions.csv ((15855, 9))\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  FINAL MODEL TRAINING & CSV OUTPUT GENERATOR (NO .PKL SAVING)\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"model_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TRAIN_OUTPUT_CSV = OUTPUT_DIR / \"train_predictions.csv\"\n",
    "TEST_OUTPUT_CSV = OUTPUT_DIR / \"test_predictions.csv\"\n",
    "FULL_OUTPUT_CSV = OUTPUT_DIR / \"full_predictions.csv\"\n",
    "\n",
    "# BEST PARAMETERS FROM HYPERPARAMETER TUNING\n",
    "BEST_PARAMS = {\n",
    "    'C': 0.0016351310838425184,\n",
    "    'class_weight': None,\n",
    "    'l1_ratio': 0.2636043819680166,\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 5000,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) DATA LOADING & PREPROCESSING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_and_prepare_data():\n",
    "    print(\"\\U0001F680 FINAL MODEL TRAINING & CSV GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"\\u274C File not found: {CSV_FILE}\")\n",
    "\n",
    "    print(f\"\\U0001F4C2 Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "\n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"\\u274C '{TARGET_COL}' column missing!\")\n",
    "\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "\n",
    "    print(f\"\\U0001F4CA Dataset shape: {X.shape}\")\n",
    "    print(f\"\\U0001F4C8 Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"ğŸŒŸ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(\"\\u26A0\\uFE0F Handling missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"\\U0001F4CA Shape after cleaning: {X.shape}\")\n",
    "\n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"\\u26A0\\uFE0F Handling {inf_mask.sum()} rows with infinite values...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"\\U0001F4CA Final shape: {X.shape}\")\n",
    "\n",
    "    return X, y, df.index[~(missing_vals.sum() > 0 or inf_mask.sum() > 0) if (missing_vals.sum() > 0 or inf_mask.sum() > 0) else slice(None)]\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    print(\"ğŸ”§ Preprocessing features...\")\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"\\u26A0\\uFE0F Removing {len(constant_cols)} constant features\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "\n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "\n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"\\u26A0\\uFE0F Removed {n_features_before - n_features_after} low-variance features\")\n",
    "\n",
    "    print(f\"âœ… Final feature count: {n_features_after}\")\n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "def create_final_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(**BEST_PARAMS))\n",
    "    ])\n",
    "\n",
    "def train_final_model(X_train, y_train):\n",
    "    print(\"\\n\\U0001F3AF TRAINING FINAL MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    pipeline = create_final_pipeline()\n",
    "    print(\"\\U0001F3C6 Using best parameters:\")\n",
    "    for param, value in BEST_PARAMS.items():\n",
    "        print(f\"   {param:<15}: {value}\")\n",
    "\n",
    "    print(\"\\nâ±ï¸ Training model...\")\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(f\"âœ… Model trained in {time.time() - start_time:.2f} seconds\")\n",
    "    return pipeline\n",
    "\n",
    "def generate_prediction_csvs(model, X_train, X_test, y_train, y_test, train_dates, test_dates):\n",
    "    print(\"\\n\\U0001F4BE GENERATING PREDICTION CSV FILES\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def create_df(dates, actual, pred, prob, set_type):\n",
    "        return pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'actual_target': actual.values,\n",
    "            'predicted_target': pred,\n",
    "            'probability_class_0': 1 - prob,\n",
    "            'probability_class_1': prob,\n",
    "            'prediction_confidence': np.maximum(prob, 1 - prob),\n",
    "            'correct_prediction': (actual.values == pred).astype(int),\n",
    "            'set_type': set_type\n",
    "        })\n",
    "\n",
    "    train_df = create_df(train_dates, y_train, y_train_pred, y_train_prob, 'train')\n",
    "    test_df = create_df(test_dates, y_test, y_test_pred, y_test_prob, 'test')\n",
    "    full_df = pd.concat([train_df, test_df]).sort_values('timestamp')\n",
    "\n",
    "    for df in [train_df, test_df, full_df]:\n",
    "        df['prediction_type'] = df.apply(lambda row: \n",
    "            'True Positive' if row['actual_target'] == 1 and row['predicted_target'] == 1\n",
    "            else 'True Negative' if row['actual_target'] == 0 and row['predicted_target'] == 0\n",
    "            else 'False Positive' if row['actual_target'] == 0 and row['predicted_target'] == 1\n",
    "            else 'False Negative', axis=1)\n",
    "\n",
    "    train_df.to_csv(TRAIN_OUTPUT_CSV, index=False)\n",
    "    test_df.to_csv(TEST_OUTPUT_CSV, index=False)\n",
    "    full_df.to_csv(FULL_OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(f\"âœ… Training predictions saved to: {TRAIN_OUTPUT_CSV} ({train_df.shape})\")\n",
    "    print(f\"âœ… Test predictions saved to: {TEST_OUTPUT_CSV} ({test_df.shape})\")\n",
    "    print(f\"âœ… Full predictions saved to: {FULL_OUTPUT_CSV} ({full_df.shape})\")\n",
    "    return train_df, test_df, full_df\n",
    "\n",
    "def main():\n",
    "    X, y, original_dates = load_and_prepare_data()\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    train_dates = X.index[:split_idx]\n",
    "    test_dates = X.index[split_idx:]\n",
    "\n",
    "    print(\"\\n\\U0001F4CA TRAIN/TEST SPLIT SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples\")\n",
    "\n",
    "    X_train_proc, X_test_proc = preprocess_features(X_train, X_test)\n",
    "    model = train_final_model(X_train_proc, y_train)\n",
    "    generate_prediction_csvs(model, X_train_proc, X_test_proc, y_train, y_test, train_dates, test_dates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf234a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d0ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ FINAL MODEL TRAINING & CSV GENERATION\n",
      "============================================================\n",
      "ğŸ“‚ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "ğŸ“Š Dataset shape: (15855, 46)\n",
      "ğŸ“ˆ Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "ğŸ¯ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "ğŸ“Š TRAIN/TEST SPLIT SUMMARY\n",
      "----------------------------------------\n",
      "   Train: 12,684 samples (80.0%)\n",
      "   Test:  3,171 samples (20.0%)\n",
      "   Train period: 2018-01-01 00:00:00 to 2023-10-16 12:00:00\n",
      "   Test period:  2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "ğŸ”§ Preprocessing features...\n",
      "âš ï¸ Removed 2 low-variance features\n",
      "âœ… Final feature count: 44\n",
      "\n",
      "ğŸ¯ TRAINING FINAL MODEL\n",
      "========================================\n",
      "ğŸ† Using best parameters:\n",
      "   C              : 0.0016351310838425184\n",
      "   class_weight   : None\n",
      "   l1_ratio       : 0.2636043819680166\n",
      "   penalty        : elasticnet\n",
      "   solver         : saga\n",
      "   max_iter       : 5000\n",
      "   random_state   : 42\n",
      "\n",
      "â±ï¸ Training model...\n",
      "âœ… Model trained in 0.09 seconds\n",
      "âœ… Model converged in 15 iterations\n",
      "\n",
      "ğŸ“Š MODEL PERFORMANCE EVALUATION\n",
      "==================================================\n",
      "\n",
      "TRAINING SET METRICS:\n",
      "------------------------------\n",
      "   accuracy    : 0.5392\n",
      "   precision   : 0.5408\n",
      "   recall      : 0.6133\n",
      "   f1          : 0.5748\n",
      "   f_beta_0.5  : 0.5539\n",
      "   roc_auc     : 0.5553\n",
      "\n",
      "TEST SET METRICS:\n",
      "------------------------------\n",
      "   accuracy    : 0.5348\n",
      "   precision   : 0.5506\n",
      "   recall      : 0.5948\n",
      "   f1          : 0.5718\n",
      "   f_beta_0.5  : 0.5589\n",
      "   roc_auc     : 0.5479\n",
      "\n",
      "ğŸ“ˆ DETAILED TEST SET ANALYSIS\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49      1515\n",
      "           1       0.55      0.59      0.57      1656\n",
      "\n",
      "    accuracy                           0.53      3171\n",
      "   macro avg       0.53      0.53      0.53      3171\n",
      "weighted avg       0.53      0.53      0.53      3171\n",
      "\n",
      "\n",
      "ğŸ¯ CONFUSION MATRIX (Test Set)\n",
      "------------------------------\n",
      "True Negatives:     711\n",
      "False Positives:    804\n",
      "False Negatives:    671\n",
      "True Positives:     985\n",
      "\n",
      "ğŸ… TOP-20 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "Rank Feature                        Coefficient  Abs Coef   Impact  \n",
      "----------------------------------------------------------------------\n",
      "1    buying_pressure                -0.0677      0.0677     Negative\n",
      "2    stoch_%K                       -0.0371      0.0371     Negative\n",
      "3    bb_position                    -0.0289      0.0289     Negative\n",
      "4    MACD_histogram                 -0.0058      0.0058     Negative\n",
      "5    cci_oversold                   0.0045       0.0045     Positive\n",
      "6    near_lower_band                0.0020       0.0020     Positive\n",
      "7    above_sma20                    -0.0012      0.0012     Negative\n",
      "8    SMA_50                         0.0000       0.0000     Negative\n",
      "9    EMA_21                         0.0000       0.0000     Negative\n",
      "10   EMA_7                          0.0000       0.0000     Negative\n",
      "11   close                          0.0000       0.0000     Negative\n",
      "12   volume                         0.0000       0.0000     Negative\n",
      "13   CCI                            0.0000       0.0000     Negative\n",
      "14   bollinger_width                0.0000       0.0000     Negative\n",
      "15   OBV                            0.0000       0.0000     Negative\n",
      "16   RSI                            0.0000       0.0000     Negative\n",
      "17   SMA_20                         0.0000       0.0000     Negative\n",
      "18   volume_mean_20                 0.0000       0.0000     Negative\n",
      "19   vwap_24h                       0.0000       0.0000     Negative\n",
      "20   true_range                     0.0000       0.0000     Negative\n",
      "\n",
      "ğŸ’¾ GENERATING PREDICTION CSV FILES\n",
      "=============================================\n",
      "âœ… Training predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\train_predictions.csv\n",
      "   Shape: (12684, 11)\n",
      "âœ… Test predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\test_predictions.csv\n",
      "   Shape: (3171, 11)\n",
      "âœ… Full predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\full_predictions.csv\n",
      "   Shape: (15855, 11)\n",
      "\n",
      "ğŸ“‹ SAMPLE PREDICTIONS (First 10 Test Records)\n",
      "------------------------------------------------------------\n",
      "          timestamp  actual_target  predicted_target  probability_class_1  prediction_confidence\n",
      "2023-10-16 16:00:00              1                 1             0.529161               0.529161\n",
      "2023-10-16 20:00:00              0                 1             0.506090               0.506090\n",
      "2023-10-17 00:00:00              0                 1             0.537758               0.537758\n",
      "2023-10-17 04:00:00              1                 1             0.526984               0.526984\n",
      "2023-10-17 08:00:00              0                 0             0.479448               0.520552\n",
      "2023-10-17 12:00:00              1                 1             0.546537               0.546537\n",
      "2023-10-17 16:00:00              0                 0             0.449245               0.550755\n",
      "2023-10-17 20:00:00              0                 1             0.503812               0.503812\n",
      "2023-10-18 00:00:00              1                 1             0.535821               0.535821\n",
      "2023-10-18 04:00:00              0                 0             0.483338               0.516662\n",
      "\n",
      "ğŸ’¾ SAVING COMPLETE MODEL\n",
      "==============================\n",
      "âœ… Complete model package saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\final_logistic_model.pkl\n",
      "\n",
      "ğŸ‰ MODEL TRAINING & CSV GENERATION COMPLETED!\n",
      "=======================================================\n",
      "ğŸ“ Output Files Generated in: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\n",
      "   ğŸ”¸ train_predictions.csv\n",
      "   ğŸ”¸ test_predictions.csv\n",
      "   ğŸ”¸ full_predictions.csv\n",
      "   ğŸ¯ bitcoin_predictions_with_probabilities.csv (MAIN PREDICTIONS FILE)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  FINAL MODEL TRAINING & CSV OUTPUT GENERATOR\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TRAIN_OUTPUT_CSV = OUTPUT_DIR / \"train_predictions.csv\"\n",
    "TEST_OUTPUT_CSV = OUTPUT_DIR / \"test_predictions.csv\"\n",
    "FULL_OUTPUT_CSV = OUTPUT_DIR / \"full_predictions.csv\"\n",
    "MAIN_PREDICTIONS_CSV = OUTPUT_DIR / \"bitcoin_predictions_with_probabilities.csv\"\n",
    "MODEL_PATH = OUTPUT_DIR / \"final_logistic_model.pkl\"\n",
    "\n",
    "# BEST PARAMETERS FROM HYPERPARAMETER TUNING\n",
    "BEST_PARAMS = {\n",
    "    'C': 0.0016351310838425184,\n",
    "    'class_weight': None,\n",
    "    'l1_ratio': 0.2636043819680166,\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 5000,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) DATA LOADING & PREPROCESSING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data for final model training.\"\"\"\n",
    "    print(\"ğŸš€ FINAL MODEL TRAINING & CSV GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"âŒ File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"ğŸ“‚ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"âŒ '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset shape: {X.shape}\")\n",
    "    print(f\"ğŸ“ˆ Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"ğŸ¯ Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle missing and infinite values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(\"âš ï¸ Handling missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"ğŸ“Š Shape after cleaning: {X.shape}\")\n",
    "    \n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"âš ï¸ Handling {inf_mask.sum()} rows with infinite values...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"ğŸ“Š Final shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, df.index[~(missing_vals.sum() > 0 or inf_mask.sum() > 0) if (missing_vals.sum() > 0 or inf_mask.sum() > 0) else slice(None)]\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Preprocess features (same as in hyperparameter tuning).\"\"\"\n",
    "    print(\"ğŸ”§ Preprocessing features...\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"âš ï¸ Removing {len(constant_cols)} constant features\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"âš ï¸ Removed {n_features_before - n_features_after} low-variance features\")\n",
    "    \n",
    "    print(f\"âœ… Final feature count: {n_features_after}\")\n",
    "    return X_train_selected, X_test_selected, variance_selector\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) MODEL TRAINING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def create_final_pipeline():\n",
    "    \"\"\"Create the final pipeline with best parameters.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(**BEST_PARAMS))\n",
    "    ])\n",
    "\n",
    "def train_final_model(X_train, y_train):\n",
    "    \"\"\"Train the final model with best parameters.\"\"\"\n",
    "    print(\"\\nğŸ¯ TRAINING FINAL MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    pipeline = create_final_pipeline()\n",
    "    \n",
    "    print(\"ğŸ† Using best parameters:\")\n",
    "    for param, value in BEST_PARAMS.items():\n",
    "        print(f\"   {param:<15}: {value}\")\n",
    "    \n",
    "    print(\"\\nâ±ï¸ Training model...\")\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check convergence\n",
    "    logreg = pipeline.named_steps['logreg']\n",
    "    if hasattr(logreg, 'n_iter_'):\n",
    "        n_iter = logreg.n_iter_[0] if isinstance(logreg.n_iter_, np.ndarray) else logreg.n_iter_\n",
    "        if n_iter >= logreg.max_iter:\n",
    "            print(f\"âš ï¸ Model may not have converged (used {n_iter}/{logreg.max_iter} iterations)\")\n",
    "        else:\n",
    "            print(f\"âœ… Model converged in {n_iter} iterations\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) EVALUATION & METRICS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Custom F-beta score with beta=0.5 (precision-weighted).\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "def evaluate_model_performance(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    print(\"\\nğŸ“Š MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training set predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Test set predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for both sets\n",
    "    def calculate_metrics(y_true, y_pred, y_prob, set_name):\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'f_beta_0.5': f_beta_half(y_true, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{set_name.upper()} SET METRICS:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"   {metric_name:<12}: {value:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_prob, \"training\")\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_prob, \"test\")\n",
    "    \n",
    "    # Detailed test set analysis\n",
    "    print(f\"\\nğŸ“ˆ DETAILED TEST SET ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "    \n",
    "    print(f\"\\nğŸ¯ CONFUSION MATRIX (Test Set)\")\n",
    "    print(\"-\" * 30)\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    return {\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': y_train_pred,\n",
    "        'train_probabilities': y_train_prob,\n",
    "        'test_predictions': y_test_pred,\n",
    "        'test_probabilities': y_test_prob\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) FEATURE IMPORTANCE ANALYSIS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def analyze_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and display feature importance.\"\"\"\n",
    "    logreg = model.named_steps['logreg']\n",
    "    coefs = logreg.coef_[0]\n",
    "    \n",
    "    # Create coefficient DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs,\n",
    "        'abs_coefficient': np.abs(coefs),\n",
    "        'importance_rank': range(1, len(coefs) + 1)\n",
    "    }).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    coef_df['importance_rank'] = range(1, len(coef_df) + 1)\n",
    "    \n",
    "    print(f\"\\nğŸ… TOP-{top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<4} {'Feature':<30} {'Coefficient':<12} {'Abs Coef':<10} {'Impact':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx, row in coef_df.head(top_n).iterrows():\n",
    "        impact = \"Positive\" if row['coefficient'] > 0 else \"Negative\"\n",
    "        print(f\"{row['importance_rank']:<4} {row['feature']:<30} {row['coefficient']:<12.4f} {row['abs_coefficient']:<10.4f} {impact:<8}\")\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) CSV OUTPUT GENERATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def generate_prediction_csvs(X_train, X_test, y_train, y_test, results, train_dates, test_dates):\n",
    "    \"\"\"Generate comprehensive CSV outputs with predictions and probabilities.\"\"\"\n",
    "    print(\"\\nğŸ’¾ GENERATING PREDICTION CSV FILES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Training set CSV\n",
    "    train_df = pd.DataFrame({\n",
    "        'timestamp': train_dates,\n",
    "        'actual_target': y_train.values,\n",
    "        'predicted_target': results['train_predictions'],\n",
    "        'probability_class_0': 1 - results['train_probabilities'],\n",
    "        'probability_class_1': results['train_probabilities'],\n",
    "        'prediction_confidence': np.maximum(results['train_probabilities'], \n",
    "                                          1 - results['train_probabilities']),\n",
    "        'correct_prediction': (y_train.values == results['train_predictions']).astype(int),\n",
    "        'set_type': 'train'\n",
    "    })\n",
    "    \n",
    "    # Test set CSV\n",
    "    test_df = pd.DataFrame({\n",
    "        'timestamp': test_dates,\n",
    "        'actual_target': y_test.values,\n",
    "        'predicted_target': results['test_predictions'],\n",
    "        'probability_class_0': 1 - results['test_probabilities'],\n",
    "        'probability_class_1': results['test_probabilities'],\n",
    "        'prediction_confidence': np.maximum(results['test_probabilities'], \n",
    "                                          1 - results['test_probabilities']),\n",
    "        'correct_prediction': (y_test.values == results['test_predictions']).astype(int),\n",
    "        'set_type': 'test'\n",
    "    })\n",
    "    \n",
    "    # Full dataset CSV\n",
    "    full_df = pd.concat([train_df, test_df], ignore_index=True).sort_values('timestamp')\n",
    "    \n",
    "    # Add additional analysis columns\n",
    "    for df in [train_df, test_df, full_df]:\n",
    "        df['prediction_type'] = df.apply(lambda row: \n",
    "            'True Positive' if row['actual_target'] == 1 and row['predicted_target'] == 1\n",
    "            else 'True Negative' if row['actual_target'] == 0 and row['predicted_target'] == 0\n",
    "            else 'False Positive' if row['actual_target'] == 0 and row['predicted_target'] == 1\n",
    "            else 'False Negative', axis=1)\n",
    "        \n",
    "        df['high_confidence'] = (df['prediction_confidence'] >= 0.7).astype(int)\n",
    "        df['very_high_confidence'] = (df['prediction_confidence'] >= 0.8).astype(int)\n",
    "    \n",
    "    # Save CSV files\n",
    "    train_df.to_csv(TRAIN_OUTPUT_CSV, index=False)\n",
    "    test_df.to_csv(TEST_OUTPUT_CSV, index=False)\n",
    "    full_df.to_csv(FULL_OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"âœ… Training predictions saved to: {TRAIN_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {train_df.shape}\")\n",
    "    print(f\"âœ… Test predictions saved to: {TEST_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {test_df.shape}\")\n",
    "    print(f\"âœ… Full predictions saved to: {FULL_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {full_df.shape}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\nğŸ“‹ SAMPLE PREDICTIONS (First 10 Test Records)\")\n",
    "    print(\"-\" * 60)\n",
    "    sample_cols = ['timestamp', 'actual_target', 'predicted_target', 'probability_class_1', 'prediction_confidence']\n",
    "    print(test_df[sample_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    return train_df, test_df, full_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7) MODEL PERSISTENCE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def save_complete_model(model, feature_names, variance_selector, results, coef_df):\n",
    "    \"\"\"Save the complete model with all metadata.\"\"\"\n",
    "    print(f\"\\nğŸ’¾ SAVING COMPLETE MODEL\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'variance_selector': variance_selector,\n",
    "        'best_parameters': BEST_PARAMS,\n",
    "        'performance_metrics': {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics']\n",
    "        },\n",
    "        'feature_importance': coef_df,\n",
    "        'model_info': {\n",
    "            'training_date': pd.Timestamp.now(),\n",
    "            'scikit_learn_version': '1.3+',\n",
    "            'total_features': len(feature_names),\n",
    "            'dropped_columns': DROP_COLS,\n",
    "            'preprocessing_steps': ['StandardScaler', 'VarianceThreshold'],\n",
    "            'algorithm': 'LogisticRegression with ElasticNet'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_package, MODEL_PATH)\n",
    "    print(f\"âœ… Complete model package saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8) MAIN EXECUTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X, y, original_dates = load_and_prepare_data()\n",
    "    \n",
    "    # Chronological split (same as hyperparameter tuning)\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    train_dates = X.index[:split_idx]\n",
    "    test_dates = X.index[split_idx:]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TRAIN/TEST SPLIT SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Train period: {train_dates.min()} to {train_dates.max()}\")\n",
    "    print(f\"   Test period:  {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed, variance_selector = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    # Train final model\n",
    "    final_model = train_final_model(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model_performance(final_model, X_train_processed, y_train, X_test_processed, y_test)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance_df = analyze_feature_importance(final_model, X_train_processed.columns)\n",
    "    \n",
    "    # Generate CSV outputs\n",
    "    train_csv, test_csv, full_csv = generate_prediction_csvs(\n",
    "        X_train_processed, X_test_processed, y_train, y_test, results, train_dates, test_dates\n",
    "    )\n",
    "    \n",
    "    # Save complete model\n",
    "    model_package = save_complete_model(\n",
    "        final_model, list(X_train_processed.columns), variance_selector, results, feature_importance_df\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ MODEL TRAINING & CSV GENERATION COMPLETED!\")\n",
    "    print(\"=\" * 55)\n",
    "    print(f\"ğŸ“ Output Files Generated in: {OUTPUT_DIR}\")\n",
    "    print(f\"   ğŸ”¸ {TRAIN_OUTPUT_CSV.name}\")\n",
    "    print(f\"   ğŸ”¸ {TEST_OUTPUT_CSV.name}\")\n",
    "    print(f\"   ğŸ”¸ {FULL_OUTPUT_CSV.name}\")\n",
    "    print(f\"   ğŸ¯ {MAIN_PREDICTIONS_CSV.name} (MAIN PREDICTIONS FILE)\")\n",
    "    \n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'results': results,\n",
    "        'csvs': {'train': train_csv, 'test': test_csv, 'full': full_csv},\n",
    "        'feature_importance': feature_importance_df,\n",
    "        'model_package': model_package\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9) SCRIPT EXECUTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
