{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5abe9e",
   "metadata": {},
   "source": [
    "# In this notebook we will create the LogisticRegression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb92670",
   "metadata": {},
   "source": [
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "             'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "             'vol_spike_1_5x',\n",
    "             'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "             'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "             'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "             'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "Dataset shape: (15855, 46)\n",
    "Target distribution: {1: 8097, 0: 7758}\n",
    "Train: (12684, 46) | Test: (3171, 46)\n",
    "\n",
    "üîç Running search 1/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 1 finished in 84.7s (best CV wF0.5 = 0.564)\n",
    "\n",
    "üîç Running search 2/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 2 finished in 194.9s (best CV wF0.5 = 0.566)\n",
    "\n",
    "üåü Overall best CV wF0.5 = 0.566\n",
    "\n",
    "üåü Best parameters:\n",
    "   logreg__C             : 0.00407559644007287\n",
    "   logreg__class_weight  : None\n",
    "   logreg__l1_ratio      : 0.5\n",
    "   logreg__penalty       : elasticnet\n",
    "\n",
    "üìä HOLD-OUT METRICS\n",
    "   Accuracy    : 0.534\n",
    "   Precision   : 0.551\n",
    "   Recall      : 0.581\n",
    "   F1          : 0.566\n",
    "   wF Œ≤=0.5    : 0.557\n",
    "   ROC-AUC     : 0.548\n",
    "\n",
    "üèÖ Top-15 absolute coefficients:\n",
    "buying_pressure   -0.084537\n",
    "stoch_%K          -0.038546\n",
    "bb_position       -0.025447\n",
    "MACD_histogram    -0.015845\n",
    "cci_oversold       0.012325\n",
    "obv_rising_24h    -0.002853\n",
    "above_sma20       -0.001778\n",
    "cci_overbought    -0.001013\n",
    "stoch_oversold     0.000942\n",
    "near_lower_band    0.000287\n",
    "EMA_7              0.000000\n",
    "EMA_21             0.000000\n",
    "close              0.000000\n",
    "volume             0.000000\n",
    "atr_14             0.000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe7780",
   "metadata": {},
   "source": [
    "DROP_COLS = ['open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "             'MACD_line', 'MACD_signal',  'momentum_alignment',\n",
    "             'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1']\n",
    "\n",
    "\n",
    "Dataset shape: (15855, 59)\n",
    "Target distribution: {1: 8097, 0: 7758}\n",
    "Train: (12684, 59) | Test: (3171, 59)\n",
    "\n",
    "üîç Running search 1/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 1 finished in 101.3s (best CV wF0.5 = 0.564)\n",
    "\n",
    "üîç Running search 2/2...\n",
    "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
    "Search 2 finished in 164.6s (best CV wF0.5 = 0.566)\n",
    "\n",
    "üåü Overall best CV wF0.5 = 0.566\n",
    "\n",
    "üåü Best parameters:\n",
    "   logreg__C             : 0.00407559644007287\n",
    "   logreg__class_weight  : None\n",
    "   logreg__l1_ratio      : 0.5\n",
    "   logreg__penalty       : elasticnet\n",
    "\n",
    "üìä HOLD-OUT METRICS\n",
    "   Accuracy    : 0.534\n",
    "   Precision   : 0.551\n",
    "   Recall      : 0.581\n",
    "   F1          : 0.566\n",
    "   wF Œ≤=0.5    : 0.557\n",
    "   ROC-AUC     : 0.548\n",
    "\n",
    "üèÖ Top-15 absolute coefficients:\n",
    "buying_pressure   -0.084537\n",
    "stoch_%K          -0.038547\n",
    "bb_position       -0.025443\n",
    "MACD_histogram    -0.015845\n",
    "cci_oversold       0.012325\n",
    "obv_rising_24h    -0.002853\n",
    "above_sma20       -0.001780\n",
    "cci_overbought    -0.001014\n",
    "stoch_oversold     0.000941\n",
    "near_lower_band    0.000288\n",
    "EMA_7              0.000000\n",
    "bollinger_lower    0.000000\n",
    "bollinger_upper    0.000000\n",
    "CCI                0.000000\n",
    "bollinger_width    0.000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c01c18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb89e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ENHANCED LOGISTIC REGRESSION HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "üìÇ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "üìä Dataset shape: (15855, 46)\n",
      "üìà Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "üéØ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "üìä TRAIN/TEST SPLIT\n",
      "   Train: 12,684 samples (80.0%)\n",
      "   Test:  3,171 samples (20.0%)\n",
      "   Train date range: 2018-01-01 00:00:00 to 2023-10-16 12:00:00\n",
      "   Test date range:  2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "   Train positive rate: 0.508\n",
      "   Test positive rate:  0.522\n",
      "üîß Preprocessing features...\n",
      "‚ö†Ô∏è Removed 2 low-variance features\n",
      "\n",
      "üîß FEATURE PREPROCESSING COMPLETE\n",
      "   Final feature count: 44\n",
      "üîç Starting hyperparameter search with 3 parameter sets...\n",
      "\n",
      "üîç Search 1/3 - L1 regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "‚úÖ Search 1 completed in 489.7s\n",
      "   Best CV F-beta(0.5): 0.5639\n",
      "   Best params: {'logreg__C': np.float64(104.6123232641137), 'logreg__class_weight': {0: 1, 1: 5}, 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "üîç Search 2/3 - L2 regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "‚úÖ Search 2 completed in 53.1s\n",
      "   Best CV F-beta(0.5): 0.5642\n",
      "   Best params: {'logreg__C': np.float64(0.00028533901052402264), 'logreg__class_weight': {0: 1, 1: 3}, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs'}\n",
      "\n",
      "üîç Search 3/3 - ELASTICNET regularization...\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "‚úÖ Search 3 completed in 392.3s\n",
      "   Best CV F-beta(0.5): 0.5664\n",
      "   Best params: {'logreg__C': np.float64(0.0016351310838425184), 'logreg__class_weight': None, 'logreg__l1_ratio': np.float64(0.2636043819680166), 'logreg__penalty': 'elasticnet', 'logreg__solver': 'saga'}\n",
      "\n",
      "üåü HYPERPARAMETER SEARCH COMPLETED\n",
      "==================================================\n",
      "   Total time: 935.1 seconds\n",
      "   Best CV F-beta(0.5): 0.5664\n",
      "\n",
      "üèÜ BEST PARAMETERS:\n",
      "   logreg__C                : 0.0016351310838425184\n",
      "   logreg__class_weight     : None\n",
      "   logreg__l1_ratio         : 0.2636043819680166\n",
      "   logreg__penalty          : elasticnet\n",
      "   logreg__solver           : saga\n",
      "\n",
      "üîç CONVERGENCE CHECK:\n",
      "‚úÖ Model converged in 15 iterations\n",
      "\n",
      "üìä HOLD-OUT TEST METRICS\n",
      "========================================\n",
      "   accuracy    : 0.5348\n",
      "   precision   : 0.5506\n",
      "   recall      : 0.5948\n",
      "   f1          : 0.5718\n",
      "   f_beta_0.5  : 0.5589\n",
      "   roc_auc     : 0.5479\n",
      "\n",
      "üìà DETAILED CLASSIFICATION REPORT\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49      1515\n",
      "           1       0.55      0.59      0.57      1656\n",
      "\n",
      "    accuracy                           0.53      3171\n",
      "   macro avg       0.53      0.53      0.53      3171\n",
      "weighted avg       0.53      0.53      0.53      3171\n",
      "\n",
      "\n",
      "üéØ CONFUSION MATRIX\n",
      "========================================\n",
      "True Negatives:     711\n",
      "False Positives:    804\n",
      "False Negatives:    671\n",
      "True Positives:     985\n",
      "\n",
      "üèÖ TOP-20 MOST IMPORTANT FEATURES\n",
      "============================================================\n",
      "Feature                        Coefficient  Abs Coef  \n",
      "------------------------------------------------------------\n",
      "buying_pressure                -0.0677      0.0677    \n",
      "stoch_%K                       -0.0371      0.0371    \n",
      "bb_position                    -0.0289      0.0289    \n",
      "MACD_histogram                 -0.0058      0.0058    \n",
      "cci_oversold                   0.0045       0.0045    \n",
      "near_lower_band                0.0020       0.0020    \n",
      "above_sma20                    -0.0012      0.0012    \n",
      "SMA_50                         0.0000       0.0000    \n",
      "EMA_21                         0.0000       0.0000    \n",
      "EMA_7                          0.0000       0.0000    \n",
      "close                          0.0000       0.0000    \n",
      "volume                         0.0000       0.0000    \n",
      "CCI                            0.0000       0.0000    \n",
      "bollinger_width                0.0000       0.0000    \n",
      "OBV                            0.0000       0.0000    \n",
      "RSI                            0.0000       0.0000    \n",
      "SMA_20                         0.0000       0.0000    \n",
      "volume_mean_20                 0.0000       0.0000    \n",
      "vwap_24h                       0.0000       0.0000    \n",
      "true_range                     0.0000       0.0000    \n",
      "\n",
      "üíæ SAVING MODEL\n",
      "   Model saved to: best_logistic_model.pkl\n",
      "\n",
      "‚úÖ HYPERPARAMETER TUNING COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  ENHANCED LOGISTIC-REGRESSION HYPER-TUNER (precision-weighted FŒ≤=0.5)\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, make_scorer, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import loguniform, uniform\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) ENHANCED CONFIGURATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Model saving\n",
    "SAVE_MODEL = True\n",
    "MODEL_PATH = Path(\"best_logistic_model.pkl\")\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) ENHANCED DATA LOADING & VALIDATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load data with comprehensive validation.\"\"\"\n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"‚ùå File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"üìÇ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"‚ùå '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    # Data validation\n",
    "    print(f\"üìä Dataset shape: {X.shape}\")\n",
    "    print(f\"üìà Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    target_dist = y.value_counts().to_dict()\n",
    "    print(f\"üéØ Target distribution: {target_dist}\")\n",
    "    \n",
    "    pos_rate = y.sum() / len(y)\n",
    "    if pos_rate < 0.01:\n",
    "        print(\"‚ö†Ô∏è Severely imbalanced target (< 1% positive class)!\")\n",
    "    elif pos_rate > 0.99:\n",
    "        print(\"‚ö†Ô∏è Severely imbalanced target (> 99% positive class)!\")\n",
    "    elif pos_rate < 0.05 or pos_rate > 0.95:\n",
    "        print(\"‚ö†Ô∏è Highly imbalanced target!\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Missing values detected:\")\n",
    "        print(missing_vals[missing_vals > 0])\n",
    "        print(\"Dropping rows with missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"üìä Shape after dropping missing: {X.shape}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è {inf_mask.sum()} rows with infinite values detected, dropping...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"üìä Shape after dropping infinite values: {X.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) ENHANCED DATA PREPROCESSING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Enhanced feature preprocessing.\"\"\"\n",
    "    print(\"üîß Preprocessing features...\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"‚ö†Ô∏è Removing {len(constant_cols)} constant features: {list(constant_cols)}\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"‚ö†Ô∏è Removed {n_features_before - n_features_after} low-variance features\")\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) ENHANCED CUSTOM SCORER\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Precision-weighted F-beta score with beta=0.5.\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "weighted_f_scorer = make_scorer(f_beta_half, greater_is_better=True)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5) ENHANCED PIPELINE WITH COMPREHENSIVE PARAMETER GRID\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def create_pipeline():\n",
    "    \"\"\"Create preprocessing and modeling pipeline.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(max_iter=5000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "def get_parameter_distributions():\n",
    "    \"\"\"Get comprehensive parameter distributions for different penalty types.\"\"\"\n",
    "    \n",
    "    # Custom class weights for imbalanced data\n",
    "    class_weights = [\n",
    "        None, \n",
    "        'balanced',\n",
    "        {0: 1, 1: 2},\n",
    "        {0: 1, 1: 3},\n",
    "        {0: 1, 1: 5}\n",
    "    ]\n",
    "    \n",
    "    param_distributions = [\n",
    "        # L1 (Lasso) regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['l1'],\n",
    "            \"logreg__solver\": ['liblinear', 'saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        },\n",
    "        \n",
    "        # L2 (Ridge) regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['l2'],\n",
    "            \"logreg__solver\": ['lbfgs', 'liblinear', 'newton-cg', 'saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        },\n",
    "        \n",
    "        # Elastic Net regularization\n",
    "        {\n",
    "            \"logreg__penalty\": ['elasticnet'],\n",
    "            \"logreg__solver\": ['saga'],\n",
    "            \"logreg__C\": loguniform(1e-4, 1e3),\n",
    "            \"logreg__l1_ratio\": uniform(0.01, 0.98),  # Between 0.01 and 0.99\n",
    "            \"logreg__class_weight\": class_weights,\n",
    "        }\n",
    "        \n",
    "        # Note: Removed \"No regularization\" as strong regularization is clearly beneficial for this dataset\n",
    "    ]\n",
    "    \n",
    "    return param_distributions\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6) ENHANCED HYPERPARAMETER SEARCH\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def perform_hyperparameter_search(X_train, y_train):\n",
    "    \"\"\"Perform comprehensive hyperparameter search.\"\"\"\n",
    "    \n",
    "    # Check if we have both classes in training set\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        sys.exit(\"‚ùå Training set doesn't contain both classes!\")\n",
    "    \n",
    "    pipeline = create_pipeline()\n",
    "    param_distributions = get_parameter_distributions()\n",
    "    \n",
    "    # Enhanced time series cross-validation\n",
    "    cv = TimeSeriesSplit(n_splits=8, gap=24)  # Add gap to prevent data leakage\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_estimator = None\n",
    "    best_params = None\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"üîç Starting hyperparameter search with {len(param_distributions)} parameter sets...\")\n",
    "    \n",
    "    for i, param_dist in enumerate(param_distributions):\n",
    "        penalty_type = param_dist['logreg__penalty'][0]\n",
    "        print(f\"\\nüîç Search {i+1}/{len(param_distributions)} - {penalty_type.upper()} regularization...\")\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, \n",
    "            param_distributions=param_dist,\n",
    "            n_iter=50,  # Increased iterations\n",
    "            cv=cv, \n",
    "            scoring=weighted_f_scorer,\n",
    "            random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, \n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        search_time = time.time() - t0\n",
    "        \n",
    "        print(f\"‚úÖ Search {i+1} completed in {search_time:.1f}s\")\n",
    "        print(f\"   Best CV F-beta(0.5): {search.best_score_:.4f}\")\n",
    "        print(f\"   Best params: {search.best_params_}\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'penalty': penalty_type,\n",
    "            'best_score': search.best_score_,\n",
    "            'best_params': search.best_params_,\n",
    "            'search_time': search_time\n",
    "        })\n",
    "        \n",
    "        if search.best_score_ > best_score:\n",
    "            best_score = search.best_score_\n",
    "            best_estimator = search.best_estimator_\n",
    "            best_params = search.best_params_\n",
    "    \n",
    "    return best_estimator, best_params, best_score, all_results\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 7) ENHANCED CONVERGENCE CHECKING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def check_model_convergence(estimator):\n",
    "    \"\"\"Check if the logistic regression model converged.\"\"\"\n",
    "    logreg = estimator.named_steps['logreg']\n",
    "    \n",
    "    if hasattr(logreg, 'n_iter_'):\n",
    "        n_iter = logreg.n_iter_\n",
    "        if isinstance(n_iter, np.ndarray):\n",
    "            n_iter = n_iter[0]\n",
    "        \n",
    "        max_iter = logreg.max_iter\n",
    "        if n_iter >= max_iter:\n",
    "            print(f\"‚ö†Ô∏è Model may not have converged (used {n_iter}/{max_iter} iterations)\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"‚úÖ Model converged in {n_iter} iterations\")\n",
    "            return True\n",
    "    return True\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 8) ENHANCED EVALUATION METRICS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def evaluate_model(estimator, X_test, y_test, show_detailed=True):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    \n",
    "    y_pred = estimator.predict(X_test)\n",
    "    y_prob = estimator.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'f_beta_0.5': f_beta_half(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä HOLD-OUT TEST METRICS\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"   {metric_name:<12}: {value:.4f}\")\n",
    "    \n",
    "    if show_detailed:\n",
    "        print(f\"\\nüìà DETAILED CLASSIFICATION REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        print(f\"\\nüéØ CONFUSION MATRIX\")\n",
    "        print(\"=\" * 40)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "        print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "        print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "        print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 9) FEATURE IMPORTANCE ANALYSIS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def analyze_feature_importance(estimator, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and display feature importance.\"\"\"\n",
    "    \n",
    "    logreg = estimator.named_steps['logreg']\n",
    "    coefs = logreg.coef_[0]\n",
    "    \n",
    "    # Create coefficient DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs,\n",
    "        'abs_coefficient': np.abs(coefs)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüèÖ TOP-{top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Feature':<30} {'Coefficient':<12} {'Abs Coef':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx, row in coef_df.head(top_n).iterrows():\n",
    "        print(f\"{row['feature']:<30} {row['coefficient']:<12.4f} {row['abs_coefficient']:<10.4f}\")\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 10) MAIN EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ ENHANCED LOGISTIC REGRESSION HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load and validate data\n",
    "    X, y = load_and_validate_data()\n",
    "    \n",
    "    # Chronological split\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"\\nüìä TRAIN/TEST SPLIT\")\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Train date range: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "    print(f\"   Test date range:  {X_test.index.min()} to {X_test.index.max()}\")\n",
    "    \n",
    "    # Check target distribution in splits\n",
    "    train_pos_rate = y_train.mean()\n",
    "    test_pos_rate = y_test.mean()\n",
    "    print(f\"   Train positive rate: {train_pos_rate:.3f}\")\n",
    "    print(f\"   Test positive rate:  {test_pos_rate:.3f}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    print(f\"\\nüîß FEATURE PREPROCESSING COMPLETE\")\n",
    "    print(f\"   Final feature count: {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    # Perform hyperparameter search\n",
    "    start_time = time.time()\n",
    "    best_estimator, best_params, best_score, all_results = perform_hyperparameter_search(\n",
    "        X_train_processed, y_train\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüåü HYPERPARAMETER SEARCH COMPLETED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"   Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"   Best CV F-beta(0.5): {best_score:.4f}\")\n",
    "    print(f\"\\nüèÜ BEST PARAMETERS:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   {param:<25}: {value}\")\n",
    "    \n",
    "    # Check convergence\n",
    "    print(f\"\\nüîç CONVERGENCE CHECK:\")\n",
    "    check_model_convergence(best_estimator)\n",
    "    \n",
    "    # Evaluate on hold-out test set\n",
    "    test_metrics = evaluate_model(best_estimator, X_test_processed, y_test)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance_df = analyze_feature_importance(\n",
    "        best_estimator, X_train_processed.columns\n",
    "    )\n",
    "    \n",
    "    # Save model if requested\n",
    "    if SAVE_MODEL:\n",
    "        print(f\"\\nüíæ SAVING MODEL\")\n",
    "        model_data = {\n",
    "            'model': best_estimator,\n",
    "            'best_params': best_params,\n",
    "            'best_cv_score': best_score,\n",
    "            'test_metrics': test_metrics,\n",
    "            'feature_names': list(X_train_processed.columns),\n",
    "            'feature_importance': feature_importance_df,\n",
    "            'training_info': {\n",
    "                'train_samples': len(X_train_processed),\n",
    "                'test_samples': len(X_test_processed),\n",
    "                'features_used': X_train_processed.shape[1],\n",
    "                'target_distribution': y_train.value_counts().to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, MODEL_PATH)\n",
    "        print(f\"   Model saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ HYPERPARAMETER TUNING COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "    return {\n",
    "        'best_estimator': best_estimator,\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'test_metrics': test_metrics,\n",
    "        'all_results': all_results\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 11) SCRIPT EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d0ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FINAL MODEL TRAINING & CSV GENERATION\n",
      "============================================================\n",
      "üìÇ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "üìä Dataset shape: (15855, 46)\n",
      "üìà Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "üéØ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "üìä TRAIN/TEST SPLIT SUMMARY\n",
      "----------------------------------------\n",
      "   Train: 12,684 samples (80.0%)\n",
      "   Test:  3,171 samples (20.0%)\n",
      "   Train period: 2018-01-01 00:00:00 to 2023-10-16 12:00:00\n",
      "   Test period:  2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "üîß Preprocessing features...\n",
      "‚ö†Ô∏è Removed 2 low-variance features\n",
      "‚úÖ Final feature count: 44\n",
      "\n",
      "üéØ TRAINING FINAL MODEL\n",
      "========================================\n",
      "üèÜ Using best parameters:\n",
      "   C              : 0.0016351310838425184\n",
      "   class_weight   : None\n",
      "   l1_ratio       : 0.2636043819680166\n",
      "   penalty        : elasticnet\n",
      "   solver         : saga\n",
      "   max_iter       : 5000\n",
      "   random_state   : 42\n",
      "\n",
      "‚è±Ô∏è Training model...\n",
      "‚úÖ Model trained in 0.09 seconds\n",
      "‚úÖ Model converged in 15 iterations\n",
      "\n",
      "üìä MODEL PERFORMANCE EVALUATION\n",
      "==================================================\n",
      "\n",
      "TRAINING SET METRICS:\n",
      "------------------------------\n",
      "   accuracy    : 0.5392\n",
      "   precision   : 0.5408\n",
      "   recall      : 0.6133\n",
      "   f1          : 0.5748\n",
      "   f_beta_0.5  : 0.5539\n",
      "   roc_auc     : 0.5553\n",
      "\n",
      "TEST SET METRICS:\n",
      "------------------------------\n",
      "   accuracy    : 0.5348\n",
      "   precision   : 0.5506\n",
      "   recall      : 0.5948\n",
      "   f1          : 0.5718\n",
      "   f_beta_0.5  : 0.5589\n",
      "   roc_auc     : 0.5479\n",
      "\n",
      "üìà DETAILED TEST SET ANALYSIS\n",
      "========================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49      1515\n",
      "           1       0.55      0.59      0.57      1656\n",
      "\n",
      "    accuracy                           0.53      3171\n",
      "   macro avg       0.53      0.53      0.53      3171\n",
      "weighted avg       0.53      0.53      0.53      3171\n",
      "\n",
      "\n",
      "üéØ CONFUSION MATRIX (Test Set)\n",
      "------------------------------\n",
      "True Negatives:     711\n",
      "False Positives:    804\n",
      "False Negatives:    671\n",
      "True Positives:     985\n",
      "\n",
      "üèÖ TOP-20 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "Rank Feature                        Coefficient  Abs Coef   Impact  \n",
      "----------------------------------------------------------------------\n",
      "1    buying_pressure                -0.0677      0.0677     Negative\n",
      "2    stoch_%K                       -0.0371      0.0371     Negative\n",
      "3    bb_position                    -0.0289      0.0289     Negative\n",
      "4    MACD_histogram                 -0.0058      0.0058     Negative\n",
      "5    cci_oversold                   0.0045       0.0045     Positive\n",
      "6    near_lower_band                0.0020       0.0020     Positive\n",
      "7    above_sma20                    -0.0012      0.0012     Negative\n",
      "8    SMA_50                         0.0000       0.0000     Negative\n",
      "9    EMA_21                         0.0000       0.0000     Negative\n",
      "10   EMA_7                          0.0000       0.0000     Negative\n",
      "11   close                          0.0000       0.0000     Negative\n",
      "12   volume                         0.0000       0.0000     Negative\n",
      "13   CCI                            0.0000       0.0000     Negative\n",
      "14   bollinger_width                0.0000       0.0000     Negative\n",
      "15   OBV                            0.0000       0.0000     Negative\n",
      "16   RSI                            0.0000       0.0000     Negative\n",
      "17   SMA_20                         0.0000       0.0000     Negative\n",
      "18   volume_mean_20                 0.0000       0.0000     Negative\n",
      "19   vwap_24h                       0.0000       0.0000     Negative\n",
      "20   true_range                     0.0000       0.0000     Negative\n",
      "\n",
      "üíæ GENERATING PREDICTION CSV FILES\n",
      "=============================================\n",
      "‚úÖ Training predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\train_predictions.csv\n",
      "   Shape: (12684, 11)\n",
      "‚úÖ Test predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\test_predictions.csv\n",
      "   Shape: (3171, 11)\n",
      "‚úÖ Full predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\full_predictions.csv\n",
      "   Shape: (15855, 11)\n",
      "\n",
      "üìã SAMPLE PREDICTIONS (First 10 Test Records)\n",
      "------------------------------------------------------------\n",
      "          timestamp  actual_target  predicted_target  probability_class_1  prediction_confidence\n",
      "2023-10-16 16:00:00              1                 1             0.529161               0.529161\n",
      "2023-10-16 20:00:00              0                 1             0.506090               0.506090\n",
      "2023-10-17 00:00:00              0                 1             0.537758               0.537758\n",
      "2023-10-17 04:00:00              1                 1             0.526984               0.526984\n",
      "2023-10-17 08:00:00              0                 0             0.479448               0.520552\n",
      "2023-10-17 12:00:00              1                 1             0.546537               0.546537\n",
      "2023-10-17 16:00:00              0                 0             0.449245               0.550755\n",
      "2023-10-17 20:00:00              0                 1             0.503812               0.503812\n",
      "2023-10-18 00:00:00              1                 1             0.535821               0.535821\n",
      "2023-10-18 04:00:00              0                 0             0.483338               0.516662\n",
      "\n",
      "üíæ SAVING COMPLETE MODEL\n",
      "==============================\n",
      "‚úÖ Complete model package saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\final_logistic_model.pkl\n",
      "\n",
      "üéâ MODEL TRAINING & CSV GENERATION COMPLETED!\n",
      "=======================================================\n",
      "üìÅ Output Files Generated in: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\n",
      "   üî∏ train_predictions.csv\n",
      "   üî∏ test_predictions.csv\n",
      "   üî∏ full_predictions.csv\n",
      "   üéØ bitcoin_predictions_with_probabilities.csv (MAIN PREDICTIONS FILE)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  FINAL MODEL TRAINING & CSV OUTPUT GENERATOR\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) CONFIGURATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TRAIN_OUTPUT_CSV = OUTPUT_DIR / \"train_predictions.csv\"\n",
    "TEST_OUTPUT_CSV = OUTPUT_DIR / \"test_predictions.csv\"\n",
    "FULL_OUTPUT_CSV = OUTPUT_DIR / \"full_predictions.csv\"\n",
    "MAIN_PREDICTIONS_CSV = OUTPUT_DIR / \"bitcoin_predictions_with_probabilities.csv\"\n",
    "MODEL_PATH = OUTPUT_DIR / \"final_logistic_model.pkl\"\n",
    "\n",
    "# BEST PARAMETERS FROM HYPERPARAMETER TUNING\n",
    "BEST_PARAMS = {\n",
    "    'C': 0.0016351310838425184,\n",
    "    'class_weight': None,\n",
    "    'l1_ratio': 0.2636043819680166,\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 5000,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) DATA LOADING & PREPROCESSING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data for final model training.\"\"\"\n",
    "    print(\"üöÄ FINAL MODEL TRAINING & CSV GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"‚ùå File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"üìÇ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"‚ùå '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    print(f\"üìä Dataset shape: {X.shape}\")\n",
    "    print(f\"üìà Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"üéØ Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle missing and infinite values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Handling missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"üìä Shape after cleaning: {X.shape}\")\n",
    "    \n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è Handling {inf_mask.sum()} rows with infinite values...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"üìä Final shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, df.index[~(missing_vals.sum() > 0 or inf_mask.sum() > 0) if (missing_vals.sum() > 0 or inf_mask.sum() > 0) else slice(None)]\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Preprocess features (same as in hyperparameter tuning).\"\"\"\n",
    "    print(\"üîß Preprocessing features...\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"‚ö†Ô∏è Removing {len(constant_cols)} constant features\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"‚ö†Ô∏è Removed {n_features_before - n_features_after} low-variance features\")\n",
    "    \n",
    "    print(f\"‚úÖ Final feature count: {n_features_after}\")\n",
    "    return X_train_selected, X_test_selected, variance_selector\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) MODEL TRAINING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def create_final_pipeline():\n",
    "    \"\"\"Create the final pipeline with best parameters.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(**BEST_PARAMS))\n",
    "    ])\n",
    "\n",
    "def train_final_model(X_train, y_train):\n",
    "    \"\"\"Train the final model with best parameters.\"\"\"\n",
    "    print(\"\\nüéØ TRAINING FINAL MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    pipeline = create_final_pipeline()\n",
    "    \n",
    "    print(\"üèÜ Using best parameters:\")\n",
    "    for param, value in BEST_PARAMS.items():\n",
    "        print(f\"   {param:<15}: {value}\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è Training model...\")\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check convergence\n",
    "    logreg = pipeline.named_steps['logreg']\n",
    "    if hasattr(logreg, 'n_iter_'):\n",
    "        n_iter = logreg.n_iter_[0] if isinstance(logreg.n_iter_, np.ndarray) else logreg.n_iter_\n",
    "        if n_iter >= logreg.max_iter:\n",
    "            print(f\"‚ö†Ô∏è Model may not have converged (used {n_iter}/{logreg.max_iter} iterations)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Model converged in {n_iter} iterations\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) EVALUATION & METRICS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Custom F-beta score with beta=0.5 (precision-weighted).\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "def evaluate_model_performance(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    print(\"\\nüìä MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training set predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Test set predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for both sets\n",
    "    def calculate_metrics(y_true, y_pred, y_prob, set_name):\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'f_beta_0.5': f_beta_half(y_true, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{set_name.upper()} SET METRICS:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"   {metric_name:<12}: {value:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_prob, \"training\")\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_prob, \"test\")\n",
    "    \n",
    "    # Detailed test set analysis\n",
    "    print(f\"\\nüìà DETAILED TEST SET ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "    \n",
    "    print(f\"\\nüéØ CONFUSION MATRIX (Test Set)\")\n",
    "    print(\"-\" * 30)\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    return {\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': y_train_pred,\n",
    "        'train_probabilities': y_train_prob,\n",
    "        'test_predictions': y_test_pred,\n",
    "        'test_probabilities': y_test_prob\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5) FEATURE IMPORTANCE ANALYSIS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def analyze_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and display feature importance.\"\"\"\n",
    "    logreg = model.named_steps['logreg']\n",
    "    coefs = logreg.coef_[0]\n",
    "    \n",
    "    # Create coefficient DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs,\n",
    "        'abs_coefficient': np.abs(coefs),\n",
    "        'importance_rank': range(1, len(coefs) + 1)\n",
    "    }).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    coef_df['importance_rank'] = range(1, len(coef_df) + 1)\n",
    "    \n",
    "    print(f\"\\nüèÖ TOP-{top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<4} {'Feature':<30} {'Coefficient':<12} {'Abs Coef':<10} {'Impact':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx, row in coef_df.head(top_n).iterrows():\n",
    "        impact = \"Positive\" if row['coefficient'] > 0 else \"Negative\"\n",
    "        print(f\"{row['importance_rank']:<4} {row['feature']:<30} {row['coefficient']:<12.4f} {row['abs_coefficient']:<10.4f} {impact:<8}\")\n",
    "    \n",
    "    return coef_df\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6) CSV OUTPUT GENERATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_prediction_csvs(X_train, X_test, y_train, y_test, results, train_dates, test_dates):\n",
    "    \"\"\"Generate comprehensive CSV outputs with predictions and probabilities.\"\"\"\n",
    "    print(\"\\nüíæ GENERATING PREDICTION CSV FILES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Training set CSV\n",
    "    train_df = pd.DataFrame({\n",
    "        'timestamp': train_dates,\n",
    "        'actual_target': y_train.values,\n",
    "        'predicted_target': results['train_predictions'],\n",
    "        'probability_class_0': 1 - results['train_probabilities'],\n",
    "        'probability_class_1': results['train_probabilities'],\n",
    "        'prediction_confidence': np.maximum(results['train_probabilities'], \n",
    "                                          1 - results['train_probabilities']),\n",
    "        'correct_prediction': (y_train.values == results['train_predictions']).astype(int),\n",
    "        'set_type': 'train'\n",
    "    })\n",
    "    \n",
    "    # Test set CSV\n",
    "    test_df = pd.DataFrame({\n",
    "        'timestamp': test_dates,\n",
    "        'actual_target': y_test.values,\n",
    "        'predicted_target': results['test_predictions'],\n",
    "        'probability_class_0': 1 - results['test_probabilities'],\n",
    "        'probability_class_1': results['test_probabilities'],\n",
    "        'prediction_confidence': np.maximum(results['test_probabilities'], \n",
    "                                          1 - results['test_probabilities']),\n",
    "        'correct_prediction': (y_test.values == results['test_predictions']).astype(int),\n",
    "        'set_type': 'test'\n",
    "    })\n",
    "    \n",
    "    # Full dataset CSV\n",
    "    full_df = pd.concat([train_df, test_df], ignore_index=True).sort_values('timestamp')\n",
    "    \n",
    "    # Add additional analysis columns\n",
    "    for df in [train_df, test_df, full_df]:\n",
    "        df['prediction_type'] = df.apply(lambda row: \n",
    "            'True Positive' if row['actual_target'] == 1 and row['predicted_target'] == 1\n",
    "            else 'True Negative' if row['actual_target'] == 0 and row['predicted_target'] == 0\n",
    "            else 'False Positive' if row['actual_target'] == 0 and row['predicted_target'] == 1\n",
    "            else 'False Negative', axis=1)\n",
    "        \n",
    "        df['high_confidence'] = (df['prediction_confidence'] >= 0.7).astype(int)\n",
    "        df['very_high_confidence'] = (df['prediction_confidence'] >= 0.8).astype(int)\n",
    "    \n",
    "    # Save CSV files\n",
    "    train_df.to_csv(TRAIN_OUTPUT_CSV, index=False)\n",
    "    test_df.to_csv(TEST_OUTPUT_CSV, index=False)\n",
    "    full_df.to_csv(FULL_OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Training predictions saved to: {TRAIN_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {train_df.shape}\")\n",
    "    print(f\"‚úÖ Test predictions saved to: {TEST_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {test_df.shape}\")\n",
    "    print(f\"‚úÖ Full predictions saved to: {FULL_OUTPUT_CSV}\")\n",
    "    print(f\"   Shape: {full_df.shape}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\nüìã SAMPLE PREDICTIONS (First 10 Test Records)\")\n",
    "    print(\"-\" * 60)\n",
    "    sample_cols = ['timestamp', 'actual_target', 'predicted_target', 'probability_class_1', 'prediction_confidence']\n",
    "    print(test_df[sample_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    return train_df, test_df, full_df\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 7) MODEL PERSISTENCE\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def save_complete_model(model, feature_names, variance_selector, results, coef_df):\n",
    "    \"\"\"Save the complete model with all metadata.\"\"\"\n",
    "    print(f\"\\nüíæ SAVING COMPLETE MODEL\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'variance_selector': variance_selector,\n",
    "        'best_parameters': BEST_PARAMS,\n",
    "        'performance_metrics': {\n",
    "            'train_metrics': results['train_metrics'],\n",
    "            'test_metrics': results['test_metrics']\n",
    "        },\n",
    "        'feature_importance': coef_df,\n",
    "        'model_info': {\n",
    "            'training_date': pd.Timestamp.now(),\n",
    "            'scikit_learn_version': '1.3+',\n",
    "            'total_features': len(feature_names),\n",
    "            'dropped_columns': DROP_COLS,\n",
    "            'preprocessing_steps': ['StandardScaler', 'VarianceThreshold'],\n",
    "            'algorithm': 'LogisticRegression with ElasticNet'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_package, MODEL_PATH)\n",
    "    print(f\"‚úÖ Complete model package saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 8) MAIN EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X, y, original_dates = load_and_prepare_data()\n",
    "    \n",
    "    # Chronological split (same as hyperparameter tuning)\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    train_dates = X.index[:split_idx]\n",
    "    test_dates = X.index[split_idx:]\n",
    "    \n",
    "    print(f\"\\nüìä TRAIN/TEST SPLIT SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Train period: {train_dates.min()} to {train_dates.max()}\")\n",
    "    print(f\"   Test period:  {test_dates.min()} to {test_dates.max()}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed, variance_selector = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    # Train final model\n",
    "    final_model = train_final_model(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model_performance(final_model, X_train_processed, y_train, X_test_processed, y_test)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance_df = analyze_feature_importance(final_model, X_train_processed.columns)\n",
    "    \n",
    "    # Generate CSV outputs\n",
    "    train_csv, test_csv, full_csv = generate_prediction_csvs(\n",
    "        X_train_processed, X_test_processed, y_train, y_test, results, train_dates, test_dates\n",
    "    )\n",
    "    \n",
    "    # Save complete model\n",
    "    model_package = save_complete_model(\n",
    "        final_model, list(X_train_processed.columns), variance_selector, results, feature_importance_df\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ MODEL TRAINING & CSV GENERATION COMPLETED!\")\n",
    "    print(\"=\" * 55)\n",
    "    print(f\"üìÅ Output Files Generated in: {OUTPUT_DIR}\")\n",
    "    print(f\"   üî∏ {TRAIN_OUTPUT_CSV.name}\")\n",
    "    print(f\"   üî∏ {TEST_OUTPUT_CSV.name}\")\n",
    "    print(f\"   üî∏ {FULL_OUTPUT_CSV.name}\")\n",
    "    print(f\"   üéØ {MAIN_PREDICTIONS_CSV.name} (MAIN PREDICTIONS FILE)\")\n",
    "    \n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'results': results,\n",
    "        'csvs': {'train': train_csv, 'test': test_csv, 'full': full_csv},\n",
    "        'feature_importance': feature_importance_df,\n",
    "        'model_package': model_package\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 9) SCRIPT EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3a8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PRECISION-FOCUSED PARAMETER COMPARISON\n",
      "======================================================================\n",
      "üìÇ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "üìä Data loaded: 15,855 samples\n",
      "   Train: 12,684 samples\n",
      "   Test:  3,171 samples\n",
      "üîß Features after preprocessing: 44\n",
      "üß™ Testing 12 parameter combinations...\n",
      "\n",
      "================================================================================\n",
      "Model                     Precision  Recall   F1       F-Œ≤(0.5) Time(s) \n",
      "================================================================================\n",
      "\n",
      "üîç Testing 1/12: Original_Best_ElasticNet\n",
      "Original_Best_ElasticNet  0.5506     0.5948   0.5718   0.5589   0.08    \n",
      "\n",
      "üîç Testing 2/12: Original_Best_L1\n",
      "Original_Best_L1          0.5222     1.0000   0.6861   0.5774   6.60    \n",
      "\n",
      "üîç Testing 3/12: L1_Ultra_Conservative\n",
      "L1_Ultra_Conservative     0.5222     1.0000   0.6861   0.5774   5.58    \n",
      "\n",
      "üîç Testing 4/12: L1_Extreme_Conservative\n",
      "L1_Extreme_Conservative   0.5222     1.0000   0.6861   0.5774   2.22    \n",
      "\n",
      "üîç Testing 5/12: L1_Higher_Regularization\n",
      "L1_Higher_Regularization  0.5222     1.0000   0.6861   0.5774   5.54    \n",
      "\n",
      "üîç Testing 6/12: L1_Lower_Regularization\n",
      "L1_Lower_Regularization   0.5222     1.0000   0.6861   0.5774   5.41    \n",
      "\n",
      "üîç Testing 7/12: L1_SAGA_Solver\n",
      "L1_SAGA_Solver            0.5222     1.0000   0.6861   0.5774   22.18   \n",
      "   ‚ö†Ô∏è Model may not have converged!\n",
      "\n",
      "üîç Testing 8/12: L2_Aggressive_Weighting\n",
      "L2_Aggressive_Weighting   0.5222     1.0000   0.6861   0.5774   0.02    \n",
      "\n",
      "üîç Testing 9/12: L2_Ultra_Conservative\n",
      "L2_Ultra_Conservative     0.5222     1.0000   0.6861   0.5774   0.01    \n",
      "\n",
      "üîç Testing 10/12: ElasticNet_Weighted\n",
      "ElasticNet_Weighted       0.5222     1.0000   0.6861   0.5774   0.08    \n",
      "\n",
      "üîç Testing 11/12: ElasticNet_Aggressive\n",
      "ElasticNet_Aggressive     0.5222     1.0000   0.6861   0.5774   0.09    \n",
      "\n",
      "üîç Testing 12/12: ElasticNet_More_L1\n",
      "ElasticNet_More_L1        0.5222     1.0000   0.6861   0.5774   0.08    \n",
      "\n",
      "üèÜ FINAL PRECISION RANKINGS\n",
      "====================================================================================================\n",
      "Rank Model                     Precision  Recall   F1       F-Œ≤(0.5) Accuracy ROC-AUC \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1    Original_Best_ElasticNet  0.5506     0.5948   0.5718   0.5589   0.5348   0.5479  \n",
      "2    Original_Best_L1          0.5222     1.0000   0.6861   0.5774   0.5222   0.5449  \n",
      "3    L1_Ultra_Conservative     0.5222     1.0000   0.6861   0.5774   0.5222   0.5459  \n",
      "4    L1_Extreme_Conservative   0.5222     1.0000   0.6861   0.5774   0.5222   0.5497  \n",
      "5    L1_Higher_Regularization  0.5222     1.0000   0.6861   0.5774   0.5222   0.5456  \n",
      "6    L1_Lower_Regularization   0.5222     1.0000   0.6861   0.5774   0.5222   0.5455  \n",
      "7    L1_SAGA_Solver            0.5222     1.0000   0.6861   0.5774   0.5222   0.5442  \n",
      "8    L2_Aggressive_Weighting   0.5222     1.0000   0.6861   0.5774   0.5222   0.5509  \n",
      "9    L2_Ultra_Conservative     0.5222     1.0000   0.6861   0.5774   0.5222   0.5508  \n",
      "10   ElasticNet_Weighted       0.5222     1.0000   0.6861   0.5774   0.5222   0.5487  \n",
      "11   ElasticNet_Aggressive     0.5222     1.0000   0.6861   0.5774   0.5222   0.5489  \n",
      "12   ElasticNet_More_L1        0.5222     1.0000   0.6861   0.5774   0.5222   0.5463  \n",
      "\n",
      "üíæ Results saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\precision_comparison_results.csv\n",
      "\n",
      "ü•á TOP 3 PRECISION MODELS - DETAILED ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üèÖ RANK 1: Original_Best_ElasticNet\n",
      "   Description: Your best hyperparameter search result\n",
      "   Precision:   0.5506\n",
      "   Recall:      0.5948\n",
      "   F1-Score:    0.5718\n",
      "   F-Œ≤(0.5):    0.5589\n",
      "   Accuracy:    0.5348\n",
      "   Training:    0.08s\n",
      "   Converged:   True\n",
      "   Parameters:\n",
      "     C: 0.0016351310838425184\n",
      "     class_weight: None\n",
      "     l1_ratio: 0.2636043819680166\n",
      "     penalty: elasticnet\n",
      "     solver: saga\n",
      "     max_iter: 5000\n",
      "     random_state: 42\n",
      "\n",
      "üèÖ RANK 2: Original_Best_L1\n",
      "   Description: Best L1 result from your search\n",
      "   Precision:   0.5222\n",
      "   Recall:      1.0000\n",
      "   F1-Score:    0.6861\n",
      "   F-Œ≤(0.5):    0.5774\n",
      "   Accuracy:    0.5222\n",
      "   Training:    6.60s\n",
      "   Converged:   True\n",
      "   Parameters:\n",
      "     C: 104.6123232641137\n",
      "     class_weight: {0: 1, 1: 5}\n",
      "     l1_ratio: nan\n",
      "     penalty: l1\n",
      "     solver: liblinear\n",
      "     max_iter: 5000\n",
      "     random_state: 42\n",
      "\n",
      "üèÖ RANK 3: L1_Ultra_Conservative\n",
      "   Description: L1 with even more aggressive class weighting\n",
      "   Precision:   0.5222\n",
      "   Recall:      1.0000\n",
      "   F1-Score:    0.6861\n",
      "   F-Œ≤(0.5):    0.5774\n",
      "   Accuracy:    0.5222\n",
      "   Training:    5.58s\n",
      "   Converged:   True\n",
      "   Parameters:\n",
      "     C: 104.6123232641137\n",
      "     class_weight: {0: 1, 1: 10}\n",
      "     l1_ratio: nan\n",
      "     penalty: l1\n",
      "     solver: liblinear\n",
      "     max_iter: 5000\n",
      "     random_state: 42\n",
      "\n",
      "‚úÖ PRECISION COMPARISON COMPLETED!\n",
      "üìÅ Detailed results saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\precision_comparison_results.csv\n",
      "üéØ Total models tested: 12\n",
      "\n",
      "üèÜ BEST PRECISION MODEL: Original_Best_ElasticNet\n",
      "   Precision: 0.5506\n",
      "   Recall:    0.5948\n",
      "   F1-Score:  0.5718\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  PRECISION-FOCUSED PARAMETER COMPARISON RUNNER\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) CONFIGURATION (Same as your main script)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "COMPARISON_CSV = OUTPUT_DIR / \"precision_comparison_results.csv\"\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) PARAMETER COMBINATIONS TO TEST\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_precision_test_parameters():\n",
    "    \"\"\"Get all parameter combinations to test for precision optimization.\"\"\"\n",
    "    \n",
    "    test_params = [\n",
    "        # 1. Original best Elastic Net (baseline)\n",
    "        {\n",
    "            'name': 'Original_Best_ElasticNet',\n",
    "            'description': 'Your best hyperparameter search result',\n",
    "            'C': 0.0016351310838425184,\n",
    "            'class_weight': None,\n",
    "            'l1_ratio': 0.2636043819680166,\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga'\n",
    "        },\n",
    "        \n",
    "        # 2. Best L1 from your search\n",
    "        {\n",
    "            'name': 'Original_Best_L1',\n",
    "            'description': 'Best L1 result from your search',\n",
    "            'C': 104.6123232641137,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        },\n",
    "        \n",
    "        # 3. L1 with ultra-conservative weighting\n",
    "        {\n",
    "            'name': 'L1_Ultra_Conservative',\n",
    "            'description': 'L1 with even more aggressive class weighting',\n",
    "            'C': 104.6123232641137,\n",
    "            'class_weight': {0: 1, 1: 10},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        },\n",
    "        \n",
    "        # 4. L1 with extreme conservative weighting\n",
    "        {\n",
    "            'name': 'L1_Extreme_Conservative',\n",
    "            'description': 'L1 with maximum class weighting',\n",
    "            'C': 104.6123232641137,\n",
    "            'class_weight': {0: 1, 1: 20},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        },\n",
    "        \n",
    "        # 5. L1 with higher regularization\n",
    "        {\n",
    "            'name': 'L1_Higher_Regularization',\n",
    "            'description': 'L1 with more regularization (lower C)',\n",
    "            'C': 50.0,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        },\n",
    "        \n",
    "        # 6. L1 with lower regularization\n",
    "        {\n",
    "            'name': 'L1_Lower_Regularization',\n",
    "            'description': 'L1 with less regularization (higher C)',\n",
    "            'C': 200.0,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'liblinear'\n",
    "        },\n",
    "        \n",
    "        # 7. L1 with SAGA solver\n",
    "        {\n",
    "            'name': 'L1_SAGA_Solver',\n",
    "            'description': 'L1 with SAGA solver instead of liblinear',\n",
    "            'C': 104.6123232641137,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'penalty': 'l1',\n",
    "            'solver': 'saga'\n",
    "        },\n",
    "        \n",
    "        # 8. L2 with aggressive weighting\n",
    "        {\n",
    "            'name': 'L2_Aggressive_Weighting',\n",
    "            'description': 'Best L2 C value with heavy class weighting',\n",
    "            'C': 0.00028533901052402264,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'penalty': 'l2',\n",
    "            'solver': 'lbfgs'\n",
    "        },\n",
    "        \n",
    "        # 9. L2 with ultra-conservative weighting\n",
    "        {\n",
    "            'name': 'L2_Ultra_Conservative',\n",
    "            'description': 'L2 with very aggressive class weighting',\n",
    "            'C': 0.00028533901052402264,\n",
    "            'class_weight': {0: 1, 1: 10},\n",
    "            'penalty': 'l2',\n",
    "            'solver': 'lbfgs'\n",
    "        },\n",
    "        \n",
    "        # 10. Elastic Net with class weighting\n",
    "        {\n",
    "            'name': 'ElasticNet_Weighted',\n",
    "            'description': 'Best ElasticNet with class weighting',\n",
    "            'C': 0.0016351310838425184,\n",
    "            'class_weight': {0: 1, 1: 3},\n",
    "            'l1_ratio': 0.2636043819680166,\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga'\n",
    "        },\n",
    "        \n",
    "        # 11. Elastic Net with aggressive weighting\n",
    "        {\n",
    "            'name': 'ElasticNet_Aggressive',\n",
    "            'description': 'Best ElasticNet with aggressive class weighting',\n",
    "            'C': 0.0016351310838425184,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'l1_ratio': 0.2636043819680166,\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga'\n",
    "        },\n",
    "        \n",
    "        # 12. Elastic Net with more L1 emphasis\n",
    "        {\n",
    "            'name': 'ElasticNet_More_L1',\n",
    "            'description': 'ElasticNet with higher L1 ratio for sparsity',\n",
    "            'C': 0.0016351310838425184,\n",
    "            'class_weight': {0: 1, 1: 5},\n",
    "            'l1_ratio': 0.8,\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return test_params\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) DATA LOADING (Same as your main script)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data (identical to your main script).\"\"\"\n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"‚ùå File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"üìÇ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"‚ùå '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    # Handle missing and infinite values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "    \n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Preprocess features (identical to your main script).\"\"\"\n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) CUSTOM METRICS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Custom F-beta score with beta=0.5 (precision-weighted).\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5) MODEL TESTING FUNCTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def test_parameter_combination(params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Test a single parameter combination and return metrics.\"\"\"\n",
    "    \n",
    "    # Create model parameters (remove non-sklearn params)\n",
    "    model_params = {k: v for k, v in params.items() \n",
    "                   if k not in ['name', 'description']}\n",
    "    model_params.update({\n",
    "        'max_iter': 5000,\n",
    "        'random_state': RANDOM_STATE\n",
    "    })\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'name': params['name'],\n",
    "            'description': params['description'],\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'f_beta_0.5': f_beta_half(y_test, y_pred),\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.0,\n",
    "            'training_time': training_time,\n",
    "            'converged': True\n",
    "        }\n",
    "        \n",
    "        # Check convergence\n",
    "        logreg = pipeline.named_steps['logreg']\n",
    "        if hasattr(logreg, 'n_iter_'):\n",
    "            n_iter = logreg.n_iter_[0] if isinstance(logreg.n_iter_, np.ndarray) else logreg.n_iter_\n",
    "            metrics['n_iterations'] = n_iter\n",
    "            metrics['converged'] = n_iter < logreg.max_iter\n",
    "        else:\n",
    "            metrics['n_iterations'] = 'N/A'\n",
    "        \n",
    "        # Add parameter details\n",
    "        metrics.update({f'param_{k}': str(v) for k, v in model_params.items()})\n",
    "        \n",
    "        return metrics, pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {params['name']}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6) MAIN COMPARISON FUNCTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_precision_comparison():\n",
    "    \"\"\"Run comprehensive precision comparison across all parameter combinations.\"\"\"\n",
    "    \n",
    "    print(\"üéØ PRECISION-FOCUSED PARAMETER COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X, y = load_and_prepare_data()\n",
    "    \n",
    "    # Split data (same as your main script)\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"üìä Data loaded: {X.shape[0]:,} samples\")\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed = preprocess_features(X_train, X_test)\n",
    "    print(f\"üîß Features after preprocessing: {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    # Get parameter combinations to test\n",
    "    test_parameters = get_precision_test_parameters()\n",
    "    print(f\"üß™ Testing {len(test_parameters)} parameter combinations...\")\n",
    "    \n",
    "    # Test each combination\n",
    "    results = []\n",
    "    successful_models = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{'Model':<25} {'Precision':<10} {'Recall':<8} {'F1':<8} {'F-Œ≤(0.5)':<8} {'Time(s)':<8}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i, params in enumerate(test_parameters):\n",
    "        print(f\"\\nüîç Testing {i+1}/{len(test_parameters)}: {params['name']}\")\n",
    "        \n",
    "        metrics, model = test_parameter_combination(\n",
    "            params, X_train_processed, y_train, X_test_processed, y_test\n",
    "        )\n",
    "        \n",
    "        if metrics is not None:\n",
    "            results.append(metrics)\n",
    "            successful_models.append((params['name'], model))\n",
    "            \n",
    "            # Print immediate results\n",
    "            print(f\"{params['name']:<25} {metrics['precision']:<10.4f} {metrics['recall']:<8.4f} \"\n",
    "                  f\"{metrics['f1']:<8.4f} {metrics['f_beta_0.5']:<8.4f} {metrics['training_time']:<8.2f}\")\n",
    "            \n",
    "            if not metrics['converged']:\n",
    "                print(f\"   ‚ö†Ô∏è Model may not have converged!\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by precision (primary), then by F-beta(0.5)\n",
    "    results_df = results_df.sort_values(['precision', 'f_beta_0.5'], ascending=[False, False])\n",
    "    \n",
    "    # Display final rankings\n",
    "    print(f\"\\nüèÜ FINAL PRECISION RANKINGS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Rank':<4} {'Model':<25} {'Precision':<10} {'Recall':<8} {'F1':<8} {'F-Œ≤(0.5)':<8} {'Accuracy':<8} {'ROC-AUC':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(results_df.iterrows()):\n",
    "        rank = idx + 1\n",
    "        print(f\"{rank:<4} {row['name']:<25} {row['precision']:<10.4f} {row['recall']:<8.4f} \"\n",
    "              f\"{row['f1']:<8.4f} {row['f_beta_0.5']:<8.4f} {row['accuracy']:<8.4f} {row['roc_auc']:<8.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(COMPARISON_CSV, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {COMPARISON_CSV}\")\n",
    "    \n",
    "    # Show top 3 models in detail\n",
    "    print(f\"\\nü•á TOP 3 PRECISION MODELS - DETAILED ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for rank in range(min(3, len(results_df))):\n",
    "        row = results_df.iloc[rank]\n",
    "        print(f\"\\nüèÖ RANK {rank + 1}: {row['name']}\")\n",
    "        print(f\"   Description: {row['description']}\")\n",
    "        print(f\"   Precision:   {row['precision']:.4f}\")\n",
    "        print(f\"   Recall:      {row['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:    {row['f1']:.4f}\")\n",
    "        print(f\"   F-Œ≤(0.5):    {row['f_beta_0.5']:.4f}\")\n",
    "        print(f\"   Accuracy:    {row['accuracy']:.4f}\")\n",
    "        print(f\"   Training:    {row['training_time']:.2f}s\")\n",
    "        print(f\"   Converged:   {row['converged']}\")\n",
    "        \n",
    "        # Show key parameters\n",
    "        param_cols = [col for col in row.index if col.startswith('param_')]\n",
    "        if param_cols:\n",
    "            print(f\"   Parameters:\")\n",
    "            for param_col in param_cols:\n",
    "                param_name = param_col.replace('param_', '')\n",
    "                print(f\"     {param_name}: {row[param_col]}\")\n",
    "    \n",
    "    return results_df, successful_models\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 7) SCRIPT EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, models = run_precision_comparison()\n",
    "    \n",
    "    print(f\"\\n‚úÖ PRECISION COMPARISON COMPLETED!\")\n",
    "    print(f\"üìÅ Detailed results saved to: {COMPARISON_CSV}\")\n",
    "    print(f\"üéØ Total models tested: {len(results_df)}\")\n",
    "    \n",
    "    # Quick summary\n",
    "    best_precision = results_df.iloc[0]\n",
    "    print(f\"\\nüèÜ BEST PRECISION MODEL: {best_precision['name']}\")\n",
    "    print(f\"   Precision: {best_precision['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {best_precision['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {best_precision['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcec99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ BEST PRECISION MODEL TRAINING\n",
      "==================================================\n",
      "üìÇ Loading data from: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\n",
      "üìä Dataset shape: (15855, 46)\n",
      "üìà Date range: 2018-01-01 00:00:00 to 2025-03-28 00:00:00\n",
      "üéØ Target distribution: {1: 8097, 0: 7758}\n",
      "\n",
      "üìä TRAIN/TEST SPLIT:\n",
      "-------------------------\n",
      "   Train: 12,684 samples\n",
      "   Test:  3,171 samples\n",
      "   Train period: 2018-01-01 00:00:00 to 2023-10-16 12:00:00\n",
      "   Test period:  2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "\n",
      "üîß Preprocessing features...\n",
      "‚ö†Ô∏è Removed 2 low-variance features\n",
      "‚úÖ Final feature count: 44\n",
      "\n",
      "üéØ TRAINING BEST PRECISION MODEL\n",
      "=============================================\n",
      "üèÜ Using best precision parameters:\n",
      "   C              : 0.0016351310838425184\n",
      "   class_weight   : None\n",
      "   l1_ratio       : 0.2636043819680166\n",
      "   penalty        : elasticnet\n",
      "   solver         : saga\n",
      "   max_iter       : 5000\n",
      "   random_state   : 42\n",
      "\n",
      "‚è±Ô∏è Training model...\n",
      "‚úÖ Model trained in 0.08 seconds\n",
      "‚úÖ Model converged in 15 iterations\n",
      "\n",
      "üìä MODEL EVALUATION\n",
      "==============================\n",
      "üìà TEST SET METRICS:\n",
      "-------------------------\n",
      "   accuracy    : 0.5348\n",
      "   precision   : 0.5506\n",
      "   recall      : 0.5948\n",
      "   f1          : 0.5718\n",
      "   f_beta_0.5  : 0.5589\n",
      "   roc_auc     : 0.5479\n",
      "\n",
      "üéØ CONFUSION MATRIX:\n",
      "-------------------------\n",
      "True Negatives:     711\n",
      "False Positives:    804\n",
      "False Negatives:    671\n",
      "True Positives:     985\n",
      "\n",
      "üíæ GENERATING PREDICTIONS CSV\n",
      "===================================\n",
      "‚úÖ Predictions saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\best_precision_predictions.csv\n",
      "üìä Total predictions: 15,855\n",
      "   Train predictions: 12,684\n",
      "   Test predictions:  3,171\n",
      "\n",
      "üìã SAMPLE PREDICTIONS (First 10 rows):\n",
      "------------------------------------------------------------\n",
      "          timestamp  prediction_of_up  prediction_of_down  final_prediction  actual_value\n",
      "2018-01-01 00:00:00            0.5289              0.4711                 1             0\n",
      "2018-01-01 04:00:00            0.4971              0.5029                 0             0\n",
      "2018-01-01 08:00:00            0.5370              0.4630                 1             0\n",
      "2018-01-01 12:00:00            0.5519              0.4481                 1             0\n",
      "2018-01-01 16:00:00            0.5424              0.4576                 1             1\n",
      "2018-01-01 20:00:00            0.4801              0.5199                 0             1\n",
      "2018-01-02 00:00:00            0.4724              0.5276                 0             0\n",
      "2018-01-02 04:00:00            0.5482              0.4518                 1             1\n",
      "2018-01-02 08:00:00            0.4759              0.5241                 0             1\n",
      "2018-01-02 12:00:00            0.4802              0.5198                 0             1\n",
      "\n",
      "üìà PREDICTION SUMMARY:\n",
      "-------------------------\n",
      "Overall accuracy: 0.5383\n",
      "Test accuracy:    0.5348\n",
      "Average confidence: 0.5262\n",
      "High confidence predictions (>0.7): 0\n",
      "\n",
      "üíæ SAVING BEST PRECISION MODEL\n",
      "===================================\n",
      "‚úÖ Model saved to: C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\best_precision_model.pkl\n",
      "\n",
      "üéâ BEST PRECISION MODEL TRAINING COMPLETED!\n",
      "==================================================\n",
      "üìÅ Files Generated:\n",
      "   üî∏ Predictions: best_precision_predictions.csv\n",
      "   üî∏ Model:       best_precision_model.pkl\n",
      "\n",
      "üèÜ Final Test Precision: 0.5506\n",
      "üéØ Final Test F1-Score:  0.5718\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  BEST PRECISION MODEL TRAINER & PREDICTION CSV GENERATOR\n",
    "# =============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, \n",
    "    f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) CONFIGURATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CSV_FILE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "TIME_COL = \"timestamp\"\n",
    "TARGET_COL = \"target\"\n",
    "START_DATE = \"2018-01-01\"\n",
    "TEST_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "PREDICTIONS_CSV = OUTPUT_DIR / \"best_precision_predictions.csv\"\n",
    "MODEL_PATH = OUTPUT_DIR / \"best_precision_model.pkl\"\n",
    "\n",
    "# BEST PRECISION PARAMETERS (Original_Best_ElasticNet)\n",
    "BEST_PRECISION_PARAMS = {\n",
    "    'C': 0.0016351310838425184,\n",
    "    'class_weight': None,\n",
    "    'l1_ratio': 0.2636043819680166,\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 5000,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'overbought_reversal', 'oversold_reversal', 'macd_cross_up',\n",
    "    'macd_cross_down', 'macd_rising', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D', 'momentum_alignment',\n",
    "    'bullish_scenario_1', 'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) DATA LOADING & PREPROCESSING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data for best precision model training.\"\"\"\n",
    "    print(\"üèÜ BEST PRECISION MODEL TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not CSV_FILE.exists():\n",
    "        sys.exit(f\"‚ùå File not found: {CSV_FILE}\")\n",
    "    \n",
    "    print(f\"üìÇ Loading data from: {CSV_FILE}\")\n",
    "    df = pd.read_csv(CSV_FILE, parse_dates=[TIME_COL]).set_index(TIME_COL).sort_index()\n",
    "    df = df.loc[START_DATE:].copy()\n",
    "    \n",
    "    if TARGET_COL not in df.columns:\n",
    "        sys.exit(f\"‚ùå '{TARGET_COL}' column missing!\")\n",
    "    \n",
    "    # Remove specified columns\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\")\n",
    "    y = df[TARGET_COL]\n",
    "    \n",
    "    print(f\"üìä Dataset shape: {X.shape}\")\n",
    "    print(f\"üìà Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"üéØ Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle missing and infinite values\n",
    "    original_size = len(X)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_vals = X.isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è Handling {missing_vals.sum()} missing values...\")\n",
    "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X, y = X[mask], y[mask]\n",
    "        print(f\"üìä Shape after removing missing: {X.shape}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_mask = np.isinf(X.select_dtypes(include=[np.number])).any(axis=1)\n",
    "    if inf_mask.sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è Handling {inf_mask.sum()} infinite values...\")\n",
    "        X, y = X[~inf_mask], y[~inf_mask]\n",
    "        print(f\"üìä Final shape: {X.shape}\")\n",
    "    \n",
    "    # Keep track of valid indices for timestamps\n",
    "    if missing_vals.sum() > 0 or inf_mask.sum() > 0:\n",
    "        if missing_vals.sum() > 0:\n",
    "            valid_mask = ~(df.drop(columns=[c for c in DROP_COLS if c in df.columns] + [TARGET_COL], errors=\"ignore\").isnull().any(axis=1) | df[TARGET_COL].isnull())\n",
    "        else:\n",
    "            valid_mask = slice(None)\n",
    "        \n",
    "        if inf_mask.sum() > 0:\n",
    "            if isinstance(valid_mask, slice):\n",
    "                valid_mask = ~inf_mask\n",
    "            else:\n",
    "                valid_mask = valid_mask & ~inf_mask\n",
    "        \n",
    "        timestamps = df.index[valid_mask]\n",
    "    else:\n",
    "        timestamps = df.index\n",
    "    \n",
    "    removed_samples = original_size - len(X)\n",
    "    if removed_samples > 0:\n",
    "        print(f\"üìâ Removed {removed_samples} samples due to missing/infinite values\")\n",
    "    \n",
    "    return X, y, timestamps\n",
    "\n",
    "def preprocess_features(X_train, X_test):\n",
    "    \"\"\"Preprocess features with variance filtering.\"\"\"\n",
    "    print(\"\\nüîß Preprocessing features...\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.std() == 0]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"‚ö†Ô∏è Removing {len(constant_cols)} constant features\")\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    # Remove low-variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    n_features_before = X_train.shape[1]\n",
    "    \n",
    "    X_train_selected = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    X_test_selected = pd.DataFrame(\n",
    "        variance_selector.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_train.columns[variance_selector.get_support()]\n",
    "    )\n",
    "    \n",
    "    n_features_after = X_train_selected.shape[1]\n",
    "    if n_features_before != n_features_after:\n",
    "        print(f\"‚ö†Ô∏è Removed {n_features_before - n_features_after} low-variance features\")\n",
    "    \n",
    "    print(f\"‚úÖ Final feature count: {n_features_after}\")\n",
    "    return X_train_selected, X_test_selected, variance_selector\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) MODEL TRAINING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def create_best_precision_pipeline():\n",
    "    \"\"\"Create pipeline with best precision parameters.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(**BEST_PRECISION_PARAMS))\n",
    "    ])\n",
    "\n",
    "def train_best_precision_model(X_train, y_train):\n",
    "    \"\"\"Train the best precision model.\"\"\"\n",
    "    print(\"\\nüéØ TRAINING BEST PRECISION MODEL\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    pipeline = create_best_precision_pipeline()\n",
    "    \n",
    "    print(\"üèÜ Using best precision parameters:\")\n",
    "    for param, value in BEST_PRECISION_PARAMS.items():\n",
    "        print(f\"   {param:<15}: {value}\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è Training model...\")\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check convergence\n",
    "    logreg = pipeline.named_steps['logreg']\n",
    "    if hasattr(logreg, 'n_iter_'):\n",
    "        n_iter = logreg.n_iter_[0] if isinstance(logreg.n_iter_, np.ndarray) else logreg.n_iter_\n",
    "        if n_iter >= logreg.max_iter:\n",
    "            print(f\"‚ö†Ô∏è Model may not have converged (used {n_iter}/{logreg.max_iter} iterations)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Model converged in {n_iter} iterations\")\n",
    "    \n",
    "    return pipeline, training_time\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) MODEL EVALUATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def f_beta_half(y_true, y_pred):\n",
    "    \"\"\"Custom F-beta score with beta=0.5 (precision-weighted).\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    beta = 0.5\n",
    "    if (p + r) == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "def evaluate_best_precision_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate the best precision model.\"\"\"\n",
    "    print(\"\\nüìä MODEL EVALUATION\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics for test set\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f_beta_0.5': f_beta_half(y_test, y_test_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_test_prob[:, 1]) if len(np.unique(y_test)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    print(\"üìà TEST SET METRICS:\")\n",
    "    print(\"-\" * 25)\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"   {metric_name:<12}: {value:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"\\nüéØ CONFUSION MATRIX:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    return {\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': y_train_pred,\n",
    "        'train_probabilities': y_train_prob,\n",
    "        'test_predictions': y_test_pred,\n",
    "        'test_probabilities': y_test_prob\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5) PREDICTION CSV GENERATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_predictions_csv(X_train, X_test, y_train, y_test, results, train_timestamps, test_timestamps):\n",
    "    \"\"\"Generate CSV with predictions as requested.\"\"\"\n",
    "    print(\"\\nüíæ GENERATING PREDICTIONS CSV\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Combine train and test data\n",
    "    all_timestamps = list(train_timestamps) + list(test_timestamps)\n",
    "    all_actual = list(y_train.values) + list(y_test.values)\n",
    "    \n",
    "    # Combine predictions and probabilities\n",
    "    all_predictions = list(results['train_predictions']) + list(results['test_predictions'])\n",
    "    all_probabilities = np.vstack([results['train_probabilities'], results['test_probabilities']])\n",
    "    \n",
    "    # Create the requested DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'timestamp': all_timestamps,\n",
    "        'prediction_of_up': all_probabilities[:, 1],      # Probability of class 1 (up)\n",
    "        'prediction_of_down': all_probabilities[:, 0],    # Probability of class 0 (down)\n",
    "        'final_prediction': all_predictions,              # Final prediction (0 or 1)\n",
    "        'actual_value': all_actual                        # Actual target value (0 or 1)\n",
    "    })\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    predictions_df = predictions_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Add additional useful columns\n",
    "    predictions_df['prediction_confidence'] = np.maximum(\n",
    "        predictions_df['prediction_of_up'], \n",
    "        predictions_df['prediction_of_down']\n",
    "    )\n",
    "    \n",
    "    predictions_df['correct_prediction'] = (\n",
    "        predictions_df['final_prediction'] == predictions_df['actual_value']\n",
    "    ).astype(int)\n",
    "    \n",
    "    predictions_df['prediction_type'] = predictions_df.apply(lambda row: \n",
    "        'True Positive' if row['actual_value'] == 1 and row['final_prediction'] == 1\n",
    "        else 'True Negative' if row['actual_value'] == 0 and row['final_prediction'] == 0\n",
    "        else 'False Positive' if row['actual_value'] == 0 and row['final_prediction'] == 1\n",
    "        else 'False Negative', axis=1)\n",
    "    \n",
    "    # Add set type (train/test)\n",
    "    train_size = len(train_timestamps)\n",
    "    predictions_df['set_type'] = ['train'] * train_size + ['test'] * len(test_timestamps)\n",
    "    \n",
    "    # Save to CSV\n",
    "    predictions_df.to_csv(PREDICTIONS_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Predictions saved to: {PREDICTIONS_CSV}\")\n",
    "    print(f\"üìä Total predictions: {len(predictions_df):,}\")\n",
    "    print(f\"   Train predictions: {train_size:,}\")\n",
    "    print(f\"   Test predictions:  {len(test_timestamps):,}\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(f\"\\nüìã SAMPLE PREDICTIONS (First 10 rows):\")\n",
    "    print(\"-\" * 60)\n",
    "    sample_cols = ['timestamp', 'prediction_of_up', 'prediction_of_down', 'final_prediction', 'actual_value']\n",
    "    print(predictions_df[sample_cols].head(10).to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà PREDICTION SUMMARY:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Overall accuracy: {predictions_df['correct_prediction'].mean():.4f}\")\n",
    "    print(f\"Test accuracy:    {predictions_df[predictions_df['set_type']=='test']['correct_prediction'].mean():.4f}\")\n",
    "    print(f\"Average confidence: {predictions_df['prediction_confidence'].mean():.4f}\")\n",
    "    print(f\"High confidence predictions (>0.7): {(predictions_df['prediction_confidence'] > 0.7).sum():,}\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6) MODEL SAVING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def save_best_precision_model(model, variance_selector, feature_names, results, training_time):\n",
    "    \"\"\"Save the complete best precision model.\"\"\"\n",
    "    print(f\"\\nüíæ SAVING BEST PRECISION MODEL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'variance_selector': variance_selector,\n",
    "        'feature_names': feature_names,\n",
    "        'parameters': BEST_PRECISION_PARAMS,\n",
    "        'performance_metrics': results['test_metrics'],\n",
    "        'training_info': {\n",
    "            'training_date': pd.Timestamp.now(),\n",
    "            'training_time_seconds': training_time,\n",
    "            'algorithm': 'LogisticRegression ElasticNet (Best Precision)',\n",
    "            'total_features': len(feature_names),\n",
    "            'preprocessing_steps': ['StandardScaler', 'VarianceThreshold'],\n",
    "            'model_description': 'Highest precision model from parameter comparison'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_package, MODEL_PATH)\n",
    "    print(f\"‚úÖ Model saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 7) MAIN EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X, y, timestamps = load_and_prepare_data()\n",
    "    \n",
    "    # Chronological split\n",
    "    split_idx = int(len(X) * (1 - TEST_FRAC))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    train_timestamps = timestamps[:split_idx]\n",
    "    test_timestamps = timestamps[split_idx:]\n",
    "    \n",
    "    print(f\"\\nüìä TRAIN/TEST SPLIT:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Test:  {X_test.shape[0]:,} samples\")\n",
    "    print(f\"   Train period: {train_timestamps.min()} to {train_timestamps.max()}\")\n",
    "    print(f\"   Test period:  {test_timestamps.min()} to {test_timestamps.max()}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed, X_test_processed, variance_selector = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    # Train best precision model\n",
    "    best_model, training_time = train_best_precision_model(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_best_precision_model(best_model, X_train_processed, y_train, X_test_processed, y_test)\n",
    "    \n",
    "    # Generate predictions CSV\n",
    "    predictions_df = generate_predictions_csv(\n",
    "        X_train_processed, X_test_processed, y_train, y_test, \n",
    "        results, train_timestamps, test_timestamps\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model_package = save_best_precision_model(\n",
    "        best_model, variance_selector, list(X_train_processed.columns), \n",
    "        results, training_time\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ BEST PRECISION MODEL TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìÅ Files Generated:\")\n",
    "    print(f\"   üî∏ Predictions: {PREDICTIONS_CSV.name}\")\n",
    "    print(f\"   üî∏ Model:       {MODEL_PATH.name}\")\n",
    "    print(f\"\\nüèÜ Final Test Precision: {results['test_metrics']['precision']:.4f}\")\n",
    "    print(f\"üéØ Final Test F1-Score:  {results['test_metrics']['f1']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'predictions_df': predictions_df,\n",
    "        'results': results,\n",
    "        'model_package': model_package\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 8) SCRIPT EXECUTION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19622fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation at threshold 0.5:\n",
      "Precision: 0.551\n",
      "Recall   : 0.595\n",
      "F1 Score : 0.572\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the predictions CSV\n",
    "csv_path = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Predictions_folder\\logisticreg_validation_predictions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names are correct and lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Extract actual and predicted values\n",
    "y_true = df['actual']\n",
    "y_pred = df['prediction']  # prediction at threshold 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# Print results\n",
    "print(\"üìä Evaluation at threshold 0.5:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1 Score : {f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
