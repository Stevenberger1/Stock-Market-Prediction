{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7bb9db",
   "metadata": {},
   "source": [
    "# In this notebook we would create the LIGHTGBM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = [ \n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'rsi_oversold', 'rsi_overbought', 'stoch_overbought',\n",
    "    'stoch_oversold', 'cci_overbought', 'cci_oversold', 'near_upper_band',\n",
    "    'near_lower_band', 'overbought_reversal', 'oversold_reversal',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'trending_market', 'trend_alignment', 'ema7_above_ema21', 'macd_rising',\n",
    "    'bollinger_upper', 'bollinger_lower', 'bullish_scenario_1',\n",
    "    'bullish_scenario_5', 'bearish_scenario_1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d167382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.69157\n",
      "Trial 01 â†’ weighted-F1 = 0.4786  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.692375\n",
      "Trial 02 â†’ weighted-F1 = 0.5135  |  params: {'learning_rate': 0.05, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 03 â†’ weighted-F1 = 0.4799  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 04 â†’ weighted-F1 = 0.5000  |  params: {'learning_rate': 0.02, 'num_leaves': 31, 'feature_fraction': 0.9, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.691752\n",
      "Trial 05 â†’ weighted-F1 = 0.4959  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's binary_logloss: 0.691597\n",
      "Trial 06 â†’ weighted-F1 = 0.4797  |  params: {'learning_rate': 0.01, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.691621\n",
      "Trial 07 â†’ weighted-F1 = 0.5030  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.691911\n",
      "Trial 08 â†’ weighted-F1 = 0.4799  |  params: {'learning_rate': 0.05, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.691611\n",
      "Trial 09 â†’ weighted-F1 = 0.4765  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.69147\n",
      "Trial 10 â†’ weighted-F1 = 0.5270  |  params: {'learning_rate': 0.05, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's binary_logloss: 0.691737\n",
      "Trial 11 â†’ weighted-F1 = 0.4908  |  params: {'learning_rate': 0.01, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.691776\n",
      "Trial 12 â†’ weighted-F1 = 0.4909  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.691455\n",
      "Trial 13 â†’ weighted-F1 = 0.4774  |  params: {'learning_rate': 0.01, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.69149\n",
      "Trial 14 â†’ weighted-F1 = 0.5002  |  params: {'learning_rate': 0.02, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 15 â†’ weighted-F1 = 0.4860  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "\n",
      "Search finished in 6.6 s\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€ Best parameters â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "learning_rate    : 0.05\n",
      "num_leaves       : 63\n",
      "feature_fraction : 0.9\n",
      "bagging_fraction : 0.8\n",
      "bagging_freq     : 1\n",
      "min_child_samples: 20\n",
      "Best weighted-F1 : 0.5270\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved LightGBM hyperparameter optimization for BTC direction prediction\n",
    "--------------------------------------------------------------------------\n",
    "* Maintains chronological order with proper validation\n",
    "* Optimizes weighted-F1 with threshold tuning\n",
    "* Comprehensive parameter search space\n",
    "* Robust error handling and cross-validation\n",
    "* Enhanced evaluation metrics\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import (precision_recall_fscore_support, roc_auc_score, \n",
    "                           precision_recall_curve, classification_report)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LightGBMOptimizer:\n",
    "    def __init__(self, csv_path, drop_cols=None, val_frac=0.20, w_precision=2.0, \n",
    "                 random_state=42, use_optuna=True):\n",
    "        \"\"\"\n",
    "        Initialize LightGBM Optimizer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        csv_path : str\n",
    "            Path to the CSV dataset\n",
    "        drop_cols : list\n",
    "            Columns to drop from dataset\n",
    "        val_frac : float\n",
    "            Fraction of data for validation (chronological split)\n",
    "        w_precision : float\n",
    "            Weight for precision in weighted F1 calculation\n",
    "        random_state : int\n",
    "            Random state for reproducibility\n",
    "        use_optuna : bool\n",
    "            Whether to use Optuna for optimization (vs random search)\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.drop_cols = drop_cols or []\n",
    "        self.val_frac = val_frac\n",
    "        self.w_precision = w_precision\n",
    "        self.random_state = random_state\n",
    "        self.use_optuna = use_optuna\n",
    "        self.best_params = None\n",
    "        self.best_score = -1\n",
    "        self.best_threshold = 0.5\n",
    "        \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess the dataset\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(self.csv_path, index_col=0, parse_dates=True)\n",
    "        print(f\"Original dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Drop specified columns\n",
    "        existing_drop_cols = [c for c in self.drop_cols if c in df.columns]\n",
    "        if existing_drop_cols:\n",
    "            df.drop(columns=existing_drop_cols, inplace=True)\n",
    "            print(f\"Dropped columns: {existing_drop_cols}\")\n",
    "        \n",
    "        # Log transform volume (if exists)\n",
    "        if \"Volume BTC\" in df.columns:\n",
    "            df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "            print(\"Applied log1p transformation to Volume BTC\")\n",
    "        \n",
    "        # Create target (next period direction)\n",
    "        if \"target\" not in df.columns:\n",
    "            df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "            print(\"Created target column (next period direction)\")\n",
    "        \n",
    "        # Clean data\n",
    "        df = df.dropna().select_dtypes(include=[np.number])\n",
    "        print(f\"Final dataset shape after cleaning: {df.shape}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop(columns=[\"target\"])\n",
    "        y = df[\"target\"].astype(int)\n",
    "        \n",
    "        # Chronological split\n",
    "        split_idx = int(len(df) * (1 - self.val_frac))\n",
    "        self.X_train = X.iloc[:split_idx]\n",
    "        self.X_val = X.iloc[split_idx:]\n",
    "        self.y_train = y.iloc[:split_idx]\n",
    "        self.y_val = y.iloc[split_idx:]\n",
    "        \n",
    "        print(f\"Training set: {self.X_train.shape[0]} samples\")\n",
    "        print(f\"Validation set: {self.X_val.shape[0]} samples\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        train_dist = self.y_train.value_counts().sort_index()\n",
    "        val_dist = self.y_val.value_counts().sort_index()\n",
    "        print(f\"Training class distribution: {dict(train_dist)}\")\n",
    "        print(f\"Validation class distribution: {dict(val_dist)}\")\n",
    "        \n",
    "        return self.X_train, self.X_val, self.y_train, self.y_val\n",
    "    \n",
    "    def weighted_f1_with_threshold_tuning(self, y_true, y_pred_prob, w=2.0):\n",
    "        \"\"\"\n",
    "        Calculate weighted F1 with optimal threshold tuning\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array\n",
    "            True binary labels\n",
    "        y_pred_prob : array\n",
    "            Predicted probabilities\n",
    "        w : float\n",
    "            Weight for precision in weighted F1\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        best_score : float\n",
    "            Best weighted F1 score\n",
    "        best_threshold : float\n",
    "            Optimal threshold\n",
    "        \"\"\"\n",
    "        # Try different thresholds\n",
    "        thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "        best_score = -1\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for thr in thresholds:\n",
    "            y_pred = (y_pred_prob >= thr).astype(int)\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if len(np.unique(y_pred)) < 2:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "                    y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0\n",
    "                )\n",
    "                \n",
    "                # Weighted F1: emphasizes precision more\n",
    "                if prec + rec > 0:\n",
    "                    weighted_f1 = (1 + w) * prec * rec / (w * prec + rec)\n",
    "                    if weighted_f1 > best_score:\n",
    "                        best_score = weighted_f1\n",
    "                        best_threshold = thr\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return best_score, best_threshold\n",
    "    \n",
    "    def evaluate_model(self, model, X_val, y_val):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Get weighted F1 with optimal threshold\n",
    "        weighted_f1_score, optimal_threshold = self.weighted_f1_with_threshold_tuning(\n",
    "            y_val, y_pred_prob, self.w_precision\n",
    "        )\n",
    "        \n",
    "        # Calculate other metrics with optimal threshold\n",
    "        y_pred = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "        \n",
    "        try:\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_val, y_pred, average=\"binary\", pos_label=1, zero_division=0\n",
    "            )\n",
    "            roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "        except:\n",
    "            prec = rec = f1 = roc_auc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'weighted_f1': weighted_f1_score,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "    \n",
    "    def objective_optuna(self, trial):\n",
    "        \"\"\"Objective function for Optuna optimization\"\"\"\n",
    "        try:\n",
    "            # Suggest parameters with expanded search space\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'subsample_for_bin': trial.suggest_int('subsample_for_bin', 50000, 300000),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 15.0),\n",
    "                'verbose': -1,\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            \n",
    "            # Create and train model\n",
    "            model = LGBMClassifier(\n",
    "                n_estimators=2000,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                self.X_train, self.y_train,\n",
    "                eval_set=[(self.X_val, self.y_val)],\n",
    "                eval_metric=\"binary_logloss\",\n",
    "                callbacks=[\n",
    "                    early_stopping(stopping_rounds=100),\n",
    "                    log_evaluation(0)  # silent\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = self.evaluate_model(model, self.X_val, self.y_val)\n",
    "            return metrics['weighted_f1']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def random_search(self, n_trials=50):\n",
    "        \"\"\"Random search optimization\"\"\"\n",
    "        print(f\"Starting random search with {n_trials} trials...\")\n",
    "        \n",
    "        # Expanded search space\n",
    "        space = {\n",
    "            \"learning_rate\": [0.01, 0.02, 0.05, 0.1, 0.15, 0.2],\n",
    "            \"num_leaves\": [15, 31, 63, 127, 255],\n",
    "            \"max_depth\": [3, 5, 7, 10, 12, 15],\n",
    "            \"feature_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            \"bagging_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            \"bagging_freq\": [1, 3, 5],\n",
    "            \"min_child_samples\": [5, 10, 20, 40, 80],\n",
    "            \"reg_alpha\": [0.0, 0.1, 0.5, 1.0, 2.0],\n",
    "            \"reg_lambda\": [0.0, 0.1, 0.5, 1.0, 2.0]\n",
    "        }\n",
    "        \n",
    "        def random_param():\n",
    "            return {k: random.choice(v) for k, v in space.items()}\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for t in range(1, n_trials + 1):\n",
    "            try:\n",
    "                params = random_param()\n",
    "                params.update({\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'binary_logloss',\n",
    "                    'verbose': -1,\n",
    "                    'random_state': self.random_state + t\n",
    "                })\n",
    "                \n",
    "                model = LGBMClassifier(\n",
    "                    n_estimators=2000,\n",
    "                    **params\n",
    "                )\n",
    "                \n",
    "                model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    eval_set=[(self.X_val, self.y_val)],\n",
    "                    eval_metric=\"binary_logloss\",\n",
    "                    callbacks=[\n",
    "                        early_stopping(stopping_rounds=100),\n",
    "                        log_evaluation(0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                metrics = self.evaluate_model(model, self.X_val, self.y_val)\n",
    "                score = metrics['weighted_f1']\n",
    "                \n",
    "                print(f\"Trial {t:02d} â†’ weighted-F1 = {score:.4f}, threshold = {metrics['optimal_threshold']:.3f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "                    best_threshold = metrics['optimal_threshold']\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Trial {t:02d} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_score = best_score\n",
    "        self.best_params = best_params\n",
    "        self.best_threshold = best_threshold\n",
    "        \n",
    "        return best_params, best_score, best_threshold\n",
    "    \n",
    "    def optuna_search(self, n_trials=100, timeout=3600):\n",
    "        \"\"\"Optuna-based optimization\"\"\"\n",
    "        print(f\"Starting Optuna optimization with {n_trials} trials...\")\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=self.random_state)\n",
    "        )\n",
    "        \n",
    "        # Optimize\n",
    "        study.optimize(self.objective_optuna, n_trials=n_trials, timeout=timeout)\n",
    "        \n",
    "        # Get best parameters\n",
    "        self.best_params = study.best_params\n",
    "        self.best_params.update({\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1,\n",
    "            'random_state': self.random_state\n",
    "        })\n",
    "        \n",
    "        # Train best model to get threshold\n",
    "        best_model = LGBMClassifier(n_estimators=2000, **self.best_params)\n",
    "        best_model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=[(self.X_val, self.y_val)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        metrics = self.evaluate_model(best_model, self.X_val, self.y_val)\n",
    "        self.best_score = metrics['weighted_f1']\n",
    "        self.best_threshold = metrics['optimal_threshold']\n",
    "        \n",
    "        return self.best_params, self.best_score, self.best_threshold\n",
    "    \n",
    "    def cross_validate(self, params, n_splits=5):\n",
    "        \"\"\"Time series cross-validation\"\"\"\n",
    "        print(f\"Performing time series cross-validation with {n_splits} splits...\")\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        scores = []\n",
    "        \n",
    "        # Combine train and val for CV\n",
    "        X_full = pd.concat([self.X_train, self.X_val])\n",
    "        y_full = pd.concat([self.y_train, self.y_val])\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full)):\n",
    "            try:\n",
    "                X_train_fold = X_full.iloc[train_idx]\n",
    "                X_val_fold = X_full.iloc[val_idx]\n",
    "                y_train_fold = y_full.iloc[train_idx]\n",
    "                y_val_fold = y_full.iloc[val_idx]\n",
    "                \n",
    "                model = LGBMClassifier(n_estimators=1000, **params)\n",
    "                model.fit(\n",
    "                    X_train_fold, y_train_fold,\n",
    "                    eval_set=[(X_val_fold, y_val_fold)],\n",
    "                    eval_metric=\"binary_logloss\",\n",
    "                    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(0)]\n",
    "                )\n",
    "                \n",
    "                metrics = self.evaluate_model(model, X_val_fold, y_val_fold)\n",
    "                scores.append(metrics['weighted_f1'])\n",
    "                print(f\"  Fold {fold + 1}: {metrics['weighted_f1']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Fold {fold + 1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        cv_mean = np.mean(scores) if scores else 0.0\n",
    "        cv_std = np.std(scores) if scores else 0.0\n",
    "        \n",
    "        print(f\"CV weighted-F1: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "        return cv_mean, cv_std\n",
    "    \n",
    "    def optimize(self, method='optuna', n_trials=100, timeout=3600):\n",
    "        \"\"\"\n",
    "        Run hyperparameter optimization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            'optuna' or 'random'\n",
    "        n_trials : int\n",
    "            Number of trials\n",
    "        timeout : int\n",
    "            Timeout in seconds (for Optuna)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if method == 'optuna':\n",
    "            best_params, best_score, best_threshold = self.optuna_search(n_trials, timeout)\n",
    "        else:\n",
    "            best_params, best_score, best_threshold = self.random_search(n_trials)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\\\nOptimization completed in {end_time - start_time:.1f} seconds\")\n",
    "        print(\"\\\\n\" + \"=\"*50)\n",
    "        print(\"BEST PARAMETERS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for k, v in best_params.items():\n",
    "            if k not in ['objective', 'metric', 'verbose', 'random_state']:\n",
    "                print(f\"{k:<20}: {v}\")\n",
    "        \n",
    "        print(f\"\\\\nBest weighted-F1   : {best_score:.4f}\")\n",
    "        print(f\"Optimal threshold   : {best_threshold:.3f}\")\n",
    "        \n",
    "        # Cross-validation with best parameters\n",
    "        cv_mean, cv_std = self.cross_validate(best_params)\n",
    "        \n",
    "        return {\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'best_threshold': best_threshold,\n",
    "            'cv_mean': cv_mean,\n",
    "            'cv_std': cv_std\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    CSV_PATH = r\"C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Coding_projects\\\\stock_market_prediction\\\\Stock-Market-Prediction\\\\data\\\\processed\\\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "    DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\", \"upper_shadow\", \"lower_shadow\"]\n",
    "    VAL_FRAC = 0.20\n",
    "    W_PRECISION = 2.0\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = LightGBMOptimizer(\n",
    "        csv_path=CSV_PATH,\n",
    "        drop_cols=DROP_COLS,\n",
    "        val_frac=VAL_FRAC,\n",
    "        w_precision=W_PRECISION,\n",
    "        random_state=42,\n",
    "        use_optuna=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        X_train, X_val, y_train, y_val = optimizer.load_and_preprocess_data()\n",
    "        \n",
    "        # Run optimization\n",
    "        results = optimizer.optimize(\n",
    "            method='optuna',  # or 'random'\n",
    "            n_trials=100,\n",
    "            timeout=3600  # 1 hour\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nðŸŽ‰ Optimization completed successfully!\")\n",
    "        return optimizer, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer, results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ce041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.701439\n",
      "[200]\tvalid_0's binary_logloss: 0.705638\n",
      "[300]\tvalid_0's binary_logloss: 0.709782\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.691879\n",
      "Evaluated only: binary_logloss\n",
      "\n",
      "â”€â”€â”€â”€ Validation metrics (thr = 0.50) â”€â”€â”€â”€\n",
      "Accuracy          :  0.529\n",
      "Class 0 (Down) â†’  Precision:  0.517  Recall:  0.580  F1:  0.547\n",
      "Class 1 (Up  ) â†’  Precision:  0.544  Recall:  0.481  F1:  0.511\n",
      "Macro-F1          :  0.529\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final LightGBM model\n",
    "--------------------\n",
    "Uses the best mini-search parameters:\n",
    "\n",
    "    learning_rate     = 0.05\n",
    "    num_leaves        = 63\n",
    "    feature_fraction  = 0.9\n",
    "    bagging_fraction  = 0.8\n",
    "    bagging_freq      = 1\n",
    "    min_child_samples = 20\n",
    "\n",
    "Prints accuracy, precision, recall and F1 for each class.\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ file path & columns to drop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\",\n",
    "             \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "VAL_FRAC = 0.20         # 80 % train Â· 20 % validation\n",
    "PREC_W   = 2.0          # precision weight for weighted-F1\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ data preparation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)   # 1 = Up\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"].astype(int)\n",
    "\n",
    "split_idx = int(len(df) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ best-parameter LightGBM model â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_params = dict(\n",
    "    objective         = \"binary\",\n",
    "    learning_rate     = 0.05,\n",
    "    num_leaves        = 63,\n",
    "    feature_fraction  = 0.9,\n",
    "    bagging_fraction  = 0.8,\n",
    "    bagging_freq      = 1,\n",
    "    min_child_samples = 20,\n",
    "    n_estimators      = 4000,   # large upper bound â€“ early stop will trim\n",
    "    verbose           = -1,\n",
    "    random_state      = 42\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(**best_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"binary_logloss\",\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=300, first_metric_only=True),\n",
    "        log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "y_prob = model.predict_proba(X_val)[:, 1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nâ”€â”€â”€â”€ Validation metrics (thr = 0.50) â”€â”€â”€â”€\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) â†’  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) â†’  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
