{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62dcffa1",
   "metadata": {},
   "source": [
    "# In this notebook we would build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep \n",
    "atr_14  , obv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becba6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep \n",
    "atr_14  , obv\n",
    "\n",
    "drop\n",
    "\n",
    "ema_cross_up, macd_cross_up, oversold_reversal, overbought_reversal, trending_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1bf238",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cnn = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_7', 'EMA_21', 'SMA_20', \n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower', \n",
    "    'resistance_level', 'support_level', 'high_low', 'high_close', 'low_close',\n",
    "    'true_range', 'volume_mean_20', 'MACD_line', 'MACD_signal',\n",
    "    'bollinger_width', 'volatility_regime',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'break_upper_band', 'break_lower_band', 'rsi_oversold', 'rsi_overbought',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21', 'macd_positive',\n",
    "    'volume_breakout', 'volume_breakdown', 'stoch_overbought', 'stoch_oversold',\n",
    "    'cci_overbought', 'cci_oversold',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6''ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal', 'trending_market'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10b65b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-12 18:17:51,210] A new study created in memory with name: no-name-2c311586-1232-4232-a74f-0a47b3d54715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Scaler fitted on 12,684 rows | features = 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.520  R=0.774  Fβ=0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:   3%|▎         | 1/30 [00:17<08:26, 17.45s/it, 17.45/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.505  R=0.474  Fβ=0.497\n",
      "✅ Trial 0 done  |  P=0.512  R=0.624  Fβ=0.531\n",
      "[I 2025-06-12 18:18:08,665] Trial 0 finished with value: 0.4694371059124449 and parameters: {'window': 24, 'batch': 32, 'lr': 0.0010401663679887319, 'epochs': 20, 'conv_blocks': 1, 'filters': 32, 'kernel': 4, 'act': 'elu', 'dropout': 0.2924774630404986, 'dense': 128, 'l2': 4.335281794951567e-06, 'pool': 'gmp', 'extra_dense': False}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.519  R=0.639  Fβ=0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:   7%|▋         | 2/30 [00:32<07:26, 15.93s/it, 32.32/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.495  R=0.467  Fβ=0.488\n",
      "✅ Trial 1 done  |  P=0.507  R=0.553  Fβ=0.516\n",
      "[I 2025-06-12 18:18:23,527] Trial 1 finished with value: 0.4843039870653453 and parameters: {'window': 30, 'batch': 64, 'lr': 0.00017258215396625024, 'epochs': 20, 'conv_blocks': 2, 'filters': 64, 'kernel': 4, 'act': 'selu', 'dropout': 0.061612603179999434, 'dense': 128, 'l2': 3.247673570627449e-06, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.522  R=0.424  Fβ=0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  10%|█         | 3/30 [00:43<06:16, 13.93s/it, 43.87/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.496  R=0.593  Fβ=0.516\n",
      "✅ Trial 2 done  |  P=0.509  R=0.509  Fβ=0.506\n",
      "[I 2025-06-12 18:18:35,086] Trial 2 finished with value: 0.4944724315762894 and parameters: {'window': 24, 'batch': 64, 'lr': 0.0005595074635794797, 'epochs': 20, 'conv_blocks': 2, 'filters': 32, 'kernel': 4, 'act': 'elu', 'dropout': 0.1800170052944527, 'dense': 128, 'l2': 3.5856126103453987e-06, 'pool': 'gap', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.530  R=0.432  Fβ=0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  13%|█▎        | 4/30 [01:04<07:08, 16.47s/it, 64.23/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.491  R=0.963  Fβ=0.557\n",
      "✅ Trial 3 done  |  P=0.511  R=0.698  Fβ=0.530\n",
      "[I 2025-06-12 18:18:55,446] Trial 3 finished with value: 0.4701032713054939 and parameters: {'window': 36, 'batch': 32, 'lr': 0.000215262809722153, 'epochs': 20, 'conv_blocks': 1, 'filters': 64, 'kernel': 2, 'act': 'relu', 'dropout': 0.18567402078956213, 'dense': 64, 'l2': 0.0002550298070162893, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.526  R=0.299  Fβ=0.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  17%|█▋        | 5/30 [01:12<05:40, 13.63s/it, 72.82/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.504  R=0.564  Fβ=0.517\n",
      "✅ Trial 4 done  |  P=0.515  R=0.431  Fβ=0.481\n",
      "[I 2025-06-12 18:19:04,028] Trial 4 finished with value: 0.5186461617577373 and parameters: {'window': 12, 'batch': 32, 'lr': 0.001732053535845956, 'epochs': 60, 'conv_blocks': 1, 'filters': 64, 'kernel': 2, 'act': 'relu', 'dropout': 0.06588958757150591, 'dense': 64, 'l2': 9.4525713910723e-06, 'pool': 'gap', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.528  R=0.271  Fβ=0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  20%|██        | 6/30 [01:20<04:42, 11.76s/it, 80.95/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.494  R=0.338  Fβ=0.445\n",
      "✅ Trial 5 done  |  P=0.511  R=0.304  Fβ=0.438\n",
      "[I 2025-06-12 18:19:12,161] Trial 5 finished with value: 0.5620542374852173 and parameters: {'window': 12, 'batch': 64, 'lr': 0.0008986552644007198, 'epochs': 60, 'conv_blocks': 2, 'filters': 64, 'kernel': 3, 'act': 'elu', 'dropout': 0.20910260281594512, 'dense': 64, 'l2': 3.35515102272148e-05, 'pool': 'gap', 'extra_dense': False}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  23%|██▎       | 7/30 [01:29<04:04, 10.61s/it, 89.20/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.519  R=0.611  Fβ=0.539\n",
      "[I 2025-06-12 18:19:20,411] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  27%|██▋       | 8/30 [01:34<03:16,  8.94s/it, 94.57/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.507  R=0.605  Fβ=0.528\n",
      "[I 2025-06-12 18:19:25,778] Trial 7 pruned. \n",
      "  ↪ Fold 0:  P=0.516  R=0.318  Fβ=0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  30%|███       | 9/30 [01:57<04:41, 13.43s/it, 117.86/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.496  R=0.682  Fβ=0.531\n",
      "✅ Trial 8 done  |  P=0.506  R=0.500  Fβ=0.490\n",
      "[I 2025-06-12 18:19:49,070] Trial 8 finished with value: 0.5100477189088047 and parameters: {'window': 18, 'batch': 64, 'lr': 0.0007145565133513971, 'epochs': 20, 'conv_blocks': 1, 'filters': 96, 'kernel': 2, 'act': 'selu', 'dropout': 0.1105138178778751, 'dense': 128, 'l2': 0.00019268985325226193, 'pool': 'gmp', 'extra_dense': False}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  33%|███▎      | 10/30 [02:03<03:43, 11.17s/it, 123.99/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.516  R=0.644  Fβ=0.542\n",
      "[I 2025-06-12 18:19:55,197] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  37%|███▋      | 11/30 [03:14<09:19, 29.44s/it, 194.84/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.526  R=0.640  Fβ=0.549\n",
      "[I 2025-06-12 18:21:06,049] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  40%|████      | 12/30 [03:21<06:42, 22.37s/it, 201.05/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.522  R=0.454  Fβ=0.504\n",
      "[I 2025-06-12 18:21:12,258] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  43%|████▎     | 13/30 [03:39<05:58, 21.08s/it, 219.16/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.515  R=0.566  Fβ=0.526\n",
      "[I 2025-06-12 18:21:30,367] Trial 12 pruned. \n",
      "  ↪ Fold 0:  P=0.515  R=0.378  Fβ=0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  47%|████▋     | 14/30 [04:20<07:13, 27.12s/it, 260.24/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.504  R=0.449  Fβ=0.490\n",
      "✅ Trial 13 done  |  P=0.510  R=0.414  Fβ=0.482\n",
      "[I 2025-06-12 18:22:11,448] Trial 13 finished with value: 0.5179879192930853 and parameters: {'window': 30, 'batch': 32, 'lr': 0.00011523031395040909, 'epochs': 20, 'conv_blocks': 2, 'filters': 32, 'kernel': 3, 'act': 'elu', 'dropout': 0.13307936030440012, 'dense': 64, 'l2': 5.4746619586844735e-05, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n",
      "  ↪ Fold 0:  P=0.520  R=0.324  Fβ=0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  50%|█████     | 15/30 [04:36<05:59, 24.00s/it, 276.99/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.489  R=0.875  Fβ=0.546\n",
      "✅ Trial 14 done  |  P=0.505  R=0.600  Fβ=0.500\n",
      "[I 2025-06-12 18:22:28,204] Trial 14 finished with value: 0.4995559216121319 and parameters: {'window': 24, 'batch': 32, 'lr': 0.001450364906694658, 'epochs': 40, 'conv_blocks': 1, 'filters': 64, 'kernel': 3, 'act': 'relu', 'dropout': 0.2554516838857716, 'dense': 128, 'l2': 1.2761974314258607e-05, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  53%|█████▎    | 16/30 [04:57<05:21, 22.94s/it, 297.47/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.528  R=0.589  Fβ=0.541\n",
      "[I 2025-06-12 18:22:48,677] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  57%|█████▋    | 17/30 [05:01<03:42, 17.13s/it, 301.10/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.512  R=0.547  Fβ=0.520\n",
      "[I 2025-06-12 18:22:52,314] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  60%|██████    | 18/30 [05:11<03:02, 15.21s/it, 311.84/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.512  R=0.648  Fβ=0.539\n",
      "[I 2025-06-12 18:23:03,054] Trial 17 pruned. \n",
      "  ↪ Fold 0:  P=0.506  R=0.355  Fβ=0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  63%|██████▎   | 19/30 [06:31<06:21, 34.65s/it, 391.79/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.456  R=0.166  Fβ=0.322\n",
      "✅ Trial 18 done  |  P=0.481  R=0.260  Fβ=0.390\n",
      "[I 2025-06-12 18:24:23,000] Trial 18 finished with value: 0.609619436419857 and parameters: {'window': 48, 'batch': 32, 'lr': 0.0004450521490456785, 'epochs': 20, 'conv_blocks': 3, 'filters': 32, 'kernel': 2, 'act': 'relu', 'dropout': 0.15364460080793052, 'dense': 64, 'l2': 0.0004364907895183181, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  67%|██████▋   | 20/30 [06:36<04:17, 25.76s/it, 396.83/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.528  R=0.557  Fβ=0.534\n",
      "[I 2025-06-12 18:24:28,037] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  70%|███████   | 21/30 [06:43<02:59, 19.93s/it, 403.17/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.524  R=0.483  Fβ=0.514\n",
      "[I 2025-06-12 18:24:34,377] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  73%|███████▎  | 22/30 [06:48<02:04, 15.51s/it, 408.38/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.516  R=0.606  Fβ=0.535\n",
      "[I 2025-06-12 18:24:39,587] Trial 21 pruned. \n",
      "  ↪ Fold 0:  P=0.517  R=0.244  Fβ=0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  77%|███████▋  | 23/30 [08:17<04:23, 37.67s/it, 497.72/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.484  R=0.422  Fβ=0.468\n",
      "✅ Trial 22 done  |  P=0.501  R=0.333  Fβ=0.438\n",
      "[I 2025-06-12 18:26:08,932] Trial 22 finished with value: 0.5619955676506027 and parameters: {'window': 30, 'batch': 64, 'lr': 0.0001550239950562203, 'epochs': 20, 'conv_blocks': 3, 'filters': 64, 'kernel': 4, 'act': 'selu', 'dropout': 0.08302150536540065, 'dense': 128, 'l2': 4.869273177241605e-06, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  80%|████████  | 24/30 [08:25<02:52, 28.77s/it, 505.74/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.519  R=0.639  Fβ=0.544\n",
      "[I 2025-06-12 18:26:16,946] Trial 23 pruned. \n",
      "  ↪ Fold 0:  P=0.505  R=0.168  Fβ=0.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  83%|████████▎ | 25/30 [09:04<02:38, 31.69s/it, 544.24/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 1:  P=0.491  R=0.460  Fβ=0.484\n",
      "✅ Trial 24 done  |  P=0.498  R=0.314  Fβ=0.412\n",
      "[I 2025-06-12 18:26:55,448] Trial 24 finished with value: 0.5875488563974622 and parameters: {'window': 36, 'batch': 64, 'lr': 0.00029344036730225725, 'epochs': 20, 'conv_blocks': 2, 'filters': 32, 'kernel': 4, 'act': 'selu', 'dropout': 0.13577478822090341, 'dense': 128, 'l2': 6.413009116721392e-06, 'pool': 'gmp', 'extra_dense': True}. Best is trial 0 with value: 0.4694371059124449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  87%|████████▋ | 26/30 [09:10<01:36, 24.20s/it, 550.96/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.515  R=0.643  Fβ=0.541\n",
      "[I 2025-06-12 18:27:02,174] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  90%|█████████ | 27/30 [09:33<01:10, 23.59s/it, 573.12/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.521  R=0.453  Fβ=0.503\n",
      "[I 2025-06-12 18:27:24,329] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  93%|█████████▎| 28/30 [09:45<00:40, 20.24s/it, 585.54/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.516  R=0.477  Fβ=0.506\n",
      "[I 2025-06-12 18:27:36,750] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437:  97%|█████████▋| 29/30 [09:54<00:16, 16.78s/it, 594.26/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.501  R=0.463  Fβ=0.492\n",
      "[I 2025-06-12 18:27:45,469] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.469437: 100%|██████████| 30/30 [10:00<00:00, 20.03s/it, 600.87/2100 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ↪ Fold 0:  P=0.512  R=0.698  Fβ=0.547\n",
      "[I 2025-06-12 18:27:52,076] Trial 29 pruned. \n",
      "\n",
      "🏆 Best F-β (β=0.56): 0.5306\n",
      "{\n",
      "  \"window\": 24,\n",
      "  \"batch\": 32,\n",
      "  \"lr\": 0.0010401663679887319,\n",
      "  \"epochs\": 20,\n",
      "  \"conv_blocks\": 1,\n",
      "  \"filters\": 32,\n",
      "  \"kernel\": 4,\n",
      "  \"act\": \"elu\",\n",
      "  \"dropout\": 0.2924774630404986,\n",
      "  \"dense\": 128,\n",
      "  \"l2\": 4.335281794951567e-06,\n",
      "  \"pool\": \"gmp\",\n",
      "  \"extra_dense\": false\n",
      "}\n",
      "💾 Params → cnn_best_params.json   |   Scaler → cnn_scaler.pkl\n",
      "🕒 Total time: 10.0 min   (2025-06-12 18:27:52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cnn_optuna_f0_56.py  (patched)\n",
    "# =============================================================\n",
    "# Walk-forward Optuna search for a 1-D CNN optimised on F-β (β=0.56)\n",
    "# =============================================================\n",
    "import os, json, warnings, joblib, optuna, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score]\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# ─────────────────────────  runtime & seeds  ───────────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ────────────────────────────  config  ─────────────────────────────────\n",
    "CSV_PATH   = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\")\n",
    "VAL_RATIO  = 0.20\n",
    "WF_FOLDS   = 2\n",
    "BETA       = 0.56                     # precision-weighted Fβ\n",
    "N_TRIALS   = 30\n",
    "TIMEOUT    = 35 * 60                  # seconds\n",
    "SCALER_OUT = \"cnn_scaler.pkl\"\n",
    "PARAMS_OUT = \"cnn_best_params.json\"\n",
    "\n",
    "DROP_COLS = [...]   # same long list as before (omitted for brevity)\n",
    "\n",
    "# ─────────────────────────── helpers  ──────────────────────────────────\n",
    "def f_beta(y_true, y_pred, beta=BETA):\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    return 0.0 if p + r == 0 else (1 + beta**2) * p * r / (beta**2 * p + r)\n",
    "\n",
    "def make_windows(arr, labels, win):\n",
    "    X, y = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        X.append(arr[i - win:i])\n",
    "        y.append(labels[i])\n",
    "    return np.asarray(X, np.float32), np.asarray(y, np.int8)\n",
    "\n",
    "def load_and_scale():\n",
    "    df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "    df = df.loc[\"2018-01-01\":]                       # discard early data\n",
    "    df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "    df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "    X_all = df.drop(columns=\"target\").values\n",
    "    y_all = df[\"target\"].astype(int).values\n",
    "    split_real = int(len(df) * (1 - VAL_RATIO))\n",
    "\n",
    "    scaler = StandardScaler().fit(X_all[:split_real])\n",
    "    X_scaled = scaler.transform(X_all).astype(np.float32)\n",
    "    joblib.dump(scaler, SCALER_OUT)\n",
    "\n",
    "    print(f\"✔ Scaler fitted on {split_real:,} rows | features = {X_all.shape[1]}\")\n",
    "    return X_scaled, y_all, split_real, X_all.shape[1]\n",
    "\n",
    "X_SCALED, Y_ALL, REAL_SPLIT_IDX, N_FEATS = load_and_scale()\n",
    "\n",
    "# ─────────────────────── model factory  ────────────────────────────────\n",
    "def build_model(trial, win):\n",
    "    conv_blocks  = trial.suggest_int(\"conv_blocks\", 1, 3)\n",
    "    base_filters = trial.suggest_int(\"filters\", 32, 96, step=32)\n",
    "    kernel       = trial.suggest_int(\"kernel\", 2, 4)\n",
    "    activation   = trial.suggest_categorical(\"act\", [\"relu\", \"elu\", \"selu\"])\n",
    "    dropout      = trial.suggest_float(\"dropout\", 0.05, 0.3)\n",
    "    dense_units  = trial.suggest_int(\"dense\", 64, 128, step=64)\n",
    "    l2reg        = trial.suggest_float(\"l2\", 1e-6, 1e-3, log=True)\n",
    "    pool_choice  = trial.suggest_categorical(\"pool\", [\"gap\", \"gmp\"])\n",
    "    extra_dense  = trial.suggest_categorical(\"extra_dense\", [True, False])\n",
    "\n",
    "    inp = layers.Input(shape=(win, N_FEATS))\n",
    "    x = inp\n",
    "    for b in range(conv_blocks):\n",
    "        f = base_filters * (2**b)\n",
    "        y = layers.Conv1D(f, kernel, padding=\"causal\",\n",
    "                          activation=activation,\n",
    "                          kernel_regularizer=regularizers.l2(l2reg))(x)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        if x.shape[-1] != y.shape[-1]:\n",
    "            x = layers.Conv1D(f, 1, padding=\"same\")(x)\n",
    "        x = layers.add([x, y])\n",
    "\n",
    "    x = (layers.GlobalAveragePooling1D() if pool_choice == \"gap\"\n",
    "         else layers.GlobalMaxPooling1D())(x)\n",
    "    x = layers.Dropout(dropout)(x)                    # always apply dropout\n",
    "\n",
    "    if extra_dense:\n",
    "        x = layers.Dense(dense_units, activation=activation,\n",
    "                         kernel_regularizer=regularizers.l2(l2reg))(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "# ───────────────────────  objective  ───────────────────────────────────\n",
    "def objective(trial):\n",
    "    win    = trial.suggest_int(\"window\", 12, 48, step=6)\n",
    "    batch  = trial.suggest_categorical(\"batch\", [32, 64])\n",
    "    lr     = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    epochs = trial.suggest_int(\"epochs\", 20, 60, step=20)\n",
    "\n",
    "    data_len = REAL_SPLIT_IDX - win\n",
    "    val_size = data_len // (WF_FOLDS + 1)\n",
    "    if val_size < 25:\n",
    "        return 1.0\n",
    "\n",
    "    f_scores, p_scores, r_scores = [], [], []\n",
    "\n",
    "    for fold in range(WF_FOLDS):\n",
    "        val_start = data_len - (WF_FOLDS - fold) * val_size\n",
    "        val_end   = val_start + val_size\n",
    "        if val_start < win * 2:\n",
    "            continue\n",
    "\n",
    "        X_fold = X_SCALED[:val_end + win]\n",
    "        y_fold = Y_ALL[:val_end + win]\n",
    "        X_win, y_win = make_windows(X_fold, y_fold, win)\n",
    "\n",
    "        X_tr, y_tr = X_win[:val_start],           y_win[:val_start]\n",
    "        X_va, y_va = X_win[val_start:val_end],    y_win[val_start:val_end]\n",
    "        if len(X_tr) < 50 or len(X_va) < 25:\n",
    "            continue\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = build_model(trial, win)\n",
    "        model.compile(keras.optimizers.Adam(lr), loss=\"binary_crossentropy\")\n",
    "\n",
    "        es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "        model.fit(X_tr, y_tr,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch,\n",
    "                  validation_data=(X_va, y_va),\n",
    "                  callbacks=[es],\n",
    "                  verbose=0)\n",
    "\n",
    "        prob = model.predict(X_va, verbose=0).ravel()\n",
    "        preds = (prob >= 0.5).astype(int)\n",
    "\n",
    "        p = precision_score(y_va, preds, zero_division=0)\n",
    "        r = recall_score(y_va, preds, zero_division=0)\n",
    "        f = f_beta(y_va, preds)\n",
    "\n",
    "        p_scores.append(p); r_scores.append(r); f_scores.append(f)\n",
    "        print(f\"  ↪ Fold {fold}:  P={p:.3f}  R={r:.3f}  Fβ={f:.3f}\")\n",
    "\n",
    "        trial.report(np.mean(f_scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    if not f_scores:\n",
    "        return 1.0\n",
    "\n",
    "    avg_p, avg_r, avg_f = map(np.mean, (p_scores, r_scores, f_scores))\n",
    "    # ----- per-trial summary print -----\n",
    "    print(f\"✅ Trial {trial.number} done  |  \"\n",
    "          f\"P={avg_p:.3f}  R={avg_r:.3f}  Fβ={avg_f:.3f}\")\n",
    "    return 1.0 - avg_f\n",
    "\n",
    "# ────────────────────────────── run  ───────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    study = optuna.create_study(direction=\"minimize\",\n",
    "                                sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5))\n",
    "\n",
    "    study.optimize(objective, n_trials=N_TRIALS,\n",
    "                   timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "    best_f = 1.0 - study.best_value\n",
    "    print(f\"\\n🏆 Best F-β (β={BETA:.2f}): {best_f:.4f}\")\n",
    "    print(json.dumps(study.best_params, indent=2))\n",
    "\n",
    "    with open(PARAMS_OUT, \"w\") as f:\n",
    "        json.dump(study.best_params, f, indent=2)\n",
    "    print(f\"💾 Params → {PARAMS_OUT}   |   Scaler → {SCALER_OUT}\")\n",
    "    print(f\"🕒 Total time: {(time.time()-t0)/60:.1f} min   \"\n",
    "          f\"({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "🏆 Top 5 Trials by F1-Score: - old param set\n",
    "Rank\tTrial\tPrecision\tRecall\tF1 Score\tNotable Hyperparameters Summary\n",
    "1️⃣\t#3\t0.527\t0.676\t0.593\twindow=36, batch=32, lr=0.00021, filters=64, kernel=2, act=relu, dropout=0.186, dense=64, conv_blocks=1, pool=gmp, extra_dense=True\n",
    "2️⃣\t#2\t0.516\t0.773\t0.620\twindow=24, batch=64, lr=0.00056, filters=32, kernel=4, act=elu, dropout=0.180, dense=128, conv_blocks=2, pool=gap, extra_dense=True\n",
    "3️⃣\t#27\t0.487\t0.615\t0.544\twindow=30, batch=32, lr=0.00202, filters=64, kernel=3, act=elu, dropout=0.147, dense=128, conv_blocks=2, pool=gmp, extra_dense=True\n",
    "4️⃣\t#4\t0.527\t0.528\t0.527\twindow=12, batch=32, lr=0.00173, filters=64, kernel=2, act=relu, dropout=0.066, dense=64, conv_blocks=1, pool=gap, extra_dense=True\n",
    "5️⃣\t#24\t0.528\t0.399\t0.454\twindow=18, batch=32, lr=0.00055, filters=32, kernel=4, act=elu, dropout=0.094, dense=128, conv_blocks=1, pool=gmp, extra_dense=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae59bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading & scaling data …\n",
      "\n",
      "Run-01  (win=12, filt=64, pool=gap)\n",
      "───────────────────────────────────\n",
      "Precision : 0.530   Recall : 0.492   F1 : 0.511\n",
      "\n",
      "Run-02  (win=30, filt=96, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.491   Recall : 0.144   F1 : 0.223\n",
      "\n",
      "Run-03  (win=36, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.515   Recall : 0.535   F1 : 0.525\n",
      "\n",
      "Run-04  (win=24, filt=32, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.525   Recall : 0.659   F1 : 0.584\n",
      "\n",
      "Run-05  (win=30, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.535   Recall : 0.138   F1 : 0.220\n",
      "\n",
      "Run-06  (win=12, filt=64, pool=gap)\n",
      "───────────────────────────────────\n",
      "Precision : 0.530   Recall : 0.437   F1 : 0.479\n",
      "\n",
      "🏆  Leaderboard (sorted by F1)\n",
      " 1. F1=0.584  P=0.525  R=0.659  (win=24, filt=32, pool=gmp)\n",
      " 2. F1=0.525  P=0.515  R=0.535  (win=36, filt=64, pool=gmp)\n",
      " 3. F1=0.511  P=0.530  R=0.492  (win=12, filt=64, pool=gap)\n",
      " 4. F1=0.479  P=0.530  R=0.437  (win=12, filt=64, pool=gap)\n",
      " 5. F1=0.223  P=0.491  R=0.144  (win=30, filt=96, pool=gmp)\n",
      " 6. F1=0.220  P=0.535  R=0.138  (win=30, filt=64, pool=gmp)\n",
      "\n",
      "📑 Comparison summary saved → cnn_param_comparison_summary.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cnn_compare_param_sets.py\n",
    "─────────────────────────\n",
    "Trains - and compares - several 1-D CNN configurations on the 4-hour BTC data.\n",
    "Each configuration is built exactly from the hyper-parameter dictionary you\n",
    "supply in `PARAM_SETS`.\n",
    "The script prints Precision, Recall, F1 for every run and a final leaderboard.\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Imports & global set-up\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "import os, json, joblib, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, roc_auc_score)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Data paths & constants\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "CSV_PATH = Path(r\"C:/Users/ADMIN/Desktop/Coding_projects/stock_market_prediction/\"\n",
    "                r\"Stock-Market-Prediction/data/processed/\"\n",
    "                r\"gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "VAL_FRAC   = 0.20          # last 20 % is the validation window\n",
    "EPOCHS     = 80            # same for every run\n",
    "EARLY_STOP = 12\n",
    "L2_REG_DEF = 1e-6          # fallback if a param dict misses 'l2'\n",
    "CONV_BLOCKS = 1            # fixed depth – keep the test fast\n",
    "\n",
    "# Columns to drop (same list you tuned on – prevents data leakage)\n",
    "DROP_COLS = [  # … (list exactly as before – shortened for brevity here)\n",
    "    'open','high','low','typical_price','EMA_7','EMA_21','SMA_20','SMA_50',\n",
    "    'vwap_24h','close_4h','bollinger_upper','bollinger_lower','resistance_level',\n",
    "    'support_level','high_low','high_close','low_close','true_range',\n",
    "    #  … snip …\n",
    "    'overbought_reversal','close'\n",
    "]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# The six candidate hyper-parameter sets\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "PARAM_SETS = [\n",
    "    {'window': 12, 'batch': 64, 'lr': 0.000898, 'filters': 64, 'kernel': 3,\n",
    "     'act': 'elu',  'dropout': 0.209, 'dense': 64,  'l2': 3.35e-05,\n",
    "     'pool': 'gap', 'extra_dense': False},\n",
    "\n",
    "    {'window': 30, 'batch': 32, 'lr': 0.001545, 'filters': 96, 'kernel': 4,\n",
    "     'act': 'elu',  'dropout': 0.14,  'dense': 128, 'l2': 0.000168,\n",
    "     'pool': 'gmp', 'extra_dense': False},\n",
    "\n",
    "    {'window': 36, 'batch': 32, 'lr': 0.000215, 'filters': 64, 'kernel': 2,\n",
    "     'act': 'relu', 'dropout': 0.186, 'dense': 64,  'l2': 0.000255,\n",
    "     'pool': 'gmp', 'extra_dense': True},\n",
    "\n",
    "    {'window': 24, 'batch': 32, 'lr': 0.00104,  'filters': 32, 'kernel': 4,\n",
    "     'act': 'elu',  'dropout': 0.292, 'dense': 128, 'l2': 4.33e-06,\n",
    "     'pool': 'gmp', 'extra_dense': False},\n",
    "\n",
    "    {'window': 30, 'batch': 64, 'lr': 0.000172, 'filters': 64, 'kernel': 4,\n",
    "     'act': 'selu','dropout': 0.061, 'dense': 128, 'l2': 3.24e-06,\n",
    "     'pool': 'gmp', 'extra_dense': True},\n",
    "\n",
    "    # Duplicate of the first – still included for completeness\n",
    "    {'window': 12, 'batch': 64, 'lr': 0.000898, 'filters': 64, 'kernel': 3,\n",
    "     'act': 'elu',  'dropout': 0.209, 'dense': 64,  'l2': 3.35e-05,\n",
    "     'pool': 'gap', 'extra_dense': False}\n",
    "]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Data prep helpers\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def make_windows(arr, labels, win):\n",
    "    xs, ys = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        xs.append(arr[i-win:i])\n",
    "        ys.append(labels[i])\n",
    "    return (np.asarray(xs, np.float32), np.asarray(ys, np.int8))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Model factory\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_model(cfg, n_features):\n",
    "    \"\"\"Construct a simple residual-style 1-D CNN from cfg dict.\"\"\"\n",
    "    l2 = cfg.get(\"l2\", L2_REG_DEF)\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_features))\n",
    "    x   = inp\n",
    "\n",
    "    for b in range(CONV_BLOCKS):\n",
    "        f = cfg[\"filters\"] * (2 ** b)\n",
    "        y = layers.Conv1D(f, cfg[\"kernel\"], padding=\"causal\",\n",
    "                          activation=cfg[\"act\"],\n",
    "                          kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        if CONV_BLOCKS > 1:\n",
    "            if x.shape[-1] != y.shape[-1]:\n",
    "                x = layers.Conv1D(f, 1, padding=\"same\")(x)\n",
    "            x = layers.Add()([x, y])\n",
    "        else:\n",
    "            x = y\n",
    "\n",
    "    pool = layers.GlobalMaxPooling1D if cfg[\"pool\"] == \"gmp\" else layers.GlobalAveragePooling1D\n",
    "    x    = pool()(x)\n",
    "    x    = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "\n",
    "    if cfg[\"extra_dense\"]:\n",
    "        x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                         kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1. Load / scale data once\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "print(\"📊 Loading & scaling data …\")\n",
    "df = (pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "        .loc[\"2018-01-01\":]\n",
    "        .drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        .dropna(subset=[\"target\"]).dropna())\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "n_features = X_raw.shape[1]\n",
    "\n",
    "split = int(len(df) * (1 - VAL_FRAC))\n",
    "scaler = StandardScaler().fit(X_raw[:split])\n",
    "X_train_scaled = scaler.transform(X_raw[:split]).astype(np.float32)\n",
    "X_val_scaled   = scaler.transform(X_raw[split:]).astype(np.float32)\n",
    "y_train_raw    = y_raw[:split]\n",
    "y_val_raw      = y_raw[split:]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2. Iterate over parameter sets\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "results = []\n",
    "\n",
    "for idx, cfg in enumerate(PARAM_SETS, 1):\n",
    "    tag = f\"Run-{idx:02d}  (win={cfg['window']}, filt={cfg['filters']}, pool={cfg['pool']})\"\n",
    "    print(f\"\\n{tag}\\n\" + \"─\" * len(tag))\n",
    "\n",
    "    # --- prepare windows for this window size\n",
    "    X_tr, y_tr = make_windows(X_train_scaled, y_train_raw, cfg[\"window\"])\n",
    "    X_va, y_va = make_windows(X_val_scaled,   y_val_raw,   cfg[\"window\"])\n",
    "\n",
    "    # --- build & train\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_model(cfg, n_features)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(cfg[\"lr\"]),\n",
    "                  loss=\"binary_crossentropy\")\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(patience=EARLY_STOP,\n",
    "                                        restore_best_weights=True,\n",
    "                                        verbose=0)]\n",
    "    model.fit(X_tr, y_tr,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=cfg[\"batch\"],\n",
    "              validation_data=(X_va, y_va),\n",
    "              callbacks=cb,\n",
    "              verbose=0)\n",
    "\n",
    "    # --- evaluate\n",
    "    prob = model.predict(X_va, verbose=0).ravel()\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    prec = precision_score(y_va, pred, zero_division=0)\n",
    "    rec  = recall_score(y_va, pred,    zero_division=0)\n",
    "    f1   = f1_score(y_va, pred,        zero_division=0)\n",
    "\n",
    "    print(f\"Precision : {prec:5.3f}   Recall : {rec:5.3f}   F1 : {f1:5.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        **cfg,\n",
    "        \"precision\": prec,\n",
    "        \"recall\"   : rec,\n",
    "        \"f1\"       : f1,\n",
    "        \"auc\"      : roc_auc_score(y_va, prob)\n",
    "    })\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3. Leaderboard\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n🏆  Leaderboard (sorted by F1)\")\n",
    "results_sorted = sorted(results, key=lambda d: d[\"f1\"], reverse=True)\n",
    "\n",
    "for rk, res in enumerate(results_sorted, 1):\n",
    "    print(f\"{rk:>2}. F1={res['f1']:.3f}  \"\n",
    "          f\"P={res['precision']:.3f}  R={res['recall']:.3f}  \"\n",
    "          f\"(win={res['window']}, filt={res['filters']}, pool={res['pool']})\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4. Optional – save summary JSON\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "summary_path = \"cnn_param_comparison_summary.json\"\n",
    "with open(summary_path, \"w\") as fp:\n",
    "    json.dump({\n",
    "        \"timestamp\" : datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        \"metrics\"   : results_sorted\n",
    "    }, fp, indent=2)\n",
    "\n",
    "print(f\"\\n📑 Comparison summary saved → {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c01488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading & scaling data …\n",
      "\n",
      "Run-01  (win=12, filt=64, pool=gap)\n",
      "───────────────────────────────────\n",
      "Precision : 0.531   Recall : 0.494   F1 : 0.512\n",
      "\n",
      "Run-02  (win=30, filt=96, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.510   Recall : 0.105   F1 : 0.174\n",
      "\n",
      "Run-03  (win=36, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.523   Recall : 0.576   F1 : 0.548\n",
      "\n",
      "Run-04  (win=24, filt=32, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.518   Recall : 0.832   F1 : 0.638\n",
      "\n",
      "Run-05  (win=30, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.615   Recall : 0.010   F1 : 0.019\n",
      "\n",
      "Run-06  (win=24, filt=32, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.522   Recall : 0.661   F1 : 0.583\n",
      "\n",
      "Run-07  (win=18, filt=96, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.515   Recall : 0.565   F1 : 0.539\n",
      "\n",
      "Run-08  (win=36, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.532   Recall : 0.266   F1 : 0.354\n",
      "\n",
      "Run-09  (win=24, filt=32, pool=gap)\n",
      "───────────────────────────────────\n",
      "Precision : 0.528   Recall : 0.720   F1 : 0.609\n",
      "\n",
      "Run-10  (win=30, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.524   Recall : 0.020   F1 : 0.039\n",
      "\n",
      "Run-11  (win=30, filt=64, pool=gmp)\n",
      "───────────────────────────────────\n",
      "Precision : 0.490   Recall : 0.015   F1 : 0.030\n",
      "\n",
      "🏆  Leaderboard (sorted by F1)\n",
      " 1. F1=0.638  P=0.518  R=0.832  (win=24, filt=32, pool=gmp)\n",
      " 2. F1=0.609  P=0.528  R=0.720  (win=24, filt=32, pool=gap)\n",
      " 3. F1=0.583  P=0.522  R=0.661  (win=24, filt=32, pool=gmp)\n",
      " 4. F1=0.548  P=0.523  R=0.576  (win=36, filt=64, pool=gmp)\n",
      " 5. F1=0.539  P=0.515  R=0.565  (win=18, filt=96, pool=gmp)\n",
      " 6. F1=0.512  P=0.531  R=0.494  (win=12, filt=64, pool=gap)\n",
      " 7. F1=0.354  P=0.532  R=0.266  (win=36, filt=64, pool=gmp)\n",
      " 8. F1=0.174  P=0.510  R=0.105  (win=30, filt=96, pool=gmp)\n",
      " 9. F1=0.039  P=0.524  R=0.020  (win=30, filt=64, pool=gmp)\n",
      "10. F1=0.030  P=0.490  R=0.015  (win=30, filt=64, pool=gmp)\n",
      "11. F1=0.019  P=0.615  R=0.010  (win=30, filt=64, pool=gmp)\n",
      "\n",
      "📑 Comparison summary saved → cnn_param_comparison_summary.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cnn_compare_param_sets.py\n",
    "─────────────────────────\n",
    "Trains - and compares - several 1-D CNN configurations on the 4-hour BTC data.\n",
    "Each configuration is built exactly from the hyper-parameter dictionary you\n",
    "supply in `PARAM_SETS`.\n",
    "The script prints Precision, Recall, F1 for every run and a final leaderboard.\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Imports & global set-up\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "import os, json, joblib, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, roc_auc_score)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Data paths & constants\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "CSV_PATH = Path(r\"C:/Users/ADMIN/Desktop/Coding_projects/stock_market_prediction/\"\n",
    "                r\"Stock-Market-Prediction/data/processed/\"\n",
    "                r\"gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "VAL_FRAC   = 0.20          # last 20 % is the validation window\n",
    "EPOCHS     = 80            # same for every run\n",
    "EARLY_STOP = 12\n",
    "L2_REG_DEF = 1e-6          # fallback if a param dict misses 'l2'\n",
    "CONV_BLOCKS = 1            # fixed depth – keep the test fast\n",
    "\n",
    "# Columns to drop (same list you tuned on – prevents data leakage)\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_7', 'EMA_21', 'SMA_20', \n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower', \n",
    "    'resistance_level', 'support_level', 'high_low', 'high_close', 'low_close',\n",
    "    'true_range', 'volume_mean_20', 'MACD_line', 'MACD_signal',\n",
    "    'bollinger_width', 'volatility_regime',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'break_upper_band', 'break_lower_band', 'rsi_oversold', 'rsi_overbought',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21', 'macd_positive',\n",
    "    'volume_breakout', 'volume_breakdown', 'stoch_overbought', 'stoch_oversold',\n",
    "    'cci_overbought', 'cci_oversold',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6''ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal', 'trending_market'\n",
    "]\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# The six candidate hyper-parameter sets\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "PARAM_SETS = [\n",
    "    # Original sets\n",
    "    {'window': 12, 'batch': 64, 'lr': 0.000898, 'filters': 64, 'kernel': 3,\n",
    "     'act': 'elu',  'dropout': 0.209, 'dense': 64,  'l2': 3.35e-05,\n",
    "     'pool': 'gap', 'extra_dense': False, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    {'window': 30, 'batch': 32, 'lr': 0.001545, 'filters': 96, 'kernel': 4,\n",
    "     'act': 'elu',  'dropout': 0.14,  'dense': 128, 'l2': 0.000168,\n",
    "     'pool': 'gmp', 'extra_dense': False, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    {'window': 36, 'batch': 32, 'lr': 0.000215, 'filters': 64, 'kernel': 2,\n",
    "     'act': 'relu', 'dropout': 0.186, 'dense': 64,  'l2': 0.000255,\n",
    "     'pool': 'gmp', 'extra_dense': True, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    {'window': 24, 'batch': 32, 'lr': 0.00104,  'filters': 32, 'kernel': 4,\n",
    "     'act': 'elu',  'dropout': 0.292, 'dense': 128, 'l2': 4.33e-06,\n",
    "     'pool': 'gmp', 'extra_dense': False, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    {'window': 30, 'batch': 64, 'lr': 0.000172, 'filters': 64, 'kernel': 4,\n",
    "     'act': 'selu','dropout': 0.061, 'dense': 128, 'l2': 3.24e-06,\n",
    "     'pool': 'gmp', 'extra_dense': True, 'conv_blocks': 2, 'epochs': 20},\n",
    "\n",
    "    # NEW ADDITIONS based on optimization results\n",
    "    \n",
    "    # 1. Current optimization winner - highest weighted F1 (0.531)\n",
    "    {'window': 24, 'batch': 32, 'lr': 0.0010401663679887319, 'filters': 32, 'kernel': 4,\n",
    "     'act': 'elu', 'dropout': 0.2924774630404986, 'dense': 128, 'l2': 4.335281794951567e-06,\n",
    "     'pool': 'gmp', 'extra_dense': False, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    # 2. Trial 8 from current optimization - your training script config\n",
    "    {'window': 18, 'batch': 64, 'lr': 0.0007145565133513971, 'filters': 96, 'kernel': 2,\n",
    "     'act': 'selu', 'dropout': 0.1105138178778751, 'dense': 128, 'l2': 0.00019268985325226193,\n",
    "     'pool': 'gmp', 'extra_dense': False, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    # 3. Old Trial #3 - top old performer (0.593 standard F1)\n",
    "    {'window': 36, 'batch': 32, 'lr': 0.000215, 'filters': 64, 'kernel': 2,\n",
    "     'act': 'relu', 'dropout': 0.186, 'dense': 64, 'l2': 0.000255,\n",
    "     'pool': 'gmp', 'extra_dense': True, 'conv_blocks': 1, 'epochs': 20},\n",
    "\n",
    "    # 4. Old Trial #2 - high recall config (0.620 standard F1, 0.773 recall)\n",
    "    {'window': 24, 'batch': 64, 'lr': 0.00056, 'filters': 32, 'kernel': 4,\n",
    "     'act': 'elu', 'dropout': 0.180, 'dense': 128, 'l2': 1e-5,\n",
    "     'pool': 'gap', 'extra_dense': True, 'conv_blocks': 2, 'epochs': 20},\n",
    "\n",
    "    # 5. Current Trial 1 - multi-block architecture\n",
    "    {'window': 30, 'batch': 64, 'lr': 0.00017258215396625024, 'filters': 64, 'kernel': 4,\n",
    "     'act': 'selu', 'dropout': 0.061612603179999434, 'dense': 128, 'l2': 3.247673570627449e-06,\n",
    "     'pool': 'gmp', 'extra_dense': True, 'conv_blocks': 2, 'epochs': 20},\n",
    "\n",
    "    # 6. Old Trial #27 - balanced config\n",
    "    {'window': 30, 'batch': 32, 'lr': 0.00202, 'filters': 64, 'kernel': 3,\n",
    "     'act': 'elu', 'dropout': 0.147, 'dense': 128, 'l2': 5e-5,\n",
    "     'pool': 'gmp', 'extra_dense': True, 'conv_blocks': 2, 'epochs': 20}\n",
    "]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Data prep helpers\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def make_windows(arr, labels, win):\n",
    "    xs, ys = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        xs.append(arr[i-win:i])\n",
    "        ys.append(labels[i])\n",
    "    return (np.asarray(xs, np.float32), np.asarray(ys, np.int8))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Model factory\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_model(cfg, n_features):\n",
    "    \"\"\"Construct a simple residual-style 1-D CNN from cfg dict.\"\"\"\n",
    "    l2 = cfg.get(\"l2\", L2_REG_DEF)\n",
    "    inp = layers.Input(shape=(cfg[\"window\"], n_features))\n",
    "    x   = inp\n",
    "\n",
    "    for b in range(CONV_BLOCKS):\n",
    "        f = cfg[\"filters\"] * (2 ** b)\n",
    "        y = layers.Conv1D(f, cfg[\"kernel\"], padding=\"causal\",\n",
    "                          activation=cfg[\"act\"],\n",
    "                          kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        if CONV_BLOCKS > 1:\n",
    "            if x.shape[-1] != y.shape[-1]:\n",
    "                x = layers.Conv1D(f, 1, padding=\"same\")(x)\n",
    "            x = layers.Add()([x, y])\n",
    "        else:\n",
    "            x = y\n",
    "\n",
    "    pool = layers.GlobalMaxPooling1D if cfg[\"pool\"] == \"gmp\" else layers.GlobalAveragePooling1D\n",
    "    x    = pool()(x)\n",
    "    x    = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "\n",
    "    if cfg[\"extra_dense\"]:\n",
    "        x = layers.Dense(cfg[\"dense\"], activation=cfg[\"act\"],\n",
    "                         kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1. Load / scale data once\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "print(\"📊 Loading & scaling data …\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "\n",
    "# Now it's safe to reference df.columns\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "n_features = X_raw.shape[1]\n",
    "\n",
    "split = int(len(df) * (1 - VAL_FRAC))\n",
    "scaler = StandardScaler().fit(X_raw[:split])\n",
    "X_train_scaled = scaler.transform(X_raw[:split]).astype(np.float32)\n",
    "X_val_scaled   = scaler.transform(X_raw[split:]).astype(np.float32)\n",
    "y_train_raw    = y_raw[:split]\n",
    "y_val_raw      = y_raw[split:]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2. Iterate over parameter sets\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "results = []\n",
    "\n",
    "for idx, cfg in enumerate(PARAM_SETS, 1):\n",
    "    tag = f\"Run-{idx:02d}  (win={cfg['window']}, filt={cfg['filters']}, pool={cfg['pool']})\"\n",
    "    print(f\"\\n{tag}\\n\" + \"─\" * len(tag))\n",
    "\n",
    "    # --- prepare windows for this window size\n",
    "    X_tr, y_tr = make_windows(X_train_scaled, y_train_raw, cfg[\"window\"])\n",
    "    X_va, y_va = make_windows(X_val_scaled,   y_val_raw,   cfg[\"window\"])\n",
    "\n",
    "    # --- build & train\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_model(cfg, n_features)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(cfg[\"lr\"]),\n",
    "                  loss=\"binary_crossentropy\")\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(patience=EARLY_STOP,\n",
    "                                        restore_best_weights=True,\n",
    "                                        verbose=0)]\n",
    "    model.fit(X_tr, y_tr,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=cfg[\"batch\"],\n",
    "              validation_data=(X_va, y_va),\n",
    "              callbacks=cb,\n",
    "              verbose=0)\n",
    "\n",
    "    # --- evaluate\n",
    "    prob = model.predict(X_va, verbose=0).ravel()\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    prec = precision_score(y_va, pred, zero_division=0)\n",
    "    rec  = recall_score(y_va, pred,    zero_division=0)\n",
    "    f1   = f1_score(y_va, pred,        zero_division=0)\n",
    "\n",
    "    print(f\"Precision : {prec:5.3f}   Recall : {rec:5.3f}   F1 : {f1:5.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        **cfg,\n",
    "        \"precision\": prec,\n",
    "        \"recall\"   : rec,\n",
    "        \"f1\"       : f1,\n",
    "        \"auc\"      : roc_auc_score(y_va, prob)\n",
    "    })\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3. Leaderboard\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n🏆  Leaderboard (sorted by F1)\")\n",
    "results_sorted = sorted(results, key=lambda d: d[\"f1\"], reverse=True)\n",
    "\n",
    "for rk, res in enumerate(results_sorted, 1):\n",
    "    print(f\"{rk:>2}. F1={res['f1']:.3f}  \"\n",
    "          f\"P={res['precision']:.3f}  R={res['recall']:.3f}  \"\n",
    "          f\"(win={res['window']}, filt={res['filters']}, pool={res['pool']})\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4. Optional – save summary JSON\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "summary_path = \"cnn_param_comparison_summary.json\"\n",
    "with open(summary_path, \"w\") as fp:\n",
    "    json.dump({\n",
    "        \"timestamp\" : datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        \"metrics\"   : results_sorted\n",
    "    }, fp, indent=2)\n",
    "\n",
    "print(f\"\\n📑 Comparison summary saved → {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e88e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    " {'window': 24, 'batch': 64, 'lr': 0.00056, 'filters': 32, 'kernel': 4,\n",
    "     'act': 'elu', 'dropout': 0.180, 'dense': 128, 'l2': 1e-5,\n",
    "     'pool': 'gap', 'extra_dense': True, 'conv_blocks': 2, 'epochs': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2db9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading & preprocessing data...\n",
      "   Total samples: 15,855\n",
      "   Train windows: 12,660\n",
      "   Val windows: 3,147\n",
      "   Features: 19\n",
      "   Window size: 24\n",
      "\n",
      "🏗️ Building model with optimal parameters...\n",
      "   Architecture: 2 conv blocks, 32 filters, kernel=4\n",
      "   Activation: elu, Pool: gap\n",
      "   Dropout: 0.18, L2: 1e-05\n",
      "\n",
      "🚀 Training model...\n",
      "Epoch 1/50\n",
      "198/198 - 2s - 10ms/step - accuracy: 0.5000 - loss: 0.7179 - val_accuracy: 0.4944 - val_loss: 0.6981 - learning_rate: 5.6000e-04\n",
      "Epoch 2/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5066 - loss: 0.7033 - val_accuracy: 0.5122 - val_loss: 0.6963 - learning_rate: 5.6000e-04\n",
      "Epoch 3/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5087 - loss: 0.7006 - val_accuracy: 0.5195 - val_loss: 0.6963 - learning_rate: 5.6000e-04\n",
      "Epoch 4/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5059 - loss: 0.6996 - val_accuracy: 0.5151 - val_loss: 0.6969 - learning_rate: 5.6000e-04\n",
      "Epoch 5/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5083 - loss: 0.6982 - val_accuracy: 0.5202 - val_loss: 0.6962 - learning_rate: 5.6000e-04\n",
      "Epoch 6/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5110 - loss: 0.6979 - val_accuracy: 0.5249 - val_loss: 0.6966 - learning_rate: 5.6000e-04\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002800000074785203.\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5130 - loss: 0.6965 - val_accuracy: 0.5234 - val_loss: 0.6973 - learning_rate: 5.6000e-04\n",
      "Epoch 8/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5160 - loss: 0.6957 - val_accuracy: 0.5091 - val_loss: 0.6950 - learning_rate: 2.8000e-04\n",
      "Epoch 9/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5214 - loss: 0.6944 - val_accuracy: 0.5110 - val_loss: 0.6949 - learning_rate: 2.8000e-04\n",
      "Epoch 10/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5166 - loss: 0.6951 - val_accuracy: 0.5135 - val_loss: 0.6947 - learning_rate: 2.8000e-04\n",
      "Epoch 11/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5155 - loss: 0.6946 - val_accuracy: 0.5068 - val_loss: 0.6956 - learning_rate: 2.8000e-04\n",
      "Epoch 12/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5159 - loss: 0.6942 - val_accuracy: 0.5132 - val_loss: 0.6952 - learning_rate: 2.8000e-04\n",
      "Epoch 13/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5209 - loss: 0.6948 - val_accuracy: 0.5129 - val_loss: 0.6957 - learning_rate: 2.8000e-04\n",
      "Epoch 14/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5223 - loss: 0.6939 - val_accuracy: 0.5135 - val_loss: 0.6954 - learning_rate: 2.8000e-04\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00014000000373926014.\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5209 - loss: 0.6945 - val_accuracy: 0.5157 - val_loss: 0.6951 - learning_rate: 2.8000e-04\n",
      "Epoch 16/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5250 - loss: 0.6934 - val_accuracy: 0.5141 - val_loss: 0.6949 - learning_rate: 1.4000e-04\n",
      "Epoch 17/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5226 - loss: 0.6932 - val_accuracy: 0.5056 - val_loss: 0.6951 - learning_rate: 1.4000e-04\n",
      "Epoch 18/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5311 - loss: 0.6917 - val_accuracy: 0.5011 - val_loss: 0.6954 - learning_rate: 1.4000e-04\n",
      "Epoch 19/50\n",
      "198/198 - 1s - 4ms/step - accuracy: 0.5282 - loss: 0.6933 - val_accuracy: 0.5062 - val_loss: 0.6953 - learning_rate: 1.4000e-04\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 7.000000186963007e-05.\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5218 - loss: 0.6929 - val_accuracy: 0.5138 - val_loss: 0.6954 - learning_rate: 1.4000e-04\n",
      "Epoch 21/50\n",
      "198/198 - 1s - 4ms/step - accuracy: 0.5363 - loss: 0.6913 - val_accuracy: 0.5068 - val_loss: 0.6958 - learning_rate: 7.0000e-05\n",
      "Epoch 22/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5296 - loss: 0.6921 - val_accuracy: 0.5021 - val_loss: 0.6960 - learning_rate: 7.0000e-05\n",
      "Epoch 23/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5292 - loss: 0.6917 - val_accuracy: 0.5011 - val_loss: 0.6962 - learning_rate: 7.0000e-05\n",
      "Epoch 24/50\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5280 - loss: 0.6920 - val_accuracy: 0.5002 - val_loss: 0.6962 - learning_rate: 7.0000e-05\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.5000000934815034e-05.\n",
      "198/198 - 1s - 3ms/step - accuracy: 0.5306 - loss: 0.6917 - val_accuracy: 0.5052 - val_loss: 0.6961 - learning_rate: 7.0000e-05\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "📊 Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Validation Metrics ──\n",
      "accuracy            :  0.514\n",
      "precision           :  0.525\n",
      "recall              :  0.713\n",
      "weighted_f1_a2      :  0.637\n",
      "auc                 :  0.506\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.48      0.30      0.37      1503\n",
      "          Up       0.53      0.71      0.60      1644\n",
      "\n",
      "    accuracy                           0.51      3147\n",
      "   macro avg       0.51      0.50      0.49      3147\n",
      "weighted avg       0.51      0.51      0.49      3147\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 444 1059]\n",
      " [ 472 1172]]\n",
      "\n",
      "📁 Generating predictions CSV...\n",
      "✅ Predictions saved → cnn_predictions.csv\n",
      "   Total predictions: 3,147\n",
      "\n",
      "📋 Sample predictions:\n",
      "          timestamp  prob_up  prob_down  winning_prob  prediction  actual\n",
      "2023-10-20 16:00:00 0.520832   0.479168      0.520832           1       1\n",
      "2023-10-20 20:00:00 0.520022   0.479978      0.520022           1       1\n",
      "2023-10-21 00:00:00 0.527823   0.472177      0.527823           1       0\n",
      "2023-10-21 04:00:00 0.537765   0.462235      0.537765           1       1\n",
      "2023-10-21 08:00:00 0.544090   0.455910      0.544090           1       1\n",
      "2023-10-21 12:00:00 0.545080   0.454920      0.545080           1       1\n",
      "2023-10-21 16:00:00 0.533631   0.466369      0.533631           1       1\n",
      "2023-10-21 20:00:00 0.528141   0.471859      0.528141           1       0\n",
      "2023-10-22 00:00:00 0.516418   0.483582      0.516418           1       0\n",
      "2023-10-22 04:00:00 0.506279   0.493721      0.506279           1       0\n",
      "\n",
      "💾 Saving artifacts...\n",
      "✅ Model saved   → cnn_optimal_final.h5\n",
      "✅ Scaler saved  → cnn_scaler_final.pkl\n",
      "✅ Summary saved → cnn_final_training_summary.json\n",
      "\n",
      "🎉 Training Complete!\n",
      "══════════════════════════════════════════════════\n",
      "📈 Final Performance:\n",
      "   Weighted F1 (α=2.0): 0.637\n",
      "   Accuracy: 0.514\n",
      "   Precision: 0.525\n",
      "   Recall: 0.713\n",
      "   AUC: 0.506\n",
      "\n",
      "📁 Files Generated:\n",
      "   • cnn_optimal_final.h5 - Trained model\n",
      "   • cnn_scaler_final.pkl - Feature scaler\n",
      "   • cnn_predictions.csv - Validation predictions (3,147 rows)\n",
      "   • cnn_final_training_summary.json - Training summary\n",
      "\n",
      "✨ Ready for production use!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cnn_final_train_predict.py\n",
    "──────────────────────────\n",
    "Trains a 1-D CNN with optimal hyperparameters and generates predictions CSV.\n",
    "Uses the best parameters found during optimization for final model training.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, joblib, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "# ═══════════════ Setup ══════════════════════════════════════════════\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ═══════════════ Configuration ══════════════════════════════════════\n",
    "CSV_PATH = Path(r\"C:/Users/ADMIN/Desktop/Coding_projects/stock_market_prediction/\"\n",
    "                r\"Stock-Market-Prediction/data/processed/\"\n",
    "                r\"gemini_btc_with_features_4h.csv\")\n",
    "\n",
    "# Output files\n",
    "MODEL_OUT = \"cnn_optimal_final.h5\"\n",
    "SCALER_OUT = \"cnn_scaler_final.pkl\"\n",
    "PREDICTIONS_OUT = \"cnn_predictions.csv\"\n",
    "SUMMARY_JSON = \"cnn_final_training_summary.json\"\n",
    "\n",
    "# Optimal hyperparameters\n",
    "OPTIMAL_PARAMS = {\n",
    "    'window': 24,\n",
    "    'batch': 64, \n",
    "    'lr': 0.00056,\n",
    "    'filters': 32,\n",
    "    'kernel': 4,\n",
    "    'act': 'elu',\n",
    "    'dropout': 0.180,\n",
    "    'dense': 128,\n",
    "    'l2': 1e-5,\n",
    "    'pool': 'gap',\n",
    "    'extra_dense': True,\n",
    "    'conv_blocks': 2,\n",
    "    'epochs': 50  # Increased from 20 for better optimization\n",
    "}\n",
    "\n",
    "VAL_FRAC = 0.20\n",
    "EARLY_STOP = 15  # Increased patience for longer training\n",
    "ALPHA = 2.0  # For weighted F1\n",
    "\n",
    "# Complete DROP_COLS list\n",
    "DROP_COLS = [\n",
    "    'open', 'high', 'low', 'typical_price', 'EMA_7', 'EMA_21', 'SMA_20', 'SMA_50',\n",
    "    'vwap_24h', 'close_4h', 'bollinger_upper', 'bollinger_lower', 'resistance_level',\n",
    "    'support_level', 'high_low', 'high_close', 'low_close', 'true_range', 'volume_mean_20',\n",
    "    'MACD_line', 'MACD_signal', 'bollinger_width', 'volatility_regime', 'CCI', 'stoch_%D',\n",
    "    'parkinson_vol', 'ema_cross_down', 'macd_cross_down', 'vol_spike_1_5x', 'near_upper_band',\n",
    "    'near_lower_band', 'break_upper_band', 'break_lower_band', 'rsi_oversold', 'rsi_overbought',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21', 'macd_positive', 'volume_breakout',\n",
    "    'volume_breakdown', 'stoch_overbought', 'stoch_oversold', 'cci_overbought', 'cci_oversold',\n",
    "    'trending_market', 'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6', 'bearish_scenario_1',\n",
    "    'bearish_scenario_2', 'bearish_scenario_3', 'bearish_scenario_4', 'bearish_scenario_6',\n",
    "    'ema_cross_up', 'macd_cross_up', 'oversold_reversal', 'overbought_reversal', 'close'\n",
    "]\n",
    "\n",
    "# ═══════════════ Helper Functions ═══════════════════════════════════\n",
    "def weighted_f1(y_true, y_pred, alpha=ALPHA):\n",
    "    \"\"\"Calculate weighted F1 score\"\"\"\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    return 0.0 if p + r == 0 else (1 + alpha) * p * r / (alpha * p + r)\n",
    "\n",
    "def make_windows(arr, labels, win):\n",
    "    \"\"\"Create sliding windows for time series data\"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(win, len(arr)):\n",
    "        xs.append(arr[i-win:i])\n",
    "        ys.append(labels[i])\n",
    "    return np.asarray(xs, np.float32), np.asarray(ys, np.int8)\n",
    "\n",
    "def build_model(win, n_features):\n",
    "    \"\"\"Build CNN model with optimal parameters\"\"\"\n",
    "    params = OPTIMAL_PARAMS\n",
    "    \n",
    "    inp = layers.Input(shape=(win, n_features))\n",
    "    x = inp\n",
    "\n",
    "    # Multi-block CNN architecture\n",
    "    for b in range(params['conv_blocks']):\n",
    "        f = params['filters'] * (2 ** b)\n",
    "        y = layers.Conv1D(f, params['kernel'], padding=\"causal\",\n",
    "                          activation=params['act'],\n",
    "                          kernel_regularizer=regularizers.l2(params['l2']))(x)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        \n",
    "        # Residual connections for multi-block\n",
    "        if params['conv_blocks'] > 1:\n",
    "            if x.shape[-1] != y.shape[-1]:\n",
    "                x = layers.Conv1D(f, 1, padding=\"same\")(x)\n",
    "            x = layers.add([x, y])\n",
    "        else:\n",
    "            x = y\n",
    "\n",
    "    # Global pooling\n",
    "    if params['pool'] == 'gmp':\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "    else:\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dropout(params['dropout'])(x)\n",
    "\n",
    "    # Extra dense layer\n",
    "    if params['extra_dense']:\n",
    "        x = layers.Dense(params['dense'], activation=params['act'],\n",
    "                         kernel_regularizer=regularizers.l2(params['l2']))(x)\n",
    "        x = layers.Dropout(params['dropout'])(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "# ═══════════════ 1. Data Loading ═══════════════════════════════════\n",
    "print(\"📊 Loading & preprocessing data...\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.loc[\"2018-01-01\":]\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "df = df.dropna(subset=[\"target\"]).dropna()\n",
    "\n",
    "X_raw = df.drop(columns=\"target\").values\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "n_features = X_raw.shape[1]\n",
    "\n",
    "# Time-based train/validation split\n",
    "split_idx = int(len(df) * (1 - VAL_FRAC))\n",
    "scaler = StandardScaler().fit(X_raw[:split_idx])\n",
    "\n",
    "X_train_s = scaler.transform(X_raw[:split_idx]).astype(np.float32)\n",
    "X_val_s = scaler.transform(X_raw[split_idx:]).astype(np.float32)\n",
    "y_train = y_raw[:split_idx]\n",
    "y_val = y_raw[split_idx:]\n",
    "\n",
    "# Create windows\n",
    "win = OPTIMAL_PARAMS['window']\n",
    "X_train, y_train = make_windows(X_train_s, y_train, win)\n",
    "X_val, y_val = make_windows(X_val_s, y_val, win)\n",
    "\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Train windows: {len(X_train):,}\")\n",
    "print(f\"   Val windows: {len(X_val):,}\")\n",
    "print(f\"   Features: {n_features}\")\n",
    "print(f\"   Window size: {win}\")\n",
    "\n",
    "# ═══════════════ 2. Model Building ══════════════════════════════════\n",
    "print(\"\\n🏗️ Building model with optimal parameters...\")\n",
    "print(f\"   Architecture: {OPTIMAL_PARAMS['conv_blocks']} conv blocks, \"\n",
    "      f\"{OPTIMAL_PARAMS['filters']} filters, kernel={OPTIMAL_PARAMS['kernel']}\")\n",
    "print(f\"   Activation: {OPTIMAL_PARAMS['act']}, Pool: {OPTIMAL_PARAMS['pool']}\")\n",
    "print(f\"   Dropout: {OPTIMAL_PARAMS['dropout']}, L2: {OPTIMAL_PARAMS['l2']}\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model(win, n_features)\n",
    "model.compile(optimizer=keras.optimizers.Adam(OPTIMAL_PARAMS['lr']),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# ═══════════════ 3. Training ════════════════════════════════════════\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=EARLY_STOP,\n",
    "                                  restore_best_weights=True,\n",
    "                                  verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                      patience=5,\n",
    "                                      verbose=1,\n",
    "                                      min_lr=1e-7)\n",
    "]\n",
    "\n",
    "print(\"\\n🚀 Training model...\")\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=OPTIMAL_PARAMS['epochs'],\n",
    "                    batch_size=OPTIMAL_PARAMS['batch'],\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=2)\n",
    "\n",
    "# ═══════════════ 4. Evaluation ══════════════════════════════════════\n",
    "print(\"\\n📊 Evaluating model...\")\n",
    "prob_up = model.predict(X_val, verbose=0).ravel()\n",
    "prob_down = 1 - prob_up\n",
    "pred = (prob_up >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_val, pred, zero_division=0)\n",
    "recall = recall_score(y_val, pred, zero_division=0)\n",
    "wf1 = weighted_f1(y_val, pred)\n",
    "accuracy = accuracy_score(y_val, pred)\n",
    "auc = roc_auc_score(y_val, prob_up)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"weighted_f1_a2\": wf1,\n",
    "    \"auc\": auc\n",
    "}\n",
    "\n",
    "print(\"\\n── Validation Metrics ──\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:20s}: {v:6.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, pred, target_names=[\"Down\", \"Up\"]))\n",
    "\n",
    "cm = confusion_matrix(y_val, pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# ═══════════════ 5. Generate Predictions CSV ═══════════════════════\n",
    "print(\"\\n📁 Generating predictions CSV...\")\n",
    "\n",
    "# Get validation timestamps (accounting for window offset)\n",
    "val_timestamps = df.index[split_idx + win:]\n",
    "\n",
    "# Create predictions dataframe\n",
    "predictions_df = pd.DataFrame({\n",
    "    'timestamp': val_timestamps,\n",
    "    'prob_up': prob_up,\n",
    "    'prob_down': prob_down,\n",
    "    'winning_prob': np.maximum(prob_up, prob_down),\n",
    "    'prediction': pred,\n",
    "    'actual': y_val\n",
    "})\n",
    "\n",
    "# Format the CSV exactly as requested\n",
    "predictions_df['timestamp'] = predictions_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Save to CSV with proper column formatting\n",
    "predictions_df.to_csv(PREDICTIONS_OUT, index=False, float_format='%.6f')\n",
    "\n",
    "print(f\"✅ Predictions saved → {PREDICTIONS_OUT}\")\n",
    "print(f\"   Total predictions: {len(predictions_df):,}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n📋 Sample predictions:\")\n",
    "print(predictions_df.head(10).to_string(index=False))\n",
    "\n",
    "# ═══════════════ 6. Save Model & Summary ═══════════════════════════\n",
    "print(\"\\n💾 Saving artifacts...\")\n",
    "\n",
    "# Save model and scaler\n",
    "keras.models.save_model(model, MODEL_OUT)\n",
    "joblib.dump(scaler, SCALER_OUT)\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"optimal_parameters\": OPTIMAL_PARAMS,\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"val_samples\": len(X_val),\n",
    "        \"features\": n_features,\n",
    "        \"window_size\": win,\n",
    "        \"train_period\": f\"{df.index[0]} to {df.index[split_idx-1]}\",\n",
    "        \"val_period\": f\"{df.index[split_idx]} to {df.index[-1]}\"\n",
    "    },\n",
    "    \"metrics\": {k: float(v) for k, v in metrics.items()},\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"class_distribution\": {\n",
    "        \"train_positive_rate\": float(np.mean(y_train)),\n",
    "        \"val_positive_rate\": float(np.mean(y_val)),\n",
    "        \"train_counts\": [int(np.sum(y_train == 0)), int(np.sum(y_train == 1))],\n",
    "        \"val_counts\": [int(np.sum(y_val == 0)), int(np.sum(y_val == 1))]\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"epochs_trained\": len(history.history['loss']),\n",
    "        \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "        \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "        \"best_val_loss\": float(min(history.history['val_loss']))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(SUMMARY_JSON, \"w\") as fp:\n",
    "    json.dump(summary, fp, indent=2)\n",
    "\n",
    "print(f\"✅ Model saved   → {MODEL_OUT}\")\n",
    "print(f\"✅ Scaler saved  → {SCALER_OUT}\")\n",
    "print(f\"✅ Summary saved → {SUMMARY_JSON}\")\n",
    "\n",
    "# ═══════════════ 7. Final Report ═══════════════════════════════════\n",
    "print(f\"\\n🎉 Training Complete!\")\n",
    "print(f\"═\" * 50)\n",
    "print(f\"📈 Final Performance:\")\n",
    "print(f\"   Weighted F1 (α={ALPHA}): {wf1:.3f}\")\n",
    "print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall: {recall:.3f}\")\n",
    "print(f\"   AUC: {auc:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"📁 Files Generated:\")\n",
    "print(f\"   • {MODEL_OUT} - Trained model\")\n",
    "print(f\"   • {SCALER_OUT} - Feature scaler\")\n",
    "print(f\"   • {PREDICTIONS_OUT} - Validation predictions ({len(predictions_df):,} rows)\")\n",
    "print(f\"   • {SUMMARY_JSON} - Training summary\")\n",
    "print(f\"\")\n",
    "print(f\"✨ Ready for production use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c578e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation at threshold 0.5:\n",
      "Precision: 0.525\n",
      "Recall   : 0.713\n",
      "F1 Score : 0.605\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the predictions CSV\n",
    "csv_path = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Final_runs_csv\\cnn_predictions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names are correct and lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Extract actual and predicted values\n",
    "y_true = df['actual']\n",
    "y_pred = df['prediction']  # prediction at threshold 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# Print results\n",
    "print(\"📊 Evaluation at threshold 0.5:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1 Score : {f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
