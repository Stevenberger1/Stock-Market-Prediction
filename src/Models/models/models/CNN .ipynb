{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62dcffa1",
   "metadata": {},
   "source": [
    "# In this notebook we would build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc938698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "──── Best hyper-parameters ────\n",
      "conv_blocks : 2\n",
      "filters_0   : 32\n",
      "kernel_0    : 3\n",
      "dropout     : 0.0\n",
      "lr          : 0.00013468371522638964\n",
      "filters_1   : 32\n",
      "kernel_1    : 3\n",
      "Epoch 1/30\n",
      "1008/1008 - 8s - 7ms/step - loss: 0.6947 - val_loss: 0.9206 - val_weighted_f1: 0.6533 - weighted_f1: 0.4944\n",
      "Epoch 2/30\n",
      "1008/1008 - 6s - 6ms/step - loss: 0.6921 - val_loss: 0.7637 - val_weighted_f1: 0.1995 - weighted_f1: 0.5108\n",
      "Epoch 3/30\n",
      "1008/1008 - 6s - 6ms/step - loss: 0.6911 - val_loss: 0.9273 - val_weighted_f1: 0.1063 - weighted_f1: 0.5117\n",
      "Epoch 4/30\n",
      "1008/1008 - 6s - 6ms/step - loss: 0.6904 - val_loss: 1.0318 - val_weighted_f1: 0.0884 - weighted_f1: 0.5115\n",
      "Epoch 5/30\n",
      "1008/1008 - 6s - 6ms/step - loss: 0.6898 - val_loss: 1.0748 - val_weighted_f1: 0.0868 - weighted_f1: 0.5112\n",
      "Epoch 6/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6893 - val_loss: 1.1608 - val_weighted_f1: 0.0752 - weighted_f1: 0.5111\n",
      "Epoch 7/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6889 - val_loss: 1.2164 - val_weighted_f1: 0.0721 - weighted_f1: 0.5112\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "──── Validation metrics (thr = 0.50) ────\n",
      "Accuracy          :  0.493\n",
      "Class 0 (Down) →  Precision:  0.490  Recall:  0.845  F1:  0.620\n",
      "Class 1 (Up  ) →  Precision:  0.512  Recall:  0.156  F1:  0.239\n",
      "Macro-F1          :  0.429\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1-D CNN for BTC next-hour direction\n",
    "----------------------------------\n",
    "* quick 5-trial hyper-parameter search (val Weighted-F1, precision×2)\n",
    "* full training with the best config\n",
    "* prints per-class metrics\n",
    "\n",
    "pip install pandas numpy scikit-learn tensorflow keras-tuner\n",
    "\"\"\"\n",
    "\n",
    "import os, shutil, numpy as np, pandas as pd, tensorflow as tf, keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ───── Paths & params ───────────────────────────────────────────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\", \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "SEQ_LEN   = 60\n",
    "VAL_FRAC  = 0.20\n",
    "W_PREC    = 2.0           # precision weight in F-score\n",
    "SEARCH_TRIALS = 5         # keep tiny → fast (<5 min on CPU)\n",
    "SEARCH_EPOCHS = 3\n",
    "FULL_EPOCHS   = 30\n",
    "BATCH         = 64\n",
    "\n",
    "# ───── Weighted-F1 metric (precision×2) ─────────────────────────────\n",
    "class WeightedF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, weight=2.0, name=\"weighted_f1\", threshold=0.5, **kw):\n",
    "        super().__init__(name=name, **kw)\n",
    "        self.w  = weight\n",
    "        self.th = threshold\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred >= self.th, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return (1 + self.w) * prec * rec / (self.w * prec + rec + 1e-7)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in (self.tp, self.fp, self.fn):\n",
    "            v.assign(0.)\n",
    "\n",
    "# ───── Data prep (identical to previous code) ───────────────────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "feature_cols = df.columns.drop(\"target\")\n",
    "split_raw = int(len(df) * (1 - VAL_FRAC))\n",
    "train_raw, val_raw = df.iloc[:split_raw], df.iloc[split_raw:]\n",
    "\n",
    "scaler = StandardScaler().fit(train_raw[feature_cols])\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.transform(df[feature_cols]),\n",
    "    columns=feature_cols, index=df.index\n",
    ")\n",
    "labels = df[\"target\"].values.astype(np.float32)\n",
    "\n",
    "def make_sequences(mat, tgt, length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(length, len(mat)):\n",
    "        Xs.append(mat[i-length:i])\n",
    "        ys.append(tgt[i])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "X_all, y_all = make_sequences(df_scaled.values, labels, SEQ_LEN)\n",
    "split_seq = int(len(X_all) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X_all[:split_seq], X_all[split_seq:]\n",
    "y_train, y_val = y_all[:split_seq], y_all[split_seq:]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# ───── Hyper-model builder ─────────────────────────────────────────-\n",
    "def build_model(hp):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Input(shape=(SEQ_LEN, n_features)))\n",
    "\n",
    "    blocks = hp.Int(\"conv_blocks\", 1, 2)\n",
    "    for i in range(blocks):\n",
    "        filters = hp.Int(f\"filters_{i}\", 32, 128, step=32)\n",
    "        ksize   = hp.Choice(f\"kernel_{i}\", [3, 5, 7, 9])\n",
    "        m.add(tf.keras.layers.Conv1D(filters, ksize, padding=\"causal\", activation=\"relu\"))\n",
    "        m.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    m.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    m.add(tf.keras.layers.Dropout(hp.Float(\"dropout\", 0.0, 0.5, step=0.1)))\n",
    "    m.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[WeightedF1(weight=W_PREC)]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# ───── Quick RandomSearch ─────────────────────────────────────────--\n",
    "TMP_DIR = \"tmp_cnn_tune\"\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)      # clean each run\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_weighted_f1\", direction=\"max\"),\n",
    "    max_trials=SEARCH_TRIALS,\n",
    "    executions_per_trial=1,\n",
    "    directory=TMP_DIR, project_name=\"run\", overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=SEARCH_EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\n──── Best hyper-parameters ────\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"{k:<12}: {v}\")\n",
    "\n",
    "# ───── Build final model with best params ─────────────────────────--\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5, restore_best_weights=True, monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=FULL_EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ───── Metrics ─────────────────────────────────────────────────────\n",
    "y_prob = model.predict(X_val, batch_size=BATCH).flatten()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n──── Validation metrics (thr = 0.50) ────\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) →  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) →  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n",
    "\n",
    "# optional: clean tuner directory\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77830dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1008/1008 - 4s - 4ms/step - loss: 0.6935 - val_loss: 0.7591 - val_weighted_f1: 0.3773 - weighted_f1: 0.5113\n",
      "Epoch 2/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6915 - val_loss: 0.7899 - val_weighted_f1: 0.2776 - weighted_f1: 0.5193\n",
      "Epoch 3/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6907 - val_loss: 0.8129 - val_weighted_f1: 0.2425 - weighted_f1: 0.5180\n",
      "Epoch 4/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6901 - val_loss: 0.8438 - val_weighted_f1: 0.2063 - weighted_f1: 0.5172\n",
      "Epoch 5/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6896 - val_loss: 0.8955 - val_weighted_f1: 0.1615 - weighted_f1: 0.5172\n",
      "Epoch 6/30\n",
      "1008/1008 - 3s - 3ms/step - loss: 0.6891 - val_loss: 0.9584 - val_weighted_f1: 0.1347 - weighted_f1: 0.5164\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "──── Validation metrics (thr = 0.50) ────\n",
      "Accuracy          :  0.497\n",
      "Class 0 (Down) →  Precision:  0.490  Recall:  0.666  F1:  0.565\n",
      "Class 1 (Up  ) →  Precision:  0.511  Recall:  0.335  F1:  0.405\n",
      "Macro-F1          :  0.485\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a 1-D CNN for BTC next-hour direction\n",
    "------------------------------------------\n",
    "Uses your selected hyper-parameters (see header).\n",
    "Outputs per-class precision, recall, F1 and overall accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ─────────── File paths & constants ───────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\", \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "SEQ_LEN   = 60          # past 60 hours\n",
    "VAL_FRAC  = 0.20        # last 20 % → validation\n",
    "W_PREC    = 2.0         # precision weight in weighted-F1\n",
    "LR        = 1.3468371522638964e-4\n",
    "MAX_EPOCH = 30\n",
    "BATCH     = 64\n",
    "\n",
    "# ─────────── Weighted-F1 (precision×2) ───────────\n",
    "class WeightedF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, weight=2.0, name=\"weighted_f1\", threshold=0.5, **kw):\n",
    "        super().__init__(name=name, **kw)\n",
    "        self.w  = weight\n",
    "        self.th = threshold\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred >= self.th, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        prec = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        rec  = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return (1 + self.w) * prec * rec / (self.w * prec + rec + 1e-7)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in (self.tp, self.fp, self.fn):\n",
    "            v.assign(0.)\n",
    "\n",
    "# ─────────── Data loading & preprocessing ───────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "\n",
    "# label: next close up (1) / down (0)\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "feature_cols = df.columns.drop(\"target\")\n",
    "split_raw = int(len(df) * (1 - VAL_FRAC))\n",
    "train_raw, val_raw = df.iloc[:split_raw], df.iloc[split_raw:]\n",
    "\n",
    "scaler = StandardScaler().fit(train_raw[feature_cols])\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.transform(df[feature_cols]),\n",
    "    columns=feature_cols, index=df.index\n",
    ")\n",
    "labels = df[\"target\"].values.astype(np.float32)\n",
    "\n",
    "def make_sequences(mat, tgt, length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(length, len(mat)):\n",
    "        Xs.append(mat[i-length:i])\n",
    "        ys.append(tgt[i])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "X_all, y_all = make_sequences(df_scaled.values, labels, SEQ_LEN)\n",
    "split_seq = int(len(X_all) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X_all[:split_seq], X_all[split_seq:]\n",
    "y_train, y_val = y_all[:split_seq], y_all[split_seq:]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# ─────────── Build CNN with fixed hyper-params ───────────\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    tf.keras.layers.Conv1D(32, 3, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(32, 3, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(0.0),      # as specified\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[WeightedF1(weight=W_PREC)]\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5, restore_best_weights=True, monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# ─────────── Train ───────────\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=MAX_EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ─────────── Evaluate ───────────\n",
    "y_prob = model.predict(X_val, batch_size=BATCH).flatten()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n──── Validation metrics (thr = 0.50) ────\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) →  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) →  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
