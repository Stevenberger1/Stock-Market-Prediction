{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec912cc",
   "metadata": {},
   "source": [
    "# In this notebook we would train the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c85cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "──── Best hyper-parameters ────\n",
      "gru_layers  : 2\n",
      "units_0     : 32\n",
      "dropout_0   : 0.4\n",
      "lr          : 0.0027215464940055565\n",
      "units_1     : 64\n",
      "dropout_1   : 0.4\n",
      "Epoch 1/50\n",
      "1008/1008 - 37s - 37ms/step - loss: 0.6944 - val_loss: 0.6941 - val_weighted_f1: 0.3110 - weighted_f1: 0.5504 - learning_rate: 0.0027\n",
      "Epoch 2/50\n",
      "1008/1008 - 34s - 34ms/step - loss: 0.6926 - val_loss: 0.6936 - val_weighted_f1: 0.3656 - weighted_f1: 0.5545 - learning_rate: 0.0027\n",
      "Epoch 3/50\n",
      "1008/1008 - 27s - 27ms/step - loss: 0.6923 - val_loss: 0.6932 - val_weighted_f1: 0.5021 - weighted_f1: 0.5455 - learning_rate: 0.0027\n",
      "Epoch 4/50\n",
      "1008/1008 - 19s - 18ms/step - loss: 0.6923 - val_loss: 0.6925 - val_weighted_f1: 0.5473 - weighted_f1: 0.5507 - learning_rate: 0.0027\n",
      "Epoch 5/50\n",
      "1008/1008 - 33s - 33ms/step - loss: 0.6922 - val_loss: 0.6934 - val_weighted_f1: 0.4332 - weighted_f1: 0.5554 - learning_rate: 0.0027\n",
      "Epoch 6/50\n",
      "1008/1008 - 35s - 34ms/step - loss: 0.6925 - val_loss: 0.6926 - val_weighted_f1: 0.6569 - weighted_f1: 0.5454 - learning_rate: 0.0027\n",
      "Epoch 7/50\n",
      "1008/1008 - 35s - 34ms/step - loss: 0.6923 - val_loss: 0.6923 - val_weighted_f1: 0.5705 - weighted_f1: 0.5403 - learning_rate: 0.0027\n",
      "Epoch 8/50\n",
      "1008/1008 - 35s - 34ms/step - loss: 0.6924 - val_loss: 0.6941 - val_weighted_f1: 0.7362 - weighted_f1: 0.5393 - learning_rate: 0.0027\n",
      "Epoch 9/50\n",
      "1008/1008 - 32s - 32ms/step - loss: 0.6925 - val_loss: 0.6926 - val_weighted_f1: 0.6132 - weighted_f1: 0.5537 - learning_rate: 0.0027\n",
      "Epoch 10/50\n",
      "1008/1008 - 28s - 28ms/step - loss: 0.6925 - val_loss: 0.6932 - val_weighted_f1: 0.7225 - weighted_f1: 0.5436 - learning_rate: 0.0027\n",
      "Epoch 11/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6924 - val_loss: 0.6942 - val_weighted_f1: 0.7277 - weighted_f1: 0.5494 - learning_rate: 0.0027\n",
      "Epoch 12/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6923 - val_loss: 0.6923 - val_weighted_f1: 0.5305 - weighted_f1: 0.5397 - learning_rate: 0.0027\n",
      "Epoch 13/50\n",
      "1008/1008 - 36s - 36ms/step - loss: 0.6924 - val_loss: 0.6928 - val_weighted_f1: 0.6997 - weighted_f1: 0.5573 - learning_rate: 0.0014\n",
      "Epoch 14/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6920 - val_loss: 0.6927 - val_weighted_f1: 0.6993 - weighted_f1: 0.5572 - learning_rate: 0.0014\n",
      "Epoch 15/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6922 - val_loss: 0.6927 - val_weighted_f1: 0.4483 - weighted_f1: 0.5589 - learning_rate: 0.0014\n",
      "Epoch 16/50\n",
      "1008/1008 - 37s - 37ms/step - loss: 0.6923 - val_loss: 0.6924 - val_weighted_f1: 0.5725 - weighted_f1: 0.5513 - learning_rate: 0.0014\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step\n",
      "\n",
      "──── Validation metrics (thr = 0.50) ────\n",
      "Accuracy          :  0.511\n",
      "Class 0 (Down) →  Precision:  0.501  Recall:  0.059  F1:  0.105\n",
      "Class 1 (Up  ) →  Precision:  0.511  Recall:  0.944  F1:  0.663\n",
      "Macro-F1          :  0.384\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "gru_btc_direction.py\n",
    "--------------------\n",
    "High-quality GRU model for predicting whether BTC's next-hour close\n",
    "will go Up (1) or Down (0).\n",
    "\n",
    "Workflow:\n",
    "1. Data prep (drop cols, log1p volume, chronological split, scaling).\n",
    "2. Quick Keras-Tuner RandomSearch (10 trials × 3 epochs).\n",
    "3. Print best hyper-params.\n",
    "4. Re-build best GRU and train with early stopping + LR schedule.\n",
    "5. Print accuracy + per-class precision / recall / F1.\n",
    "\n",
    "Author: ChatGPT (OpenAI o3)\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────── imports ──────────────────────\n",
    "import numpy as np, pandas as pd, tensorflow as tf, keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ─────────────── paths & config ───────────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\",\n",
    "             \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "SEQ_LEN   = 60        # past 60 hours → predict next hour\n",
    "VAL_FRAC  = 0.20      # 80 % train · 20 % val\n",
    "W_PREC    = 2.0       # precision weight in weighted-F1\n",
    "TUNE_TRIALS  = 7     # quick search\n",
    "TUNE_EPOCHS  = 3\n",
    "FINAL_EPOCHS = 25\n",
    "BATCH        = 64\n",
    "SEED         = 42\n",
    "\n",
    "# ─────────── weighted-F1 metric ────────────\n",
    "class WeightedF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, weight=2.0, name=\"weighted_f1\", threshold=0.5, **kw):\n",
    "        super().__init__(name=name, **kw)\n",
    "        self.w  = weight; self.th = threshold\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred >= self.th, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "    def result(self):\n",
    "        p = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        r = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return (1 + self.w) * p * r / (self.w * p + r + 1e-7)\n",
    "    def reset_states(self):\n",
    "        for v in (self.tp, self.fp, self.fn): v.assign(0.)\n",
    "\n",
    "# ─────────── data loading & preprocessing ───────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "X_raw = df.drop(columns=[\"target\"])\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "\n",
    "split_row   = int(len(df) * (1 - VAL_FRAC))\n",
    "X_train_raw = X_raw.iloc[:split_row]\n",
    "X_val_raw   = X_raw.iloc[split_row:]\n",
    "y_train     = y_raw[:split_row]\n",
    "y_val       = y_raw[split_row:]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "# build rolling sequences\n",
    "def to_seq(mat, tgt, length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(length, len(mat)):\n",
    "        Xs.append(mat[i-length:i])\n",
    "        ys.append(tgt[i])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "X_seq, y_seq = to_seq(X_scaled, y_raw, SEQ_LEN)\n",
    "split_seq    = int(len(X_seq) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X_seq[:split_seq], X_seq[split_seq:]\n",
    "y_train, y_val = y_seq[:split_seq], y_seq[split_seq:]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# ─────────── hyper-model builder ───────────\n",
    "def build_gru(hp: kt.HyperParameters):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(SEQ_LEN, n_features)))\n",
    "\n",
    "    # optional 1-2 GRU layers\n",
    "    for i in range(hp.Int(\"gru_layers\", 1, 2)):\n",
    "        units = hp.Int(f\"units_{i}\", 32, 128, step=32)\n",
    "        return_seq = (i < hp.get(\"gru_layers\") - 1)\n",
    "        model.add(tf.keras.layers.GRU(units,\n",
    "                                      activation=\"tanh\",\n",
    "                                      recurrent_activation=\"sigmoid\",\n",
    "                                      dropout=hp.Float(f\"dropout_{i}\", 0.0, 0.4, step=0.1),\n",
    "                                      return_sequences=return_seq))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[WeightedF1(weight=W_PREC)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ─────────── quick RandomSearch ───────────\n",
    "tuner = kt.RandomSearch(\n",
    "    build_gru,\n",
    "    objective=kt.Objective(\"val_weighted_f1\", direction=\"max\"),\n",
    "    max_trials=TUNE_TRIALS,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tmp_gru_tune\", project_name=\"run\", overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=TUNE_EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\n──── Best hyper-parameters ────\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"{k:<12}: {v}\")\n",
    "\n",
    "# ─────────── final GRU build & train ───────────\n",
    "model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor=\"val_weighted_f1\", mode=\"max\"),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, monitor=\"val_weighted_f1\", mode=\"max\")\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ─────────── evaluation ───────────\n",
    "y_prob = model.predict(X_val, batch_size=BATCH).flatten()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n──── Validation metrics (thr = 0.50) ────\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) →  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) →  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a31e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1008/1008 - 21s - 21ms/step - loss: 0.6942 - val_loss: 0.6947 - val_weighted_f1: 0.2462 - weighted_f1: 0.5537 - learning_rate: 0.0027\n",
      "Epoch 2/50\n",
      "1008/1008 - 19s - 19ms/step - loss: 0.6922 - val_loss: 0.6932 - val_weighted_f1: 0.4798 - weighted_f1: 0.5613 - learning_rate: 0.0027\n",
      "Epoch 3/50\n",
      "1008/1008 - 30s - 29ms/step - loss: 0.6923 - val_loss: 0.6933 - val_weighted_f1: 0.3422 - weighted_f1: 0.5678 - learning_rate: 0.0027\n",
      "Epoch 4/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6922 - val_loss: 0.6926 - val_weighted_f1: 0.5837 - weighted_f1: 0.5724 - learning_rate: 0.0027\n",
      "Epoch 5/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6920 - val_loss: 0.6929 - val_weighted_f1: 0.5935 - weighted_f1: 0.5694 - learning_rate: 0.0027\n",
      "Epoch 6/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6924 - val_loss: 0.6929 - val_weighted_f1: 0.4824 - weighted_f1: 0.5637 - learning_rate: 0.0027\n",
      "Epoch 7/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6918 - val_loss: 0.6932 - val_weighted_f1: 0.6431 - weighted_f1: 0.5502 - learning_rate: 0.0027\n",
      "Epoch 8/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6925 - val_loss: 0.6939 - val_weighted_f1: 0.6850 - weighted_f1: 0.5567 - learning_rate: 0.0027\n",
      "Epoch 9/50\n",
      "1008/1008 - 35s - 35ms/step - loss: 0.6919 - val_loss: 0.6937 - val_weighted_f1: 0.4356 - weighted_f1: 0.5541 - learning_rate: 0.0027\n",
      "Epoch 10/50\n",
      "1008/1008 - 37s - 36ms/step - loss: 0.6928 - val_loss: 0.6972 - val_weighted_f1: 0.0017 - weighted_f1: 0.5665 - learning_rate: 0.0027\n",
      "Epoch 11/50\n",
      "1008/1008 - 30s - 30ms/step - loss: 0.6939 - val_loss: 0.6934 - val_weighted_f1: 0.5667 - weighted_f1: 0.5513 - learning_rate: 0.0027\n",
      "Epoch 12/50\n",
      "1008/1008 - 19s - 19ms/step - loss: 0.6938 - val_loss: 0.6937 - val_weighted_f1: 0.3785 - weighted_f1: 0.5574 - learning_rate: 0.0027\n",
      "Epoch 13/50\n",
      "1008/1008 - 19s - 19ms/step - loss: 0.6929 - val_loss: 0.6927 - val_weighted_f1: 0.6169 - weighted_f1: 0.5658 - learning_rate: 0.0014\n",
      "Epoch 14/50\n",
      "1008/1008 - 19s - 19ms/step - loss: 0.6930 - val_loss: 0.6931 - val_weighted_f1: 0.3724 - weighted_f1: 0.5912 - learning_rate: 0.0014\n",
      "Epoch 15/50\n",
      "1008/1008 - 19s - 18ms/step - loss: 0.6928 - val_loss: 0.6930 - val_weighted_f1: 0.5691 - weighted_f1: 0.5769 - learning_rate: 0.0014\n",
      "Epoch 16/50\n",
      "1008/1008 - 20s - 19ms/step - loss: 0.6924 - val_loss: 0.6930 - val_weighted_f1: 0.4426 - weighted_f1: 0.5705 - learning_rate: 0.0014\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\n",
      "──── Validation metrics (thr = 0.50) ────\n",
      "Accuracy          :  0.510\n",
      "Class 0 (Down) →  Precision:  0.498  Recall:  0.260  F1:  0.341\n",
      "Class 1 (Up  ) →  Precision:  0.513  Recall:  0.750  F1:  0.609\n",
      "Macro-F1          :  0.475\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (16119) does not match length of index (16131)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# ───────── optional: save predictions ─────────\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAVE_PROB:\n\u001b[1;32m--> 147\u001b[0m     pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprob_up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# original timestamps\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     pred_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgru_val_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved → gru_val_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:630\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    627\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    629\u001b[0m     val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 630\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m     refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    633\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (16119) does not match length of index (16131)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "final_gru_train.py\n",
    "------------------\n",
    "GRU model for BTC next-hour direction with fixed hyper-parameters:\n",
    "\n",
    "    # ── layer 0 ──\n",
    "    units       = 32\n",
    "    dropout     = 0.40\n",
    "    return_seq  = True\n",
    "    # ── layer 1 ──\n",
    "    units       = 64\n",
    "    dropout     = 0.40\n",
    "    return_seq  = False\n",
    "    # ── optimiser ──\n",
    "    learning rate = 0.0027215464940055565\n",
    "\n",
    "Outputs per-class metrics and (optionally) prediction table.\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ───────── configuration ─────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\", \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "SEQ_LEN   = 60           # timesteps per sample\n",
    "VAL_FRAC  = 0.20         # 80 % train · 20 % validation\n",
    "PREC_W    = 2.0          # β=2  (precision has ×2 weight in F-score)\n",
    "LR        = 0.0027215464940055565\n",
    "EPOCHS    = 25\n",
    "BATCH     = 64\n",
    "SEED      = 42\n",
    "SAVE_PROB = True         # set False if you don’t want CSV\n",
    "\n",
    "# ───────── custom weighted-F1 metric ─────────\n",
    "class WeightedF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, beta=2.0, name=\"weighted_f1\", threshold=0.5, **kw):\n",
    "        super().__init__(name=name, **kw)\n",
    "        self.b  = beta; self.th = threshold\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred >= self.th, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        p = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        r = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return (1 + self.b**2) * p * r / (self.b**2 * p + r + 1e-7)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in (self.tp, self.fp, self.fn):\n",
    "            v.assign(0.)\n",
    "\n",
    "# ───────── data loading & preprocessing ─────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "X_raw = df.drop(columns=[\"target\"])\n",
    "y_raw = df[\"target\"].astype(int).values\n",
    "\n",
    "split_raw = int(len(df) * (1 - VAL_FRAC))\n",
    "X_train_raw, X_val_raw = X_raw.iloc[:split_raw], X_raw.iloc[split_raw:]\n",
    "y_train, y_val         = y_raw[:split_raw], y_raw[split_raw:]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "def to_seq(matrix, labels, length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(length, len(matrix)):\n",
    "        xs.append(matrix[i-length:i])\n",
    "        ys.append(labels[i])\n",
    "    return np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "X_seq, y_seq = to_seq(X_scaled, y_raw, SEQ_LEN)\n",
    "split_seq    = int(len(X_seq) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X_seq[:split_seq], X_seq[split_seq:]\n",
    "y_train, y_val = y_seq[:split_seq], y_seq[split_seq:]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# ───────── build GRU model ─────────\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    tf.keras.layers.GRU(32, dropout=0.4, return_sequences=True,\n",
    "                        activation=\"tanh\", recurrent_activation=\"sigmoid\"),\n",
    "    tf.keras.layers.GRU(64, dropout=0.4, return_sequences=False,\n",
    "                        activation=\"tanh\", recurrent_activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[WeightedF1(beta=PREC_W)]   # β=2 → precision ×2\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_weighted_f1\", mode=\"max\",\n",
    "        patience=8, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_weighted_f1\", mode=\"max\",\n",
    "        factor=0.5, patience=4)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ───────── evaluation ─────────\n",
    "y_prob = model.predict(X_val, batch_size=BATCH).flatten()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n──── Validation metrics (thr = 0.50) ────\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) →  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) →  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n",
    "\n",
    "# ───────── optional: save predictions ─────────\n",
    "if SAVE_PROB:\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"prob_up\": y_prob,\n",
    "        \"pred\": y_pred\n",
    "    }, index=X_val_raw.index)              # original timestamps\n",
    "    pred_df.to_csv(\"gru_val_predictions.csv\")\n",
    "    print(\"Predictions saved → gru_val_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
