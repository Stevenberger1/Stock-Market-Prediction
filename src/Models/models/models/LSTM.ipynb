{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5e0dd8",
   "metadata": {},
   "source": [
    "# In this notebook we will train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ab63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_seq_4h = [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'bollinger_upper', 'bollinger_lower', 'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "    'EMA_21', 'SMA_20',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'rsi_oversold', 'rsi_overbought', 'stoch_overbought',\n",
    "    'stoch_oversold', 'cci_overbought', 'cci_oversold', 'near_upper_band',\n",
    "    'near_lower_band', 'overbought_reversal', 'oversold_reversal',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'macd_positive', 'momentum_alignment', 'macd_rising', 'obv_rising_24h',\n",
    "    'trending_market', 'trend_alignment',\n",
    "    'support_level', 'resistance_level', 'volatility_regime',\n",
    "    'close_daily', 'rsi_daily'\n",
    "]\n",
    "\n",
    "‚úÖ DROP_COLS['4H']['LSTM']\n",
    "‚úÖ DROP_COLS['4H']['GRU']\n",
    "‚úÖ DROP_COLS['4H']['CNN']\n",
    "‚úÖ DROP_COLS['4H']['CNN_LSTM']\n",
    "‚úÖ DROP_COLS['4H']['TCN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e941d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop columns catnboost\n",
    "drop_catboost_4h = [\n",
    "    'open', 'high', 'low',\n",
    "    'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'EMA_21', 'SMA_20', 'bollinger_upper', 'bollinger_lower',\n",
    "    'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3', \n",
    "    'bearish_scenario_6',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'near_upper_band', 'near_lower_band',\n",
    "    'rsi_oversold', 'rsi_overbought', 'stoch_overbought', 'stoch_oversold',\n",
    "    'cci_overbought', 'cci_oversold', 'overbought_reversal', 'oversold_reversal',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'trending_market', 'trend_alignment', 'momentum_alignment',\n",
    "    'ema7_above_ema21', 'obv_rising_24h', 'above_sma20', 'above_sma50',\n",
    "    'macd_positive', 'macd_rising'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns of xgboost + LightGBM are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff953f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up GPU...\n",
      "‚ö†Ô∏è  No GPU detected - running on CPU (will be slower)\n",
      "üß™ Testing basic functionality...\n",
      "‚úÖ Model building test passed\n",
      "‚úÖ Model training test passed\n",
      "‚úÖ Model prediction test passed\n",
      "üìä Loading data...\n",
      "‚úÖ Data loaded: 15855 samples, 43 features\n",
      "\n",
      "üöÄ Starting Successive Halving Search\n",
      "Initial candidates: 24, Survivors: 6\n",
      "\n",
      "üìä Stage 1: Quick evaluation (8 epochs)\n",
      "\n",
      "‚ö° Config 1/24: {'sequence_length': 24, 'units': 96, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.002892684180163678, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.0001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   F1: 0.6523\n",
      "\n",
      "‚ö° Config 2/24: {'sequence_length': 24, 'units': 128, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.3, 'learning_rate': 0.00015608202800330114, 'batch_size': 32, 'optimizer': 'adam', 'scaler': 'standard', 'l1': 0.0, 'l2': 0.001, 'dense_units': 48, 'activation': 'relu', 'threshold': 0.6}\n",
      "   F1: 0.1952\n",
      "\n",
      "‚ö° Config 3/24: {'sequence_length': 32, 'units': 128, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.0029809895254204535, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.0001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.5}\n",
      "   F1: 0.4828\n",
      "\n",
      "‚ö° Config 4/24: {'sequence_length': 20, 'units': 96, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.0016480592659116344, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.0, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.4}\n",
      "   F1: 0.6519\n",
      "\n",
      "‚ö° Config 5/24: {'sequence_length': 24, 'units': 96, 'layers': 1, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.00014972195645893965, 'batch_size': 32, 'optimizer': 'adam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.001, 'dense_units': 64, 'activation': 'tanh', 'threshold': 0.5}\n",
      "   F1: 0.5858\n",
      "\n",
      "‚ö° Config 6/24: {'sequence_length': 24, 'units': 96, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.4, 'learning_rate': 0.00021063397453490902, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.0, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0444\n",
      "\n",
      "‚ö° Config 7/24: {'sequence_length': 16, 'units': 96, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.0005770940371347331, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.0001, 'dense_units': 64, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0173\n",
      "\n",
      "‚ö° Config 8/24: {'sequence_length': 32, 'units': 96, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.00011111882744186237, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.0001, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.5}\n",
      "   F1: 0.4573\n",
      "\n",
      "‚ö° Config 9/24: {'sequence_length': 16, 'units': 96, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.3, 'learning_rate': 0.00042408741269246486, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.0001, 'dense_units': 48, 'activation': 'relu', 'threshold': 0.6}\n",
      "   F1: 0.0108\n",
      "\n",
      "‚ö° Config 10/24: {'sequence_length': 24, 'units': 64, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.0005433364546383785, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'standard', 'l1': 0.0, 'l2': 0.001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.4}\n",
      "   F1: 0.6523\n",
      "\n",
      "‚ö° Config 11/24: {'sequence_length': 24, 'units': 96, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.0001626782037987215, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.001, 'l2': 0.0001, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0444\n",
      "\n",
      "‚ö° Config 12/24: {'sequence_length': 24, 'units': 128, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.00024186048041139295, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.001, 'l2': 0.0001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.6}\n",
      "   F1: 0.0426\n",
      "\n",
      "‚ö° Config 13/24: {'sequence_length': 16, 'units': 128, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.00014101315472650686, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.001, 'l2': 0.001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0441\n",
      "\n",
      "‚ö° Config 14/24: {'sequence_length': 32, 'units': 64, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.001769260055533629, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0364\n",
      "\n",
      "‚ö° Config 15/24: {'sequence_length': 20, 'units': 64, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.0006733789018586589, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.0001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0174\n",
      "\n",
      "‚ö° Config 16/24: {'sequence_length': 20, 'units': 96, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.3, 'learning_rate': 0.003807811768058355, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   F1: 0.6519\n",
      "\n",
      "‚ö° Config 17/24: {'sequence_length': 16, 'units': 96, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.00024986910173897695, 'batch_size': 32, 'optimizer': 'adam', 'scaler': 'standard', 'l1': 0.0, 'l2': 0.001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.1085\n",
      "\n",
      "‚ö° Config 18/24: {'sequence_length': 24, 'units': 64, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.4, 'learning_rate': 0.0001988270898176423, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   F1: 0.6523\n",
      "\n",
      "‚ö° Config 19/24: {'sequence_length': 20, 'units': 96, 'layers': 1, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.00018462478745811988, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'standard', 'l1': 0.0, 'l2': 0.0, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.4}\n",
      "   F1: 0.6429\n",
      "\n",
      "‚ö° Config 20/24: {'sequence_length': 16, 'units': 96, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.004469556091012348, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.001, 'l2': 0.0001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   F1: 0.6519\n",
      "\n",
      "‚ö° Config 21/24: {'sequence_length': 32, 'units': 128, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.00016269977779763856, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.0, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.6}\n",
      "   F1: 0.0000\n",
      "\n",
      "‚ö° Config 22/24: {'sequence_length': 16, 'units': 64, 'layers': 1, 'layer_type': 'GRU', 'dropout': 0.4, 'learning_rate': 0.00010573455710277193, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0663\n",
      "\n",
      "‚ö° Config 23/24: {'sequence_length': 32, 'units': 128, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.00023463410334547438, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.0001, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.5}\n",
      "   F1: 0.4238\n",
      "\n",
      "‚ö° Config 24/24: {'sequence_length': 20, 'units': 128, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.00038984283348348915, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0, 'l2': 0.0001, 'dense_units': 64, 'activation': 'tanh', 'threshold': 0.6}\n",
      "   F1: 0.0854\n",
      "\n",
      "üî• Top 6 configs from Stage 1:\n",
      "   Rank 1: F1 = 0.6523\n",
      "   Rank 2: F1 = 0.6523\n",
      "   Rank 3: F1 = 0.6523\n",
      "   Rank 4: F1 = 0.6519\n",
      "   Rank 5: F1 = 0.6519\n",
      "   Rank 6: F1 = 0.6519\n",
      "\n",
      "üöÄ Stage 2: Full training (30 epochs)\n",
      "\n",
      "üî• Training config 1/6: {'sequence_length': 24, 'units': 96, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.4, 'learning_rate': 0.002892684180163678, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.0001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   Val F1: 0.6523, Test F1: 0.6861\n",
      "\n",
      "üî• Training config 2/6: {'sequence_length': 24, 'units': 64, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.0005433364546383785, 'batch_size': 64, 'optimizer': 'adam', 'scaler': 'standard', 'l1': 0.0, 'l2': 0.001, 'dense_units': 32, 'activation': 'tanh', 'threshold': 0.4}\n",
      "   Val F1: 0.6523, Test F1: 0.6861\n",
      "\n",
      "üî• Training config 3/6: {'sequence_length': 24, 'units': 64, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.4, 'learning_rate': 0.0001988270898176423, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   Val F1: 0.6523, Test F1: 0.6861\n",
      "\n",
      "üî• Training config 4/6: {'sequence_length': 16, 'units': 96, 'layers': 2, 'layer_type': 'GRU', 'dropout': 0.2, 'learning_rate': 0.004469556091012348, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.001, 'l2': 0.0001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   Val F1: 0.6519, Test F1: 0.6866\n",
      "\n",
      "üî• Training config 5/6: {'sequence_length': 20, 'units': 96, 'layers': 2, 'layer_type': 'LSTM', 'dropout': 0.2, 'learning_rate': 0.0016480592659116344, 'batch_size': 64, 'optimizer': 'nadam', 'scaler': 'standard', 'l1': 0.0001, 'l2': 0.0, 'dense_units': 48, 'activation': 'tanh', 'threshold': 0.4}\n",
      "   Val F1: 0.6519, Test F1: 0.6861\n",
      "\n",
      "üî• Training config 6/6: {'sequence_length': 20, 'units': 96, 'layers': 1, 'layer_type': 'LSTM', 'dropout': 0.3, 'learning_rate': 0.003807811768058355, 'batch_size': 32, 'optimizer': 'nadam', 'scaler': 'mixed', 'l1': 0.0001, 'l2': 0.001, 'dense_units': 32, 'activation': 'relu', 'threshold': 0.4}\n",
      "   Val F1: 0.6519, Test F1: 0.6861\n",
      "\n",
      "================================================================================\n",
      "üèÜ HYPERPARAMETER SEARCH COMPLETE\n",
      "================================================================================\n",
      "Best Validation F1: 0.6523\n",
      "\n",
      "üìã Best Configuration:\n",
      "   sequence_length   : 24\n",
      "   units             : 96\n",
      "   layers            : 2\n",
      "   layer_type        : LSTM\n",
      "   dropout           : 0.4\n",
      "   learning_rate     : 0.002892684180163678\n",
      "   batch_size        : 64\n",
      "   optimizer         : nadam\n",
      "   scaler            : standard\n",
      "   l1                : 0.0001\n",
      "   l2                : 0.0001\n",
      "   dense_units       : 32\n",
      "   activation        : relu\n",
      "   threshold         : 0.4\n",
      "\n",
      "üìä Test Set Performance:\n",
      "   accuracy    : 0.5222\n",
      "   precision   : 0.5222\n",
      "   recall      : 1.0000\n",
      "   f1          : 0.6861\n",
      "   auc         : 0.4924\n",
      "\n",
      "‚è±Ô∏è  Total Runtime: 6.8 minutes\n",
      "üîß GPU Used: No\n",
      "üíæ Results saved to: results\\optimized_lstm_tuning_results.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  OPTIMIZED CUDA-ENABLED LSTM/GRU SUCCESSIVE-HALVING TUNER\n",
    "# =============================================================\n",
    "import warnings, json, gc, time, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Silence warnings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ TensorFlow imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Sklearn imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Reproducibility ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# =============================================================\n",
    "# IMPROVED GPU SETUP\n",
    "# =============================================================\n",
    "def setup_gpu():\n",
    "    \"\"\"Properly configure GPU with better error handling\"\"\"\n",
    "    print(\"üîß Setting up GPU...\")\n",
    "    \n",
    "    # List available devices\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if not gpus:\n",
    "        print(\"‚ö†Ô∏è  No GPU detected - running on CPU (will be slower)\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Configure GPU memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Set mixed precision policy (but make it optional)\n",
    "        try:\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(f\"‚úÖ {len(gpus)} GPU(s) configured with mixed_float16\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Mixed precision failed ({e}), using float32\")\n",
    "            mixed_precision.set_global_policy('float32')\n",
    "        \n",
    "        # Test GPU availability\n",
    "        with tf.device('/GPU:0'):\n",
    "            test_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "            result = tf.reduce_sum(test_tensor)\n",
    "        \n",
    "        print(f\"‚úÖ GPU test successful: {result.numpy()}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GPU setup failed: {e}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        return False\n",
    "\n",
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'csv_file': Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\"),\n",
    "        'time_col': 'timestamp',\n",
    "        'target_col': 'target',\n",
    "        'start_date': '2018-01-01',\n",
    "        'test_frac': 0.20,\n",
    "        'val_frac': 0.15,\n",
    "        'drop_cols': [\n",
    "    'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "    'bollinger_upper', 'bollinger_lower', 'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "    'EMA_21', 'SMA_20',\n",
    "    'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "    'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "    'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "    'bearish_scenario_4', 'bearish_scenario_6',\n",
    "    'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "    'vol_spike_1_5x', 'rsi_oversold', 'rsi_overbought', 'stoch_overbought',\n",
    "    'stoch_oversold', 'cci_overbought', 'cci_oversold', 'near_upper_band',\n",
    "    'near_lower_band', 'overbought_reversal', 'oversold_reversal',\n",
    "    'above_sma20', 'above_sma50', 'ema7_above_ema21',\n",
    "    'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "    'macd_positive', 'momentum_alignment', 'macd_rising', 'obv_rising_24h',\n",
    "    'trending_market', 'trend_alignment',\n",
    "    'support_level', 'resistance_level', 'volatility_regime',\n",
    "    'close_daily', 'rsi_daily'\n",
    "],\n",
    "        'bounded_cols': ['rsi', 'stoch_%K', 'bb_position', 'williams_%R']\n",
    "    }\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# OPTIMIZED DATA PROCESSING\n",
    "# =============================================================\n",
    "class OptimizedDataProcessor:\n",
    "    \"\"\"Streamlined data processing for better performance\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config['data']\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load and prepare data efficiently\"\"\"\n",
    "        print(\"üìä Loading data...\")\n",
    "        \n",
    "        if not self.config['csv_file'].exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {self.config['csv_file']}\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(self.config['csv_file'], parse_dates=[self.config['time_col']])\n",
    "        df = df.set_index(self.config['time_col']).sort_index()\n",
    "        df = df.loc[self.config['start_date']:]\n",
    "        \n",
    "        # Prepare features and target\n",
    "        cols_to_drop = list(set(self.config['drop_cols']) & set(df.columns)) + [self.config['target_col']]\n",
    "        X = df.drop(columns=cols_to_drop)\n",
    "        y = df[self.config['target_col']]\n",
    "        \n",
    "        # Handle missing values\n",
    "        if X.isna().any().any():\n",
    "            X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        return X, y\n",
    "    \n",
    "    def create_sequences(self, X, y, seq_len):\n",
    "        \"\"\"Create sequences efficiently using numpy operations\"\"\"\n",
    "        n_samples = len(X) - seq_len + 1\n",
    "        \n",
    "        # Pre-allocate arrays\n",
    "        X_seq = np.empty((n_samples, seq_len, X.shape[1]), dtype=np.float32)\n",
    "        y_seq = np.empty(n_samples, dtype=np.float32)\n",
    "        \n",
    "        # Use array views for efficiency\n",
    "        X_values = X.values.astype(np.float32)\n",
    "        y_values = y.values.astype(np.float32)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            X_seq[i] = X_values[i:i+seq_len]\n",
    "            y_seq[i] = y_values[i+seq_len-1]\n",
    "            \n",
    "        return X_seq, y_seq\n",
    "    \n",
    "    def split_and_scale_data(self, X, y, seq_len, scaler_type='mixed'):\n",
    "        \"\"\"Split and scale data efficiently\"\"\"\n",
    "        # Calculate split indices\n",
    "        n_samples = len(X)\n",
    "        test_idx = int(n_samples * (1 - self.config['test_frac']))\n",
    "        val_idx = int(test_idx * (1 - self.config['val_frac']))\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test = X.iloc[:val_idx], X.iloc[val_idx:test_idx], X.iloc[test_idx:]\n",
    "        y_train, y_val, y_test = y.iloc[:val_idx], y.iloc[val_idx:test_idx], y.iloc[test_idx:]\n",
    "        \n",
    "        # Scale features\n",
    "        if scaler_type == 'mixed':\n",
    "            # Use MinMax for bounded features, Standard for others\n",
    "            bounded_mask = X_train.columns.isin(self.config['bounded_cols'])\n",
    "            \n",
    "            # Scale bounded features\n",
    "            self.scalers['minmax'] = MinMaxScaler()\n",
    "            X_train_bounded = self.scalers['minmax'].fit_transform(X_train.loc[:, bounded_mask])\n",
    "            X_val_bounded = self.scalers['minmax'].transform(X_val.loc[:, bounded_mask])\n",
    "            X_test_bounded = self.scalers['minmax'].transform(X_test.loc[:, bounded_mask])\n",
    "            \n",
    "            # Scale unbounded features\n",
    "            self.scalers['standard'] = StandardScaler()\n",
    "            X_train_unbounded = self.scalers['standard'].fit_transform(X_train.loc[:, ~bounded_mask])\n",
    "            X_val_unbounded = self.scalers['standard'].transform(X_val.loc[:, ~bounded_mask])\n",
    "            X_test_unbounded = self.scalers['standard'].transform(X_test.loc[:, ~bounded_mask])\n",
    "            \n",
    "            # Combine scaled features\n",
    "            X_train_scaled = np.column_stack([X_train_bounded, X_train_unbounded]).astype(np.float32)\n",
    "            X_val_scaled = np.column_stack([X_val_bounded, X_val_unbounded]).astype(np.float32)\n",
    "            X_test_scaled = np.column_stack([X_test_bounded, X_test_unbounded]).astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            # Use single scaler\n",
    "            scaler_class = StandardScaler if scaler_type == 'standard' else MinMaxScaler\n",
    "            self.scalers[scaler_type] = scaler_class()\n",
    "            \n",
    "            X_train_scaled = self.scalers[scaler_type].fit_transform(X_train).astype(np.float32)\n",
    "            X_val_scaled = self.scalers[scaler_type].transform(X_val).astype(np.float32)\n",
    "            X_test_scaled = self.scalers[scaler_type].transform(X_test).astype(np.float32)\n",
    "        \n",
    "        # Convert to DataFrames for sequence creation\n",
    "        columns = X_train.columns\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=columns)\n",
    "        X_val_df = pd.DataFrame(X_val_scaled, index=X_val.index, columns=columns)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=columns)\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_seq, y_train_seq = self.create_sequences(X_train_df, y_train, seq_len)\n",
    "        X_val_seq, y_val_seq = self.create_sequences(X_val_df, y_val, seq_len)\n",
    "        X_test_seq, y_test_seq = self.create_sequences(X_test_df, y_test, seq_len)\n",
    "        \n",
    "        return (X_train_seq, y_train_seq), (X_val_seq, y_val_seq), (X_test_seq, y_test_seq)\n",
    "\n",
    "# =============================================================\n",
    "# OPTIMIZED MODEL BUILDING\n",
    "# =============================================================\n",
    "class ModelBuilder:\n",
    "    \"\"\"Optimized model building with better GPU utilization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_model(input_shape, config):\n",
    "        \"\"\"Build RNN model with optimizations\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Choose RNN layer\n",
    "        rnn_layer = LSTM if config['layer_type'] == 'LSTM' else GRU\n",
    "        \n",
    "        # Ensure all numeric values are proper Python types\n",
    "        units = int(config['units'])\n",
    "        layers = int(config['layers'])\n",
    "        dropout = float(config['dropout'])\n",
    "        dense_units = int(config['dense_units'])\n",
    "        l1_val = float(config.get('l1', 0.0))\n",
    "        l2_val = float(config.get('l2', 0.0))\n",
    "        learning_rate = float(config['learning_rate'])\n",
    "        \n",
    "        # Add RNN layers\n",
    "        for i in range(layers):\n",
    "            return_sequences = (i < layers - 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                model.add(rnn_layer(\n",
    "                    units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    input_shape=input_shape,\n",
    "                    kernel_regularizer=l1_l2(l1_val, l2_val)\n",
    "                ))\n",
    "            else:\n",
    "                model.add(rnn_layer(\n",
    "                    units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    kernel_regularizer=l1_l2(l1_val, l2_val)\n",
    "                ))\n",
    "        \n",
    "        # Add dense layers\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(dense_units, activation=config.get('activation', 'relu')))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # Output layer with explicit dtype for mixed precision\n",
    "        model.add(Dense(1, activation='sigmoid', dtype='float32'))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer_map = {'adam': Adam, 'nadam': Nadam}\n",
    "        optimizer = optimizer_map[config['optimizer']](learning_rate=learning_rate)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "# =============================================================\n",
    "# CALLBACKS AND METRICS\n",
    "# =============================================================\n",
    "class F1EarlyStopping(Callback):\n",
    "    \"\"\"Custom callback for F1-based early stopping\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data, threshold=0.5, patience=5, min_delta=0.001):\n",
    "        super().__init__()\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        y_pred_prob = self.model.predict(self.X_val, verbose=0).flatten()\n",
    "        y_pred = (y_pred_prob >= self.threshold).astype(int)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        current_f1 = f1_score(self.y_val, y_pred, zero_division=0)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if current_f1 > self.best_f1 + self.min_delta:\n",
    "            self.best_f1 = current_f1\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        if self.wait >= self.patience:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def calculate_metrics(model, X, y, threshold=0.5):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    y_pred_prob = model.predict(X, verbose=0).flatten()\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y, y_pred_prob)\n",
    "    except ValueError:\n",
    "        auc = 0.5  # Default for single-class cases\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, zero_division=0),\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# =============================================================\n",
    "# OPTIMIZED HYPERPARAMETER SEARCH\n",
    "# =============================================================\n",
    "class SuccessiveHalvingTuner:\n",
    "    \"\"\"Successive halving hyperparameter tuner optimized for GPU\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space, data_processor):\n",
    "        self.search_space = search_space\n",
    "        self.data_processor = data_processor\n",
    "        self.results = []\n",
    "        \n",
    "    def sample_config(self):\n",
    "        \"\"\"Sample a random configuration\"\"\"\n",
    "        config = {}\n",
    "        for param, values in self.search_space.items():\n",
    "            if isinstance(values, (list, tuple)):\n",
    "                sampled_value = np.random.choice(values)\n",
    "                # Convert numpy types to native Python types\n",
    "                if hasattr(sampled_value, 'item'):\n",
    "                    config[param] = sampled_value.item()\n",
    "                else:\n",
    "                    config[param] = sampled_value\n",
    "            elif isinstance(values, dict):\n",
    "                if 'min' in values and 'max' in values:\n",
    "                    if values.get('log', False):\n",
    "                        config[param] = np.random.uniform(\n",
    "                            np.log10(values['min']), \n",
    "                            np.log10(values['max'])\n",
    "                        )\n",
    "                        config[param] = float(10 ** config[param])\n",
    "                    else:\n",
    "                        config[param] = float(np.random.uniform(values['min'], values['max']))\n",
    "        return config\n",
    "    \n",
    "    def run_search(self, X, y, n_initial=24, n_survivors=6, quick_epochs=8, full_epochs=30):\n",
    "        \"\"\"Run successive halving search\"\"\"\n",
    "        print(f\"\\nüöÄ Starting Successive Halving Search\")\n",
    "        print(f\"Initial candidates: {n_initial}, Survivors: {n_survivors}\")\n",
    "        \n",
    "        # Stage 1: Quick evaluation\n",
    "        print(f\"\\nüìä Stage 1: Quick evaluation ({quick_epochs} epochs)\")\n",
    "        stage1_results = []\n",
    "        \n",
    "        for i in range(n_initial):\n",
    "            config = self.sample_config()\n",
    "            print(f\"\\n‚ö° Config {i+1}/{n_initial}: {config}\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                train_data, val_data, test_data = self.data_processor.split_and_scale_data(\n",
    "                    X, y, int(config['sequence_length']), config.get('scaler', 'mixed')\n",
    "                )\n",
    "                \n",
    "                X_train, y_train = train_data\n",
    "                X_val, y_val = val_data\n",
    "                \n",
    "                # Build and train model\n",
    "                model = ModelBuilder.build_model((X_train.shape[1], X_train.shape[2]), config)\n",
    "                \n",
    "                # Create callbacks\n",
    "                callbacks = [\n",
    "                    F1EarlyStopping((X_val, y_val), float(config.get('threshold', 0.5)), patience=3),\n",
    "                    EarlyStopping(patience=3, restore_best_weights=True, verbose=0)\n",
    "                ]\n",
    "                \n",
    "                # Train model\n",
    "                with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=quick_epochs,\n",
    "                        batch_size=int(config.get('batch_size', 32)),\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks\n",
    "                    )\n",
    "                \n",
    "                # Calculate validation F1\n",
    "                metrics = calculate_metrics(model, X_val, y_val, float(config.get('threshold', 0.5)))\n",
    "                f1_score = metrics['f1']\n",
    "                \n",
    "                stage1_results.append((f1_score, config, metrics))\n",
    "                print(f\"   F1: {f1_score:.4f}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "                stage1_results.append((0.0, config, {}))\n",
    "        \n",
    "        # Select top performers\n",
    "        stage1_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_configs = [result[1] for result in stage1_results[:n_survivors]]\n",
    "        top_f1_scores = [result[0] for result in stage1_results[:n_survivors]]\n",
    "        \n",
    "        print(f\"\\nüî• Top {n_survivors} configs from Stage 1:\")\n",
    "        for i, f1 in enumerate(top_f1_scores):\n",
    "            print(f\"   Rank {i+1}: F1 = {f1:.4f}\")\n",
    "        \n",
    "        # Stage 2: Full training\n",
    "        print(f\"\\nüöÄ Stage 2: Full training ({full_epochs} epochs)\")\n",
    "        best_f1 = 0\n",
    "        best_config = None\n",
    "        best_test_metrics = None\n",
    "        \n",
    "        for i, config in enumerate(top_configs):\n",
    "            print(f\"\\nüî• Training config {i+1}/{n_survivors}: {config}\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                train_data, val_data, test_data = self.data_processor.split_and_scale_data(\n",
    "                    X, y, int(config['sequence_length']), config.get('scaler', 'mixed')\n",
    "                )\n",
    "                \n",
    "                X_train, y_train = train_data\n",
    "                X_val, y_val = val_data\n",
    "                X_test, y_test = test_data\n",
    "                \n",
    "                # Build and train model\n",
    "                model = ModelBuilder.build_model((X_train.shape[1], X_train.shape[2]), config)\n",
    "                \n",
    "                # Create callbacks\n",
    "                callbacks = [\n",
    "                    F1EarlyStopping((X_val, y_val), float(config.get('threshold', 0.5)), patience=5),\n",
    "                    EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "                ]\n",
    "                \n",
    "                # Train model\n",
    "                with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=full_epochs,\n",
    "                        batch_size=int(config.get('batch_size', 32)),\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks\n",
    "                    )\n",
    "                \n",
    "                # Calculate validation and test metrics\n",
    "                val_metrics = calculate_metrics(model, X_val, y_val, float(config.get('threshold', 0.5)))\n",
    "                test_metrics = calculate_metrics(model, X_test, y_test, float(config.get('threshold', 0.5)))\n",
    "                \n",
    "                print(f\"   Val F1: {val_metrics['f1']:.4f}, Test F1: {test_metrics['f1']:.4f}\")\n",
    "                \n",
    "                # Update best if improved\n",
    "                if val_metrics['f1'] > best_f1:\n",
    "                    best_f1 = val_metrics['f1']\n",
    "                    best_config = config\n",
    "                    best_test_metrics = test_metrics\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "        \n",
    "        return best_config, best_f1, best_test_metrics\n",
    "\n",
    "# =============================================================\n",
    "# SEARCH SPACE DEFINITION\n",
    "# =============================================================\n",
    "SEARCH_SPACE = {\n",
    "    'sequence_length': [16, 20, 24, 32],\n",
    "    'units': [64, 96, 128],\n",
    "    'layers': [1, 2],\n",
    "    'layer_type': ['LSTM', 'GRU'],\n",
    "    'dropout': [0.2, 0.3, 0.4],\n",
    "    'learning_rate': {'min': 1e-4, 'max': 1e-2, 'log': True},\n",
    "    'batch_size': [32, 64],\n",
    "    'optimizer': ['adam', 'nadam'],\n",
    "    'scaler': ['mixed', 'standard'],\n",
    "    'l1': [0.0, 1e-4, 1e-3],\n",
    "    'l2': [0.0, 1e-4, 1e-3],\n",
    "    'dense_units': [32, 48, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'threshold': [0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================\n",
    "def test_basic_functionality():\n",
    "    \"\"\"Test basic functionality before running full search\"\"\"\n",
    "    print(\"üß™ Testing basic functionality...\")\n",
    "    \n",
    "    # Test model building\n",
    "    test_config = {\n",
    "        'units': 64,\n",
    "        'layers': 1,\n",
    "        'layer_type': 'LSTM',\n",
    "        'dropout': 0.2,\n",
    "        'dense_units': 32,\n",
    "        'l1': 0.0,\n",
    "        'l2': 0.0,\n",
    "        'activation': 'relu',\n",
    "        'optimizer': 'adam',\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = ModelBuilder.build_model((10, 5), test_config)\n",
    "        print(\"‚úÖ Model building test passed\")\n",
    "        \n",
    "        # Test with dummy data\n",
    "        X_dummy = np.random.random((32, 10, 5)).astype(np.float32)\n",
    "        y_dummy = np.random.randint(0, 2, 32).astype(np.float32)\n",
    "        \n",
    "        model.fit(X_dummy, y_dummy, epochs=1, verbose=0)\n",
    "        print(\"‚úÖ Model training test passed\")\n",
    "        \n",
    "        # Test predictions\n",
    "        pred = model.predict(X_dummy[:5], verbose=0)\n",
    "        print(\"‚úÖ Model prediction test passed\")\n",
    "        \n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Basic functionality test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setup GPU\n",
    "    gpu_available = setup_gpu()\n",
    "    \n",
    "    # Test basic functionality first\n",
    "    if not test_basic_functionality():\n",
    "        print(\"‚ùå Basic tests failed. Please check your TensorFlow installation.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize data processor\n",
    "    data_processor = OptimizedDataProcessor(CONFIG)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        X, y = data_processor.load_and_prepare_data()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load data: {e}\")\n",
    "        print(\"Please check your file path and data format.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = SuccessiveHalvingTuner(SEARCH_SPACE, data_processor)\n",
    "    \n",
    "    # Run search\n",
    "    best_config, best_val_f1, test_metrics = tuner.run_search(X, y)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ HYPERPARAMETER SEARCH COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if best_config is not None:\n",
    "        print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "        print(\"\\nüìã Best Configuration:\")\n",
    "        for key, value in best_config.items():\n",
    "            print(f\"   {key:<18}: {value}\")\n",
    "        \n",
    "        print(\"\\nüìä Test Set Performance:\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"   {metric:<12}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful configurations found!\")\n",
    "        print(\"All configurations failed - please check your data and model setup.\")\n",
    "        return\n",
    "    \n",
    "    runtime = (time.time() - start_time) / 60\n",
    "    print(f\"\\n‚è±Ô∏è  Total Runtime: {runtime:.1f} minutes\")\n",
    "    print(f\"üîß GPU Used: {'Yes' if gpu_available else 'No'}\")\n",
    "    \n",
    "    # Save results only if we have a successful run\n",
    "    if best_config is not None:\n",
    "        results_dir = Path(\"results\")\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        results = {\n",
    "            'best_config': best_config,\n",
    "            'best_val_f1': float(best_val_f1),\n",
    "            'test_metrics': {k: float(v) for k, v in test_metrics.items()},\n",
    "            'runtime_minutes': runtime,\n",
    "            'gpu_used': gpu_available,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        results_file = results_dir / \"optimized_lstm_tuning_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {results_file}\")\n",
    "    else:\n",
    "        print(\"‚ùå No results to save - all configurations failed.\")\n",
    "        \n",
    "        # Debug information\n",
    "        print(\"\\nüîç Debugging Information:\")\n",
    "        print(\"- Check that your CSV file path is correct\")\n",
    "        print(\"- Verify your data has the expected columns\")\n",
    "        print(\"- Ensure your target column contains binary values\")\n",
    "        print(\"- Try running with a simpler configuration first\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3db53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ FLAWLESS LSTM TRAINER WITH OPTIMAL PARAMETERS\n",
      "============================================================\n",
      "Using your optimal configuration:\n",
      "   sequence_length: 24\n",
      "   units: 96\n",
      "   layers: 2\n",
      "   layer_type: LSTM\n",
      "   dropout: 0.4\n",
      "   learning_rate: 0.002892684180163678\n",
      "   batch_size: 64\n",
      "   optimizer: nadam\n",
      "   scaler: standard\n",
      "   l1: 0.0001\n",
      "   l2: 0.0001\n",
      "   dense_units: 32\n",
      "   activation: relu\n",
      "   threshold: 0.4\n",
      "üìä Loading data...\n",
      "‚úÖ Data loaded: 15855 samples, 24 features\n",
      "   Target distribution: 51.1% positive, 48.9% negative\n",
      "üìä Data splits:\n",
      "   Train: samples 0 to 10780\n",
      "   Validation: samples 10781 to 12683\n",
      "   Test: samples 12684 to 15854\n",
      "‚úÖ Sequences created:\n",
      "   Train: 10758 sequences\n",
      "   Validation: 1880 sequences\n",
      "   Test: 3148 sequences\n",
      "ü§ñ Building optimal LSTM model...\n",
      "   Architecture: 2 LSTM layers with 96 units each\n",
      "   Dense layer: 32 units (relu activation)\n",
      "   Dropout: 0.4\n",
      "   L1/L2 regularization: 0.0001/0.0001\n",
      "   Learning rate: 0.002893\n",
      "   Batch size: 64\n",
      "   Optimizer: nadam\n",
      "\n",
      "üöÄ Training optimal LSTM model...\n",
      "   Max epochs: 50\n",
      "   Early stopping patience: 12\n",
      "Epoch 1/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5108 - loss: 1.2577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5108 - loss: 1.2528 - val_accuracy: 0.4894 - val_loss: 0.8672 - learning_rate: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5049 - loss: 0.8414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5049 - loss: 0.8406 - val_accuracy: 0.4840 - val_loss: 0.7712 - learning_rate: 0.0029\n",
      "Epoch 3/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5061 - loss: 0.7616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5061 - loss: 0.7613 - val_accuracy: 0.4840 - val_loss: 0.7363 - learning_rate: 0.0029\n",
      "Epoch 4/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5079 - loss: 0.7324"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5079 - loss: 0.7324 - val_accuracy: 0.4840 - val_loss: 0.7174 - learning_rate: 0.0029\n",
      "Epoch 5/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5141 - loss: 0.7151"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5140 - loss: 0.7151 - val_accuracy: 0.4840 - val_loss: 0.7101 - learning_rate: 0.0029\n",
      "Epoch 6/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5144 - loss: 0.7070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5143 - loss: 0.7070 - val_accuracy: 0.4840 - val_loss: 0.7043 - learning_rate: 0.0029\n",
      "Epoch 7/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5173 - loss: 0.7016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5173 - loss: 0.7016 - val_accuracy: 0.4840 - val_loss: 0.7014 - learning_rate: 0.0029\n",
      "Epoch 8/50\n",
      "\u001b[1m168/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5037 - loss: 0.6996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5037 - loss: 0.6996 - val_accuracy: 0.4840 - val_loss: 0.6988 - learning_rate: 0.0029\n",
      "Epoch 9/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5198 - loss: 0.6969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5198 - loss: 0.6969 - val_accuracy: 0.4840 - val_loss: 0.6974 - learning_rate: 0.0029\n",
      "Epoch 10/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5114 - loss: 0.6964 - val_accuracy: 0.4840 - val_loss: 0.6987 - learning_rate: 0.0029\n",
      "Epoch 11/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5035 - loss: 0.6978 - val_accuracy: 0.4840 - val_loss: 0.6977 - learning_rate: 0.0029\n",
      "Epoch 12/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5051 - loss: 0.6967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5051 - loss: 0.6967 - val_accuracy: 0.4840 - val_loss: 0.6971 - learning_rate: 0.0029\n",
      "Epoch 13/50\n",
      "\u001b[1m167/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5097 - loss: 0.6962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5096 - loss: 0.6962 - val_accuracy: 0.4840 - val_loss: 0.6968 - learning_rate: 0.0029\n",
      "Epoch 14/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5141 - loss: 0.6959 - val_accuracy: 0.4840 - val_loss: 0.6974 - learning_rate: 0.0029\n",
      "Epoch 15/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5055 - loss: 0.6964 - val_accuracy: 0.4840 - val_loss: 0.6970 - learning_rate: 0.0029\n",
      "Epoch 16/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5113 - loss: 0.6965 - val_accuracy: 0.4840 - val_loss: 0.6974 - learning_rate: 0.0029\n",
      "Epoch 17/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5072 - loss: 0.6969 - val_accuracy: 0.4840 - val_loss: 0.6981 - learning_rate: 0.0029\n",
      "Epoch 18/50\n",
      "\u001b[1m168/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5105 - loss: 0.6964\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.001446342095732689.\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5105 - loss: 0.6964 - val_accuracy: 0.4840 - val_loss: 0.7012 - learning_rate: 0.0029\n",
      "Epoch 19/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5140 - loss: 0.6961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5140 - loss: 0.6961 - val_accuracy: 0.4840 - val_loss: 0.6963 - learning_rate: 0.0014\n",
      "Epoch 20/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5184 - loss: 0.6947 - val_accuracy: 0.4840 - val_loss: 0.6968 - learning_rate: 0.0014\n",
      "Epoch 21/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5167 - loss: 0.6953 - val_accuracy: 0.4867 - val_loss: 0.6966 - learning_rate: 0.0014\n",
      "Epoch 22/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5178 - loss: 0.6958 - val_accuracy: 0.4872 - val_loss: 0.6971 - learning_rate: 0.0014\n",
      "Epoch 23/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5170 - loss: 0.6960 - val_accuracy: 0.4920 - val_loss: 0.6971 - learning_rate: 0.0014\n",
      "Epoch 24/50\n",
      "\u001b[1m166/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5171 - loss: 0.6962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5171 - loss: 0.6961 - val_accuracy: 0.5074 - val_loss: 0.6956 - learning_rate: 0.0014\n",
      "Epoch 25/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5164 - loss: 0.6978 - val_accuracy: 0.5011 - val_loss: 0.6984 - learning_rate: 0.0014\n",
      "Epoch 26/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5129 - loss: 0.6980 - val_accuracy: 0.5112 - val_loss: 0.6970 - learning_rate: 0.0014\n",
      "Epoch 27/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5223 - loss: 0.6958 - val_accuracy: 0.5138 - val_loss: 0.6964 - learning_rate: 0.0014\n",
      "Epoch 28/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5179 - loss: 0.6961 - val_accuracy: 0.5122 - val_loss: 0.6963 - learning_rate: 0.0014\n",
      "Epoch 29/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5203 - loss: 0.6953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5204 - loss: 0.6952 - val_accuracy: 0.5165 - val_loss: 0.6954 - learning_rate: 0.0014\n",
      "Epoch 30/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5305 - loss: 0.6951 - val_accuracy: 0.5298 - val_loss: 0.6966 - learning_rate: 0.0014\n",
      "Epoch 31/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5119 - loss: 0.7000 - val_accuracy: 0.5255 - val_loss: 0.6962 - learning_rate: 0.0014\n",
      "Epoch 32/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5196 - loss: 0.6976 - val_accuracy: 0.5309 - val_loss: 0.6957 - learning_rate: 0.0014\n",
      "Epoch 33/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5208 - loss: 0.6965 - val_accuracy: 0.5043 - val_loss: 0.6995 - learning_rate: 0.0014\n",
      "Epoch 34/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5178 - loss: 0.6972\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0007231710478663445.\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5178 - loss: 0.6972 - val_accuracy: 0.5229 - val_loss: 0.6984 - learning_rate: 0.0014\n",
      "Epoch 35/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5295 - loss: 0.6961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5295 - loss: 0.6960 - val_accuracy: 0.5351 - val_loss: 0.6939 - learning_rate: 7.2317e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m168/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5271 - loss: 0.6947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5271 - loss: 0.6947 - val_accuracy: 0.5282 - val_loss: 0.6939 - learning_rate: 7.2317e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m166/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5304 - loss: 0.6942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5305 - loss: 0.6942 - val_accuracy: 0.5372 - val_loss: 0.6924 - learning_rate: 7.2317e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m167/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5289 - loss: 0.6944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5289 - loss: 0.6944 - val_accuracy: 0.5441 - val_loss: 0.6917 - learning_rate: 7.2317e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m166/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5354 - loss: 0.6925"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5355 - loss: 0.6925 - val_accuracy: 0.5362 - val_loss: 0.6910 - learning_rate: 7.2317e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5321 - loss: 0.6928 - val_accuracy: 0.5383 - val_loss: 0.6928 - learning_rate: 7.2317e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5344 - loss: 0.6927 - val_accuracy: 0.5372 - val_loss: 0.6914 - learning_rate: 7.2317e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5402 - loss: 0.6920 - val_accuracy: 0.5399 - val_loss: 0.6917 - learning_rate: 7.2317e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5316 - loss: 0.6930 - val_accuracy: 0.5378 - val_loss: 0.6919 - learning_rate: 7.2317e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m165/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5278 - loss: 0.6953\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0003615855239331722.\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5280 - loss: 0.6953 - val_accuracy: 0.5383 - val_loss: 0.6937 - learning_rate: 7.2317e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5348 - loss: 0.6937 - val_accuracy: 0.5410 - val_loss: 0.6914 - learning_rate: 3.6159e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m166/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5297 - loss: 0.6935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5299 - loss: 0.6934 - val_accuracy: 0.5394 - val_loss: 0.6903 - learning_rate: 3.6159e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5360 - loss: 0.6928 - val_accuracy: 0.5388 - val_loss: 0.6907 - learning_rate: 3.6159e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5354 - loss: 0.6915 - val_accuracy: 0.5383 - val_loss: 0.6906 - learning_rate: 3.6159e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m166/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5388 - loss: 0.6911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5388 - loss: 0.6911 - val_accuracy: 0.5346 - val_loss: 0.6898 - learning_rate: 3.6159e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5379 - loss: 0.6921 - val_accuracy: 0.5383 - val_loss: 0.6901 - learning_rate: 3.6159e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "‚úÖ Training completed in 1.9 minutes\n",
      "\n",
      "üìä Evaluating model performance...\n",
      "   Evaluating train set...\n",
      "   Evaluating val set...\n",
      "   Evaluating test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DETAILED MODEL EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "üîç TRAIN SET RESULTS:\n",
      "\n",
      "   With optimal threshold (0.4):\n",
      "     Accuracy:              0.5186\n",
      "     Precision:             0.5155\n",
      "     Recall:                0.9851\n",
      "     F1-Score:              0.6769\n",
      "     AUC:                   0.5626\n",
      "     Positive predictions:  10521/10758 (97.8%)\n",
      "     Confusion matrix:\n",
      "       TP: 5424 | FP: 5097\n",
      "       FN:   82 | TN:  155\n",
      "\n",
      "   Comparison with 0.5 threshold:\n",
      "     Precision: 0.5424 vs 0.5155 (optimal)\n",
      "     Recall:    0.6502 vs 0.9851 (optimal)\n",
      "     F1-Score:  0.5914 vs 0.6769 (optimal)\n",
      "\n",
      "üîç VAL SET RESULTS:\n",
      "\n",
      "   With optimal threshold (0.4):\n",
      "     Accuracy:              0.4920\n",
      "     Precision:             0.4878\n",
      "     Recall:                0.9857\n",
      "     F1-Score:              0.6526\n",
      "     AUC:                   0.5589\n",
      "     Positive predictions:  1839/1880 (97.8%)\n",
      "     Confusion matrix:\n",
      "       TP:  897 | FP:  942\n",
      "       FN:   13 | TN:   28\n",
      "\n",
      "   Comparison with 0.5 threshold:\n",
      "     Precision: 0.5156 vs 0.4878 (optimal)\n",
      "     Recall:    0.6341 vs 0.9857 (optimal)\n",
      "     F1-Score:  0.5688 vs 0.6526 (optimal)\n",
      "\n",
      "üîç TEST SET RESULTS:\n",
      "\n",
      "   With optimal threshold (0.4):\n",
      "     Accuracy:              0.5289\n",
      "     Precision:             0.5286\n",
      "     Recall:                0.9063\n",
      "     F1-Score:              0.6677\n",
      "     AUC:                   0.5390\n",
      "     Positive predictions:  2819/3148 (89.5%)\n",
      "     Confusion matrix:\n",
      "       TP: 1490 | FP: 1329\n",
      "       FN:  154 | TN:  175\n",
      "\n",
      "   Comparison with 0.5 threshold:\n",
      "     Precision: 0.5513 vs 0.5286 (optimal)\n",
      "     Recall:    0.4708 vs 0.9063 (optimal)\n",
      "     F1-Score:  0.5079 vs 0.6677 (optimal)\n",
      "üíæ Model saved: results\\optimal_lstm_model_20250603_172029.h5\n",
      "üíæ Train predictions saved: results\\lstm_train_predictions_20250603_172029.csv (10758 rows)\n",
      "üíæ Val predictions saved: results\\lstm_val_predictions_20250603_172029.csv (1880 rows)\n",
      "üíæ Test predictions saved: results\\lstm_test_predictions_20250603_172029.csv (3148 rows)\n",
      "üíæ Combined predictions saved: results\\lstm_all_predictions_20250603_172029.csv (15786 rows)\n",
      "\n",
      "üéØ MAIN OUTPUT: results\\lstm_test_predictions_20250603_172029.csv\n",
      "   Test set predictions: 3148 rows\n",
      "üíæ Results summary saved: results\\optimal_lstm_results_20250603_172029.json\n",
      "\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "   üéØ MAIN TEST PREDICTIONS: results\\lstm_test_predictions_20250603_172029.csv\n",
      "   üìÅ All files saved:\n",
      "      train: results\\lstm_train_predictions_20250603_172029.csv\n",
      "      val: results\\lstm_val_predictions_20250603_172029.csv\n",
      "      test: results\\lstm_test_predictions_20250603_172029.csv\n",
      "      combined: results\\lstm_all_predictions_20250603_172029.csv\n",
      "   ü§ñ Trained model: N/A\n",
      "   üìä Results summary: results\\optimal_lstm_results_20250603_172029.json\n",
      "\n",
      "üéØ FINAL TEST SET PERFORMANCE (threshold = 0.4):\n",
      "   Precision: 0.5286\n",
      "   Recall:    0.9063\n",
      "   F1-Score:  0.6677\n",
      "   AUC:       0.5390\n",
      "\n",
      "üìÑ Test predictions CSV: 3148 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  FLAWLESS LSTM TRAINER WITH OPTIMAL PARAMETERS\n",
    "# =============================================================\n",
    "import warnings, os, time, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Silence warnings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ TensorFlow imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Sklearn imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Reproducibility ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# =============================================================\n",
    "# CONFIGURATION WITH YOUR OPTIMAL PARAMETERS\n",
    "# =============================================================\n",
    "DATA_CONFIG = {\n",
    "    'csv_file': Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_with_features_4h.csv\"),\n",
    "    'time_col': 'timestamp',\n",
    "    'target_col': 'target',\n",
    "    'start_date': '2018-01-01',\n",
    "    'test_frac': 0.20,\n",
    "    'val_frac': 0.15,\n",
    "    'drop_cols': [\n",
    "        'open', 'high', 'low', 'high_low', 'high_close', 'low_close', 'typical_price',\n",
    "        'bollinger_upper', 'bollinger_lower', 'MACD_line', 'MACD_signal', 'stoch_%D',\n",
    "        'EMA_21', 'SMA_20',\n",
    "        'bullish_scenario_1', 'bullish_scenario_2', 'bullish_scenario_3',\n",
    "        'bullish_scenario_4', 'bullish_scenario_5', 'bullish_scenario_6',\n",
    "        'bearish_scenario_1', 'bearish_scenario_2', 'bearish_scenario_3',\n",
    "        'bearish_scenario_4', 'bearish_scenario_6',\n",
    "        'volume_breakout', 'volume_breakdown', 'break_upper_band', 'break_lower_band',\n",
    "        'vol_spike_1_5x', 'rsi_oversold', 'rsi_overbought', 'stoch_overbought',\n",
    "        'stoch_oversold', 'cci_overbought', 'cci_oversold', 'near_upper_band',\n",
    "        'near_lower_band', 'overbought_reversal', 'oversold_reversal',\n",
    "        'above_sma20', 'above_sma50', 'ema7_above_ema21',\n",
    "        'ema_cross_up', 'ema_cross_down', 'macd_cross_up', 'macd_cross_down',\n",
    "        'macd_positive', 'momentum_alignment', 'macd_rising', 'obv_rising_24h',\n",
    "        'trending_market', 'trend_alignment',\n",
    "        'support_level', 'resistance_level', 'volatility_regime',\n",
    "        'close_daily', 'rsi_daily'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Your optimal hyperparameters\n",
    "OPTIMAL_CONFIG = {\n",
    "    'sequence_length': 24,\n",
    "    'units': 96,\n",
    "    'layers': 2,\n",
    "    'layer_type': 'LSTM',\n",
    "    'dropout': 0.4,\n",
    "    'learning_rate': 0.002892684180163678,\n",
    "    'batch_size': 64,\n",
    "    'optimizer': 'nadam',\n",
    "    'scaler': 'standard',\n",
    "    'l1': 0.0001,\n",
    "    'l2': 0.0001,\n",
    "    'dense_units': 32,\n",
    "    'activation': 'relu',\n",
    "    'threshold': 0.4\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# DATA PROCESSING CLASS (SAME AS YOUR SEARCH)\n",
    "# =============================================================\n",
    "class OptimizedDataProcessor:\n",
    "    \"\"\"Streamlined data processing matching your hyperparameter search\"\"\"\n",
    "    \n",
    "    def __init__(self, data_config):\n",
    "        self.config = data_config\n",
    "        self.scaler = None\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load and prepare data efficiently\"\"\"\n",
    "        print(\"üìä Loading data...\")\n",
    "        \n",
    "        if not self.config['csv_file'].exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {self.config['csv_file']}\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(self.config['csv_file'], parse_dates=[self.config['time_col']])\n",
    "        df = df.set_index(self.config['time_col']).sort_index()\n",
    "        df = df.loc[self.config['start_date']:]\n",
    "        \n",
    "        # Keep original timestamps for predictions\n",
    "        original_timestamps = df.index.copy()\n",
    "        \n",
    "        # Prepare features and target\n",
    "        cols_to_drop = list(set(self.config['drop_cols']) & set(df.columns)) + [self.config['target_col']]\n",
    "        X = df.drop(columns=cols_to_drop)\n",
    "        y = df[self.config['target_col']]\n",
    "        \n",
    "        # Handle missing values\n",
    "        if X.isna().any().any():\n",
    "            X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        print(f\"   Target distribution: {y.mean():.1%} positive, {1-y.mean():.1%} negative\")\n",
    "        \n",
    "        return X, y, original_timestamps\n",
    "    \n",
    "    def create_sequences(self, X, y, seq_len):\n",
    "        \"\"\"Create sequences efficiently using numpy operations\"\"\"\n",
    "        n_samples = len(X) - seq_len + 1\n",
    "        \n",
    "        # Pre-allocate arrays\n",
    "        X_seq = np.empty((n_samples, seq_len, X.shape[1]), dtype=np.float32)\n",
    "        y_seq = np.empty(n_samples, dtype=np.float32)\n",
    "        \n",
    "        # Use array views for efficiency\n",
    "        X_values = X.values.astype(np.float32)\n",
    "        y_values = y.values.astype(np.float32)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            X_seq[i] = X_values[i:i+seq_len]\n",
    "            y_seq[i] = y_values[i+seq_len-1]\n",
    "            \n",
    "        return X_seq, y_seq\n",
    "    \n",
    "    def split_and_scale_data(self, X, y, timestamps, seq_len):\n",
    "        \"\"\"Split and scale data using standard scaler (matching your optimal config)\"\"\"\n",
    "        # Calculate split indices\n",
    "        n_samples = len(X)\n",
    "        test_idx = int(n_samples * (1 - self.config['test_frac']))\n",
    "        val_idx = int(test_idx * (1 - self.config['val_frac']))\n",
    "        \n",
    "        print(f\"üìä Data splits:\")\n",
    "        print(f\"   Train: samples 0 to {val_idx-1}\")\n",
    "        print(f\"   Validation: samples {val_idx} to {test_idx-1}\")\n",
    "        print(f\"   Test: samples {test_idx} to {n_samples-1}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test = X.iloc[:val_idx], X.iloc[val_idx:test_idx], X.iloc[test_idx:]\n",
    "        y_train, y_val, y_test = y.iloc[:val_idx], y.iloc[val_idx:test_idx], y.iloc[test_idx:]\n",
    "        ts_train = timestamps[:val_idx]\n",
    "        ts_val = timestamps[val_idx:test_idx]\n",
    "        ts_test = timestamps[test_idx:]\n",
    "        \n",
    "        # Scale features using StandardScaler (your optimal scaler)\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train).astype(np.float32)\n",
    "        X_val_scaled = self.scaler.transform(X_val).astype(np.float32)\n",
    "        X_test_scaled = self.scaler.transform(X_test).astype(np.float32)\n",
    "        \n",
    "        # Convert back to DataFrames for sequence creation\n",
    "        columns = X_train.columns\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=columns)\n",
    "        X_val_df = pd.DataFrame(X_val_scaled, index=X_val.index, columns=columns)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=columns)\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_seq, y_train_seq = self.create_sequences(X_train_df, y_train, seq_len)\n",
    "        X_val_seq, y_val_seq = self.create_sequences(X_val_df, y_val, seq_len)\n",
    "        X_test_seq, y_test_seq = self.create_sequences(X_test_df, y_test, seq_len)\n",
    "        \n",
    "        # Adjust timestamps for sequences\n",
    "        ts_train_seq = ts_train[seq_len-1:]\n",
    "        ts_val_seq = ts_val[seq_len-1:]\n",
    "        ts_test_seq = ts_test[seq_len-1:]\n",
    "        \n",
    "        print(f\"‚úÖ Sequences created:\")\n",
    "        print(f\"   Train: {X_train_seq.shape[0]} sequences\")\n",
    "        print(f\"   Validation: {X_val_seq.shape[0]} sequences\")\n",
    "        print(f\"   Test: {X_test_seq.shape[0]} sequences\")\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train_seq, y_train_seq, ts_train_seq),\n",
    "            'val': (X_val_seq, y_val_seq, ts_val_seq),\n",
    "            'test': (X_test_seq, y_test_seq, ts_test_seq)\n",
    "        }\n",
    "\n",
    "# =============================================================\n",
    "# MODEL BUILDING (MATCHING YOUR SEARCH ARCHITECTURE)\n",
    "# =============================================================\n",
    "def build_optimal_lstm_model(input_shape, config):\n",
    "    \"\"\"Build LSTM model with your optimal configuration\"\"\"\n",
    "    print(f\"ü§ñ Building optimal LSTM model...\")\n",
    "    print(f\"   Architecture: {config['layers']} LSTM layers with {config['units']} units each\")\n",
    "    print(f\"   Dense layer: {config['dense_units']} units ({config['activation']} activation)\")\n",
    "    print(f\"   Dropout: {config['dropout']}\")\n",
    "    print(f\"   L1/L2 regularization: {config['l1']}/{config['l2']}\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    for i in range(config['layers']):\n",
    "        return_sequences = (i < config['layers'] - 1)\n",
    "        \n",
    "        if i == 0:\n",
    "            model.add(LSTM(\n",
    "                config['units'],\n",
    "                return_sequences=return_sequences,\n",
    "                input_shape=input_shape,\n",
    "                kernel_regularizer=l1_l2(config['l1'], config['l2']),\n",
    "                recurrent_regularizer=l1_l2(config['l1'], config['l2'])\n",
    "            ))\n",
    "        else:\n",
    "            model.add(LSTM(\n",
    "                config['units'],\n",
    "                return_sequences=return_sequences,\n",
    "                kernel_regularizer=l1_l2(config['l1'], config['l2']),\n",
    "                recurrent_regularizer=l1_l2(config['l1'], config['l2'])\n",
    "            ))\n",
    "    \n",
    "    # Add dense layers\n",
    "    model.add(Dropout(config['dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(config['dense_units'], activation=config['activation']))\n",
    "    model.add(Dropout(config['dropout']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid', dtype='float32'))\n",
    "    \n",
    "    # Compile with your optimal optimizer and learning rate\n",
    "    optimizer = Nadam(learning_rate=config['learning_rate'])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"   Learning rate: {config['learning_rate']:.6f}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    print(f\"   Optimizer: {config['optimizer']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =============================================================\n",
    "# TRAINING AND EVALUATION\n",
    "# =============================================================\n",
    "def calculate_comprehensive_metrics(y_true, y_pred_prob, threshold=0.5):\n",
    "    \"\"\"Calculate all metrics\"\"\"\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    except ValueError:\n",
    "        auc = 0.5\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'auc': auc,\n",
    "        'true_positives': int(tp),\n",
    "        'false_positives': int(fp),\n",
    "        'true_negatives': int(tn),\n",
    "        'false_negatives': int(fn),\n",
    "        'positive_predictions': int(y_pred.sum()),\n",
    "        'total_predictions': len(y_pred),\n",
    "        'positive_rate': float(y_pred.mean()),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "def train_optimal_lstm(data_splits, config, epochs=50):\n",
    "    \"\"\"Train LSTM with optimal parameters and proper callbacks\"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    X_train, y_train, ts_train = data_splits['train']\n",
    "    X_val, y_val, ts_val = data_splits['val']\n",
    "    X_test, y_test, ts_test = data_splits['test']\n",
    "    \n",
    "    # Build model\n",
    "    model = build_optimal_lstm_model((X_train.shape[1], X_train.shape[2]), config)\n",
    "    \n",
    "    # Setup improved callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=12,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Training optimal LSTM model...\")\n",
    "    print(f\"   Max epochs: {epochs}\")\n",
    "    print(f\"   Early stopping patience: 12\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=config['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Training completed in {training_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Generate predictions and evaluate\n",
    "    print(f\"\\nüìä Evaluating model performance...\")\n",
    "    \n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    for split_name, (X_split, y_split, ts_split) in data_splits.items():\n",
    "        print(f\"   Evaluating {split_name} set...\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred_prob = model.predict(X_split, verbose=0).flatten()\n",
    "        \n",
    "        # Calculate metrics with optimal threshold\n",
    "        metrics = calculate_comprehensive_metrics(y_split, y_pred_prob, config['threshold'])\n",
    "        \n",
    "        # Also calculate with 0.5 threshold for comparison\n",
    "        metrics_05 = calculate_comprehensive_metrics(y_split, y_pred_prob, 0.5)\n",
    "        \n",
    "        results[split_name] = {\n",
    "            'optimal_threshold': metrics,\n",
    "            'threshold_05': metrics_05\n",
    "        }\n",
    "        \n",
    "        predictions[split_name] = {\n",
    "            'timestamps': ts_split,\n",
    "            'y_true': y_split,\n",
    "            'y_pred_prob': y_pred_prob,\n",
    "            'y_pred_optimal': (y_pred_prob >= config['threshold']).astype(int),\n",
    "            'y_pred_05': (y_pred_prob >= 0.5).astype(int)\n",
    "        }\n",
    "    \n",
    "    return model, results, predictions, history, training_time\n",
    "\n",
    "# =============================================================\n",
    "# RESULTS DISPLAY AND EXPORT\n",
    "# =============================================================\n",
    "def print_detailed_results(results, config):\n",
    "    \"\"\"Print comprehensive results\"\"\"\n",
    "    print(f\"\\nüìä DETAILED MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for split_name, split_results in results.items():\n",
    "        print(f\"\\nüîç {split_name.upper()} SET RESULTS:\")\n",
    "        \n",
    "        # Results with optimal threshold\n",
    "        opt_metrics = split_results['optimal_threshold']\n",
    "        print(f\"\\n   With optimal threshold ({config['threshold']}):\")\n",
    "        print(f\"     Accuracy:              {opt_metrics['accuracy']:.4f}\")\n",
    "        print(f\"     Precision:             {opt_metrics['precision']:.4f}\")\n",
    "        print(f\"     Recall:                {opt_metrics['recall']:.4f}\")\n",
    "        print(f\"     F1-Score:              {opt_metrics['f1']:.4f}\")\n",
    "        print(f\"     AUC:                   {opt_metrics['auc']:.4f}\")\n",
    "        print(f\"     Positive predictions:  {opt_metrics['positive_predictions']}/{opt_metrics['total_predictions']} ({opt_metrics['positive_rate']:.1%})\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        print(f\"     Confusion matrix:\")\n",
    "        print(f\"       TP: {opt_metrics['true_positives']:4d} | FP: {opt_metrics['false_positives']:4d}\")\n",
    "        print(f\"       FN: {opt_metrics['false_negatives']:4d} | TN: {opt_metrics['true_negatives']:4d}\")\n",
    "        \n",
    "        # Comparison with 0.5 threshold\n",
    "        comp_metrics = split_results['threshold_05']\n",
    "        print(f\"\\n   Comparison with 0.5 threshold:\")\n",
    "        print(f\"     Precision: {comp_metrics['precision']:.4f} vs {opt_metrics['precision']:.4f} (optimal)\")\n",
    "        print(f\"     Recall:    {comp_metrics['recall']:.4f} vs {opt_metrics['recall']:.4f} (optimal)\")\n",
    "        print(f\"     F1-Score:  {comp_metrics['f1']:.4f} vs {opt_metrics['f1']:.4f} (optimal)\")\n",
    "\n",
    "def create_predictions_csv(predictions, config, output_dir=\"results\"):\n",
    "    \"\"\"Create separate CSV files for each split, with test as main output\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Create CSV for each split\n",
    "    for split_name, pred_data in predictions.items():\n",
    "        split_df = pd.DataFrame({\n",
    "            'timestamp': pred_data['timestamps'],\n",
    "            'actual': pred_data['y_true'],\n",
    "            'probability': pred_data['y_pred_prob'],\n",
    "            'prediction_optimal': pred_data['y_pred_optimal'],\n",
    "            'prediction_05': pred_data['y_pred_05'],\n",
    "            'confidence': np.abs(pred_data['y_pred_prob'] - 0.5),\n",
    "            'high_confidence': (np.abs(pred_data['y_pred_prob'] - 0.5) > 0.3).astype(int),\n",
    "            'very_high_confidence': (np.abs(pred_data['y_pred_prob'] - 0.5) > 0.4).astype(int)\n",
    "        })\n",
    "        \n",
    "        # Add useful columns\n",
    "        split_df['correct_optimal'] = (split_df['actual'] == split_df['prediction_optimal']).astype(int)\n",
    "        split_df['correct_05'] = (split_df['actual'] == split_df['prediction_05']).astype(int)\n",
    "        split_df['date'] = split_df['timestamp'].dt.date\n",
    "        split_df['year'] = split_df['timestamp'].dt.year\n",
    "        split_df['month'] = split_df['timestamp'].dt.month\n",
    "        split_df['hour'] = split_df['timestamp'].dt.hour\n",
    "        \n",
    "        # Save individual CSV\n",
    "        csv_file = output_path / f\"lstm_{split_name}_predictions_{timestamp}.csv\"\n",
    "        split_df.to_csv(csv_file, index=False)\n",
    "        saved_files[split_name] = csv_file\n",
    "        \n",
    "        print(f\"üíæ {split_name.capitalize()} predictions saved: {csv_file} ({len(split_df)} rows)\")\n",
    "    \n",
    "    # Create combined CSV as well (for completeness)\n",
    "    all_data = []\n",
    "    for split_name, pred_data in predictions.items():\n",
    "        split_df = pd.DataFrame({\n",
    "            'timestamp': pred_data['timestamps'],\n",
    "            'split': split_name,\n",
    "            'actual': pred_data['y_true'],\n",
    "            'probability': pred_data['y_pred_prob'],\n",
    "            'prediction_optimal': pred_data['y_pred_optimal'],\n",
    "            'prediction_05': pred_data['y_pred_05'],\n",
    "            'confidence': np.abs(pred_data['y_pred_prob'] - 0.5),\n",
    "            'high_confidence': (np.abs(pred_data['y_pred_prob'] - 0.5) > 0.3).astype(int),\n",
    "            'very_high_confidence': (np.abs(pred_data['y_pred_prob'] - 0.5) > 0.4).astype(int)\n",
    "        })\n",
    "        all_data.append(split_df)\n",
    "    \n",
    "    # Combined file\n",
    "    combined_df = pd.concat(all_data, ignore_index=False)\n",
    "    combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    combined_df['correct_optimal'] = (combined_df['actual'] == combined_df['prediction_optimal']).astype(int)\n",
    "    combined_df['correct_05'] = (combined_df['actual'] == combined_df['prediction_05']).astype(int)\n",
    "    combined_df['date'] = combined_df['timestamp'].dt.date\n",
    "    combined_df['year'] = combined_df['timestamp'].dt.year\n",
    "    combined_df['month'] = combined_df['timestamp'].dt.month\n",
    "    combined_df['hour'] = combined_df['timestamp'].dt.hour\n",
    "    \n",
    "    combined_file = output_path / f\"lstm_all_predictions_{timestamp}.csv\"\n",
    "    combined_df.to_csv(combined_file, index=False)\n",
    "    saved_files['combined'] = combined_file\n",
    "    \n",
    "    print(f\"üíæ Combined predictions saved: {combined_file} ({len(combined_df)} rows)\")\n",
    "    \n",
    "    # Return test predictions as main DataFrame (most important)\n",
    "    test_file = saved_files['test']\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    print(f\"\\nüéØ MAIN OUTPUT: {test_file}\")\n",
    "    print(f\"   Test set predictions: {len(test_df)} rows\")\n",
    "    \n",
    "    return test_df, saved_files\n",
    "\n",
    "def save_complete_results(model, results, predictions, config, training_time, output_dir=\"results\"):\n",
    "    \"\"\"Save model and complete results\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model\n",
    "    model_file = output_path / f\"optimal_lstm_model_{timestamp}.h5\"\n",
    "    model.save(model_file)\n",
    "    print(f\"üíæ Model saved: {model_file}\")\n",
    "    \n",
    "    # Save predictions CSVs (separate files for each split)\n",
    "    test_predictions_df, saved_files = create_predictions_csv(predictions, config, output_dir)\n",
    "    \n",
    "    # Save complete results summary\n",
    "    results_summary = {\n",
    "        'optimal_config': config,\n",
    "        'training_time_minutes': training_time / 60,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_file': str(model_file),\n",
    "        'prediction_files': {k: str(v) for k, v in saved_files.items()},\n",
    "        'main_test_file': str(saved_files['test']),\n",
    "        'data_config': DATA_CONFIG,\n",
    "        'results_by_split': {\n",
    "            split: {\n",
    "                threshold_type: {k: float(v) if isinstance(v, (int, float, np.number)) else v \n",
    "                               for k, v in metrics.items()}\n",
    "                for threshold_type, metrics in split_data.items()\n",
    "            }\n",
    "            for split, split_data in results.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    json_file = output_path / f\"optimal_lstm_results_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Results summary saved: {json_file}\")\n",
    "    \n",
    "    return test_predictions_df, saved_files, json_file\n",
    "\n",
    "# =============================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üéØ FLAWLESS LSTM TRAINER WITH OPTIMAL PARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Using your optimal configuration:\")\n",
    "    for key, value in OPTIMAL_CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize data processor\n",
    "        data_processor = OptimizedDataProcessor(DATA_CONFIG)\n",
    "        \n",
    "        # Load and prepare data\n",
    "        X, y, timestamps = data_processor.load_and_prepare_data()\n",
    "        \n",
    "        # Split and scale data\n",
    "        data_splits = data_processor.split_and_scale_data(\n",
    "            X, y, timestamps, OPTIMAL_CONFIG['sequence_length']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model, results, predictions, history, training_time = train_optimal_lstm(\n",
    "            data_splits, OPTIMAL_CONFIG, epochs=50\n",
    "        )\n",
    "        \n",
    "        # Print detailed results\n",
    "        print_detailed_results(results, OPTIMAL_CONFIG)\n",
    "        \n",
    "        # Save everything\n",
    "        test_predictions_df, saved_files, json_file = save_complete_results(\n",
    "            model, results, predictions, OPTIMAL_CONFIG, training_time\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ TRAINING COMPLETE!\")\n",
    "        print(f\"   üéØ MAIN TEST PREDICTIONS: {saved_files['test']}\")\n",
    "        print(f\"   üìÅ All files saved:\")\n",
    "        for split_name, file_path in saved_files.items():\n",
    "            print(f\"      {split_name}: {file_path}\")\n",
    "        print(f\"   ü§ñ Trained model: {saved_files.get('model', 'N/A')}\")\n",
    "        print(f\"   üìä Results summary: {json_file}\")\n",
    "        \n",
    "        # Final summary\n",
    "        test_results = results['test']['optimal_threshold']\n",
    "        print(f\"\\nüéØ FINAL TEST SET PERFORMANCE (threshold = {OPTIMAL_CONFIG['threshold']}):\")\n",
    "        print(f\"   Precision: {test_results['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {test_results['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {test_results['f1']:.4f}\")\n",
    "        print(f\"   AUC:       {test_results['auc']:.4f}\")\n",
    "        print(f\"\\nüìÑ Test predictions CSV: {len(test_predictions_df)} rows\")\n",
    "        \n",
    "        return model, test_predictions_df, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, predictions_df, results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
