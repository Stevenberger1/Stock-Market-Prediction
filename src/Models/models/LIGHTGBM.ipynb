{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7bb9db",
   "metadata": {},
   "source": [
    "# In this notebook we would create the LIGHTGBM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d167382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.69157\n",
      "Trial 01 → weighted-F1 = 0.4786  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.692375\n",
      "Trial 02 → weighted-F1 = 0.5135  |  params: {'learning_rate': 0.05, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 03 → weighted-F1 = 0.4799  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 04 → weighted-F1 = 0.5000  |  params: {'learning_rate': 0.02, 'num_leaves': 31, 'feature_fraction': 0.9, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.691752\n",
      "Trial 05 → weighted-F1 = 0.4959  |  params: {'learning_rate': 0.02, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's binary_logloss: 0.691597\n",
      "Trial 06 → weighted-F1 = 0.4797  |  params: {'learning_rate': 0.01, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.691621\n",
      "Trial 07 → weighted-F1 = 0.5030  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.691911\n",
      "Trial 08 → weighted-F1 = 0.4799  |  params: {'learning_rate': 0.05, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.691611\n",
      "Trial 09 → weighted-F1 = 0.4765  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.69147\n",
      "Trial 10 → weighted-F1 = 0.5270  |  params: {'learning_rate': 0.05, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's binary_logloss: 0.691737\n",
      "Trial 11 → weighted-F1 = 0.4908  |  params: {'learning_rate': 0.01, 'num_leaves': 63, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 20}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.691776\n",
      "Trial 12 → weighted-F1 = 0.4909  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.691455\n",
      "Trial 13 → weighted-F1 = 0.4774  |  params: {'learning_rate': 0.01, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.69149\n",
      "Trial 14 → weighted-F1 = 0.5002  |  params: {'learning_rate': 0.02, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.691437\n",
      "Trial 15 → weighted-F1 = 0.4860  |  params: {'learning_rate': 0.02, 'num_leaves': 127, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'min_child_samples': 40}\n",
      "\n",
      "Search finished in 6.6 s\n",
      "\n",
      "──────── Best parameters ────────\n",
      "learning_rate    : 0.05\n",
      "num_leaves       : 63\n",
      "feature_fraction : 0.9\n",
      "bagging_fraction : 0.8\n",
      "bagging_freq     : 1\n",
      "min_child_samples: 20\n",
      "Best weighted-F1 : 0.5270\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mini LightGBM hyper-parameter search (15 random trials) for BTC direction\n",
    "------------------------------------------------------------------------\n",
    "* Keeps chronological 80 / 20 split\n",
    "* Evaluates weighted-F1 (precision ×2)\n",
    "* Prints the best parameter set and its weighted-F1\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np, pandas as pd, random, itertools, time\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────── config ─────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\", \"upper_shadow\", \"lower_shadow\"]\n",
    "VAL_FRAC  = 0.20\n",
    "W_PREC    = 2.0\n",
    "N_TRIALS  = 15          # “a little bit more runs”\n",
    "\n",
    "# ───────── data prep (same as before) ─────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"].astype(int)\n",
    "split_idx = int(len(df) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# ───────── search space ─────────\n",
    "space = {\n",
    "    \"learning_rate\"   : [0.01, 0.02, 0.05],\n",
    "    \"num_leaves\"      : [31, 63, 127],\n",
    "    \"feature_fraction\": [0.8, 0.9],\n",
    "    \"bagging_fraction\": [0.7, 0.8],\n",
    "    \"bagging_freq\"    : [1],\n",
    "    \"min_child_samples\": [20, 40]\n",
    "}\n",
    "\n",
    "def random_param():\n",
    "    return {k: random.choice(v) for k, v in space.items()}\n",
    "\n",
    "# ───────── weighted-F1 helper ─────────\n",
    "def weighted_f1(y_true, y_pred_prob, thr=0.5, w=2.0):\n",
    "    y_pred = (y_pred_prob >= thr).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    return (1 + w) * prec * rec / (w * prec + rec + 1e-12)\n",
    "\n",
    "# ───────── mini search ─────────\n",
    "best_score, best_params = -1, None\n",
    "tic = time.time()\n",
    "\n",
    "for t in range(1, N_TRIALS + 1):\n",
    "    params = random_param()\n",
    "    model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=2000,\n",
    "        **params,\n",
    "        verbose=-1,\n",
    "        random_state=42+t\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=150),\n",
    "            log_evaluation(0)  # silent\n",
    "        ]\n",
    "    )\n",
    "    y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    score = weighted_f1(y_val, y_prob, w=W_PREC)\n",
    "\n",
    "    print(f\"Trial {t:02d} → weighted-F1 = {score:.4f}  |  params: {params}\")\n",
    "    if score > best_score:\n",
    "        best_score, best_params = score, params\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"\\nSearch finished in {toc - tic:.1f} s\")\n",
    "\n",
    "print(\"\\n──────── Best parameters ────────\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k:<17}: {v}\")\n",
    "print(f\"Best weighted-F1 : {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ce041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.701439\n",
      "[200]\tvalid_0's binary_logloss: 0.705638\n",
      "[300]\tvalid_0's binary_logloss: 0.709782\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.691879\n",
      "Evaluated only: binary_logloss\n",
      "\n",
      "──── Validation metrics (thr = 0.50) ────\n",
      "Accuracy          :  0.529\n",
      "Class 0 (Down) →  Precision:  0.517  Recall:  0.580  F1:  0.547\n",
      "Class 1 (Up  ) →  Precision:  0.544  Recall:  0.481  F1:  0.511\n",
      "Macro-F1          :  0.529\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final LightGBM model\n",
    "--------------------\n",
    "Uses the best mini-search parameters:\n",
    "\n",
    "    learning_rate     = 0.05\n",
    "    num_leaves        = 63\n",
    "    feature_fraction  = 0.9\n",
    "    bagging_fraction  = 0.8\n",
    "    bagging_freq      = 1\n",
    "    min_child_samples = 20\n",
    "\n",
    "Prints accuracy, precision, recall and F1 for each class.\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────── imports ─────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ────────── file path & columns to drop ────────────\n",
    "CSV_PATH  = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    "DROP_COLS = [\"vol_ratio_24h\", \"macd_diff\", \"macd_line\",\n",
    "             \"upper_shadow\", \"lower_shadow\"]\n",
    "\n",
    "VAL_FRAC = 0.20         # 80 % train · 20 % validation\n",
    "PREC_W   = 2.0          # precision weight for weighted-F1\n",
    "\n",
    "# ───────────────── data preparation ────────────────\n",
    "df = pd.read_csv(CSV_PATH, index_col=0, parse_dates=True)\n",
    "df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True)\n",
    "df[\"Volume BTC\"] = np.log1p(df[\"Volume BTC\"])\n",
    "\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(int)   # 1 = Up\n",
    "df = df.dropna().select_dtypes(include=[np.number])\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"].astype(int)\n",
    "\n",
    "split_idx = int(len(df) * (1 - VAL_FRAC))\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# ─────────── best-parameter LightGBM model ─────────\n",
    "best_params = dict(\n",
    "    objective         = \"binary\",\n",
    "    learning_rate     = 0.05,\n",
    "    num_leaves        = 63,\n",
    "    feature_fraction  = 0.9,\n",
    "    bagging_fraction  = 0.8,\n",
    "    bagging_freq      = 1,\n",
    "    min_child_samples = 20,\n",
    "    n_estimators      = 4000,   # large upper bound – early stop will trim\n",
    "    verbose           = -1,\n",
    "    random_state      = 42\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(**best_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"binary_logloss\",\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=300, first_metric_only=True),\n",
    "        log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ─────────────────── evaluation ────────────────────\n",
    "y_prob = model.predict_proba(X_val)[:, 1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_val, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, labels=[0, 1], zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n──── Validation metrics (thr = 0.50) ────\")\n",
    "print(f\"Accuracy          : {acc:6.3f}\")\n",
    "print(f\"Class 0 (Down) →  Precision: {prec[0]:6.3f}  Recall: {rec[0]:6.3f}  F1: {f1[0]:6.3f}\")\n",
    "print(f\"Class 1 (Up  ) →  Precision: {prec[1]:6.3f}  Recall: {rec[1]:6.3f}  F1: {f1[1]:6.3f}\")\n",
    "print(f\"Macro-F1          : {f1.mean():6.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
