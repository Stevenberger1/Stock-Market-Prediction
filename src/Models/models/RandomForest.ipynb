{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a84382f",
   "metadata": {},
   "source": [
    "# In this notebook we will train the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb96deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e71fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Year  HorizonH   Bin  Precision  Recall    F1  Samples\n",
      " 2016         1 0.005      0.807   0.335 0.301    80654\n",
      " 2016         1 0.010      0.315   0.333 0.324    80654\n",
      " 2016         1 0.020      0.330   0.333 0.332    80654\n",
      " 2016         2 0.005      0.425   0.338 0.287    80653\n",
      " 2016         2 0.010      0.629   0.334 0.314    80653\n",
      " 2016         2 0.020      0.325   0.333 0.329    80653\n",
      " 2016         4 0.005      0.409   0.378 0.353    80651\n",
      " 2016         4 0.010      0.414   0.334 0.297    80651\n",
      " 2016         4 0.020      0.312   0.333 0.322    80651\n",
      " 2016        12 0.005      0.395   0.374 0.339    80643\n",
      " 2016        12 0.010      0.369   0.366 0.341    80643\n",
      " 2016        12 0.020      0.396   0.335 0.305    80643\n",
      " 2018         1 0.005      0.592   0.335 0.295    63355\n",
      " 2018         1 0.010      0.311   0.333 0.322    63355\n",
      " 2018         1 0.020      0.329   0.333 0.331    63355\n",
      " 2018         2 0.005      0.448   0.339 0.282    63354\n",
      " 2018         2 0.010      0.624   0.334 0.311    63354\n",
      " 2018         2 0.020      0.323   0.333 0.328    63354\n",
      " 2018         4 0.005      0.391   0.368 0.338    63352\n",
      " 2018         4 0.010      0.422   0.334 0.291    63352\n",
      " 2018         4 0.020      0.309   0.333 0.321    63352\n",
      " 2018        12 0.005      0.363   0.346 0.279    63344\n",
      " 2018        12 0.010      0.352   0.352 0.320    63344\n",
      " 2018        12 0.020      0.379   0.334 0.298    63344\n",
      " 2020         1 0.005      0.388   0.335 0.292    45874\n",
      " 2020         1 0.010      0.310   0.333 0.321    45874\n",
      " 2020         1 0.020      0.329   0.333 0.331    45874\n",
      " 2020         2 0.005      0.449   0.344 0.290    45873\n",
      " 2020         2 0.010      0.287   0.333 0.309    45873\n",
      " 2020         2 0.020      0.322   0.333 0.327    45873\n",
      " 2020         4 0.005      0.415   0.377 0.346    45871\n",
      " 2020         4 0.010      0.418   0.334 0.287    45871\n",
      " 2020         4 0.020      0.307   0.333 0.320    45871\n",
      " 2020        12 0.005      0.341   0.340 0.328    45863\n",
      " 2020        12 0.010      0.359   0.346 0.319    45863\n",
      " 2020        12 0.020      0.465   0.335 0.296    45863\n",
      " 2022         1 0.005      0.462   0.336 0.292    28340\n",
      " 2022         1 0.010      0.309   0.333 0.321    28340\n",
      " 2022         1 0.020      0.329   0.333 0.331    28340\n",
      " 2022         2 0.005      0.439   0.345 0.290    28339\n",
      " 2022         2 0.010      0.287   0.333 0.309    28339\n",
      " 2022         2 0.020      0.321   0.333 0.327    28339\n",
      " 2022         4 0.005      0.399   0.360 0.312    28337\n",
      " 2022         4 0.010      0.251   0.333 0.286    28337\n",
      " 2022         4 0.020      0.306   0.333 0.319    28337\n",
      " 2022        12 0.005      0.347   0.344 0.336    28329\n",
      " 2022        12 0.010      0.364   0.361 0.330    28329\n",
      " 2022        12 0.020      0.296   0.335 0.307    28329\n"
     ]
    }
   ],
   "source": [
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d978de8",
   "metadata": {},
   "source": [
    "### 📊 Quick RF Baseline Results (macro-averaged)\n",
    "\n",
    "| Year | HorizonH |   Bin   | Precision | Recall |   F1   | Samples |\n",
    "|------|----------|---------|-----------|--------|--------|---------|\n",
    "| 2016 |        1 | 0.005   |     0.807 |  0.335 |  0.301 |   80654 |\n",
    "| 2016 |        1 | 0.010   |     0.315 |  0.333 |  0.324 |   80654 |\n",
    "| 2016 |        1 | 0.020   |     0.330 |  0.333 |  0.332 |   80654 |\n",
    "| 2016 |        2 | 0.005   |     0.425 |  0.338 |  0.287 |   80653 |\n",
    "| 2016 |        2 | 0.010   |     0.629 |  0.334 |  0.314 |   80653 |\n",
    "| 2016 |        2 | 0.020   |     0.325 |  0.333 |  0.329 |   80653 |\n",
    "| 2016 |        4 | 0.005   |     0.409 |  0.378 |  0.353 |   80651 |\n",
    "| 2016 |        4 | 0.010   |     0.414 |  0.334 |  0.297 |   80651 |\n",
    "| 2016 |        4 | 0.020   |     0.312 |  0.333 |  0.322 |   80651 |\n",
    "| 2016 |       12 | 0.005   |     0.395 |  0.374 |  0.339 |   80643 |\n",
    "| 2016 |       12 | 0.010   |     0.369 |  0.366 |  0.341 |   80643 |\n",
    "| 2016 |       12 | 0.020   |     0.396 |  0.335 |  0.305 |   80643 |\n",
    "| 2018 |        1 | 0.005   |     0.592 |  0.335 |  0.295 |   63355 |\n",
    "| 2018 |        1 | 0.010   |     0.311 |  0.333 |  0.322 |   63355 |\n",
    "| 2018 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   63355 |\n",
    "| 2018 |        2 | 0.005   |     0.448 |  0.339 |  0.282 |   63354 |\n",
    "| 2018 |        2 | 0.010   |     0.624 |  0.334 |  0.311 |   63354 |\n",
    "| 2018 |        2 | 0.020   |     0.323 |  0.333 |  0.328 |   63354 |\n",
    "| 2018 |        4 | 0.005   |     0.391 |  0.368 |  0.338 |   63352 |\n",
    "| 2018 |        4 | 0.010   |     0.422 |  0.334 |  0.291 |   63352 |\n",
    "| 2018 |        4 | 0.020   |     0.309 |  0.333 |  0.321 |   63352 |\n",
    "| 2018 |       12 | 0.005   |     0.363 |  0.346 |  0.279 |   63344 |\n",
    "| 2018 |       12 | 0.010   |     0.352 |  0.352 |  0.320 |   63344 |\n",
    "| 2018 |       12 | 0.020   |     0.379 |  0.334 |  0.298 |   63344 |\n",
    "| 2020 |        1 | 0.005   |     0.388 |  0.335 |  0.292 |   45874 |\n",
    "| 2020 |        1 | 0.010   |     0.310 |  0.333 |  0.321 |   45874 |\n",
    "| 2020 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   45874 |\n",
    "| 2020 |        2 | 0.005   |     0.449 |  0.344 |  0.290 |   45873 |\n",
    "| 2020 |        2 | 0.010   |     0.287 |  0.333 |  0.309 |   45873 |\n",
    "| 2020 |        2 | 0.020   |     0.322 |  0.333 |  0.327 |   45873 |\n",
    "| 2020 |        4 | 0.005   |     0.415 |  0.377 |  0.346 |   45871 |\n",
    "| 2020 |        4 | 0.010   |     0.418 |  0.334 |  0.287 |   45871 |\n",
    "| 2020 |        4 | 0.020   |     0.307 |  0.333 |  0.320 |   45871 |\n",
    "| 2020 |       12 | 0.005   |     0.341 |  0.340 |  0.328 |   45863 |\n",
    "| 2020 |       12 | 0.010   |     0.359 |  0.346 |  0.319 |   45863 |\n",
    "| 2020 |       12 | 0.020   |     0.465 |  0.335 |  0.296 |   45863 |\n",
    "| 2022 |        1 | 0.005   |     0.462 |  0.336 |  0.292 |   28340 |\n",
    "| 2022 |        1 | 0.010   |     0.309 |  0.333 |  0.321 |   28340 |\n",
    "| 2022 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   28340 |\n",
    "| 2022 |        2 | 0.005   |     0.439 |  0.345 |  0.290 |   28339 |\n",
    "| 2022 |        2 | 0.010   |     0.287 |  0.333 |  0.309 |   28339 |\n",
    "| 2022 |        2 | 0.020   |     0.321 |  0.333 |  0.327 |   28339 |\n",
    "| 2022 |        4 | 0.005   |     0.399 |  0.360 |  0.312 |   28337 |\n",
    "| 2022 |        4 | 0.010   |     0.251 |  0.333 |  0.286 |   28337 |\n",
    "| 2022 |        4 | 0.020   |     0.306 |  0.333 |  0.319 |   28337 |\n",
    "| 2022 |       12 | 0.005   |     0.347 |  0.344 |  0.336 |   28329 |\n",
    "| 2022 |       12 | 0.010   |     0.364 |  0.361 |  0.330 |   28329 |\n",
    "| 2022 |       12 | 0.020   |     0.296 |  0.335 |  0.307 |   28329 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Hold-out classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.276     0.238     0.256      3249\n",
      "           1      0.609     0.840     0.706      9224\n",
      "           2      0.341     0.057     0.098      3658\n",
      "\n",
      "    accuracy                          0.541     16131\n",
      "   macro avg      0.409     0.378     0.353     16131\n",
      "weighted avg      0.481     0.541     0.478     16131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── CONFIG & IMPORTS ─────────────────────────────\n",
    "import warnings, shap, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # prevent GPU errors for SHAP\n",
    "\n",
    "# ────────────────────────────── CONFIG  ─────────────────────────────────────\n",
    "DATA_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "HORIZON   = 4\n",
    "BINS      = [-np.inf, -0.005, 0.005, np.inf]\n",
    "TEST_SIZE = 0.2\n",
    "RAND_SEED = 42\n",
    "\n",
    "RF_KWARGS = dict(\n",
    "    n_estimators = 200,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED\n",
    ")\n",
    "\n",
    "# ─────────────────────── LOAD + LABEL ──────────────────────────────────────\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "# ─────────────────────── TRAIN/TEST SPLIT ──────────────────────────────────\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ─────────────────────── MODEL SETUP + TRAIN ───────────────────────────────\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "prep = ColumnTransformer([(\"num\", \"passthrough\", num_cols)], remainder=\"drop\", verbose_feature_names_out=False)]]\n",
    "\n",
    "model = Pipeline([(\"prep\", prep), (\"rf\", RandomForestClassifier(**RF_KWARGS))])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📊 Hold-out classification report:\")\n",
    "print(classification_report(y_test, model.predict(X_test), digits=3, zero_division=0))\n",
    "\n",
    "# ─────────────────────── SHAP ON 200 SAMPLES PER YEAR ──────────────────────\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "sampled_df = pd.concat([\n",
    "    df[df[\"year\"] == year].sample(n=200, random_state=RAND_SEED)\n",
    "    for year in df[\"year\"].unique()\n",
    "    if len(df[df[\"year\"] == year]) >= 200\n",
    "])\n",
    "\n",
    "X_shap = sampled_df.drop(columns=[\"date\", \"rel_ret\", \"y\", \"year\"])\n",
    "prepped_X = model.named_steps[\"prep\"].transform(X_shap)\n",
    "rf_model  = model.named_steps[\"rf\"]\n",
    "\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(prepped_X)\n",
    "\n",
    "print(f\"✅ SHAP input shape: {prepped_X.shape}, Classes: {len(shap_values)}\")\n",
    "\n",
    "# Summary plot — comment this if you don’t want graph immediately\n",
    "shap.summary_plot(shap_values, features=X_shap, feature_names=num_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558a9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 20 % hold-out metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.255     0.601     0.358      3249\n",
      "           1      0.711     0.529     0.607      9224\n",
      "           2      0.311     0.136     0.189      3658\n",
      "\n",
      "    accuracy                          0.454     16131\n",
      "   macro avg      0.426     0.422     0.385     16131\n",
      "weighted avg      0.528     0.454     0.462     16131\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ╭─ CONFIG ─╮\n",
    "DATA_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "HORIZON = 4\n",
    "BIN_WIDTH = 0.005\n",
    "TEST_SIZE = 0.20\n",
    "RAND_SEED = 42\n",
    "MAX_TRAIN_ROWS = 100_000\n",
    "SHAP_PER_YEAR = 120\n",
    "\n",
    "RF_KWARGS = dict(\n",
    "    n_estimators=100, max_depth=12, max_features=\"sqrt\", class_weight=\"balanced\",\n",
    "    max_samples=0.6, n_jobs=-1, random_state=RAND_SEED\n",
    ")\n",
    "\n",
    "# ─── LOAD & LABEL ───\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "if len(df) > MAX_TRAIN_ROWS:\n",
    "    df = df.iloc[:: int(len(df) / MAX_TRAIN_ROWS)]\n",
    "\n",
    "bins = [-np.inf, -BIN_WIDTH, BIN_WIDTH, np.inf]\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"] = pd.cut(df[\"rel_ret\"], bins=bins, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ─── PIPELINE ───\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "prep = ColumnTransformer([(\"num\", \"passthrough\", num_cols)], remainder=\"drop\", verbose_feature_names_out=False)\n",
    "model = Pipeline([(\"prep\", prep), (\"rf\", RandomForestClassifier(**RF_KWARGS))])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📊 20 % hold-out metrics:\")\n",
    "print(classification_report(y_test, model.predict(X_test), digits=3, zero_division=0))\n",
    "\n",
    "# ─── LIGHT SHAP SAMPLE ───\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"y_lbl\"] = y\n",
    "samples = []\n",
    "for yr, g in df.groupby(\"year\", observed=True):\n",
    "    n = min(SHAP_PER_YEAR, len(g))\n",
    "    g_sample = (g.groupby(\"y_lbl\", group_keys=False, observed=True)\n",
    "                  .apply(lambda d: resample(d, n_samples=int(n * len(d) / len(g)),\n",
    "                                            replace=False, random_state=RAND_SEED)))\n",
    "    samples.append(g_sample)\n",
    "\n",
    "X_shap_df = pd.concat(samples).drop(columns=[\"date\", \"rel_ret\", \"y_lbl\", \"year\"])\n",
    "prepped_X = model.named_steps[\"prep\"].transform(X_shap_df)\n",
    "\n",
    "feat_names = model.named_steps[\"prep\"].get_feature_names_out()\n",
    "rf_model = model.named_steps[\"rf\"]\n",
    "\n",
    "# Force CPU-only SHAP explainability\n",
    "explainer = shap.TreeExplainer(rf_model, feature_perturbation=\"tree_path_dependent\", approximate=True)\n",
    "shap_list = explainer.shap_values(prepped_X, check_additivity=False)\n",
    "\n",
    "# ─── GLOBAL IMPORTANCE BAR PLOT ───\n",
    "mean_abs = np.mean([np.abs(s) for s in shap_list], axis=0)\n",
    "global_importance = np.mean(mean_abs, axis=0)\n",
    "sorted_idx = np.argsort(global_importance)[::-1]\n",
    "sorted_names = np.array(feat_names)[sorted_idx]\n",
    "sorted_values = global_importance[sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_names[:20][::-1], sorted_values[:20][::-1])\n",
    "plt.xlabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Top 20 Global Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c5fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊  Hold-out (20 %) macro report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.273     0.226     0.247      3249\n",
      "           1      0.608     0.848     0.708      9224\n",
      "           2      0.340     0.053     0.091      3658\n",
      "\n",
      "    accuracy                          0.542     16131\n",
      "   macro avg      0.407     0.376     0.349     16131\n",
      "weighted avg      0.480     0.542     0.475     16131\n",
      "\n",
      "\n",
      "🔍 Combined importance ranking (every feature):\n",
      "         Feature  Gini rank  Gini_score  Perm rank  Perm_score\n",
      "              tr          4     0.05405          1      0.0242\n",
      "          atr_14          3     0.05884          2     0.02261\n",
      "      Volume BTC          5     0.05348          3     0.02009\n",
      "            high         17     0.04176          4     0.01238\n",
      "      band_width          1     0.06914          5    0.009333\n",
      "price_above_ma50          6     0.04952          6    0.007758\n",
      "           close         15     0.04243          7    0.007549\n",
      "             low         19     0.04133          8    0.006551\n",
      "          boll_b         18     0.04155          9    0.006342\n",
      "    lower_shadow         21     0.04025         10    0.004556\n",
      "            open         16     0.04224         11    0.003963\n",
      "     macd_signal          9      0.0472         12    0.003402\n",
      "         roc_24h          8     0.04818         13    0.002415\n",
      "       macd_line         11     0.04525         14    0.001334\n",
      "       roc_7days          2     0.05924         15    0.001301\n",
      "    upper_shadow         20     0.04027         16   0.0005053\n",
      "   vol_ratio_24h         10     0.04563         17   0.0001707\n",
      "    ret_over_atr         14      0.0432         18  -0.0003266\n",
      "            body         13     0.04369         19   -0.001194\n",
      "          roc_4h         12     0.04383         20    -0.00254\n",
      "       macd_diff          7     0.04892         21   -0.003411\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── CONFIG ──────────────────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "DATA_PATH  = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "                  r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "\n",
    "HORIZON    = 4                        # hours forward you want to predict\n",
    "BINS       = [-np.inf, -0.005, 0.005, np.inf]   # three-way ↓ / flat / ↑\n",
    "TEST_SIZE  = 0.20                     # last 20 % = test\n",
    "RAND_SEED  = 42\n",
    "\n",
    "RF_PARAMS  = dict(                    # quick but decent forest\n",
    "    n_estimators = 300,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED,\n",
    ")\n",
    "# ───────────────────── LOAD & LABEL ──────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "# ───────────────────── CHRONOLOGICAL SPLIT ──────────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────────────────── TRAIN & REPORT ───────────────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n📊  Hold-out ({int(TEST_SIZE*100)} %) macro report:\")\n",
    "print(classification_report(y_test, rf.predict(X_test),\n",
    "                            digits=3, zero_division=0))\n",
    "\n",
    "# ───────────────────── FEATURE IMPORTANCE ───────────────\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "# 1) Gini / impurity importance\n",
    "gini_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Gini_score\": rf.feature_importances_})\n",
    "              .sort_values(\"Gini_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "gini_imp[\"Gini rank\"] = np.arange(1, len(gini_imp)+1)\n",
    "\n",
    "# 2) Permutation importance (macro-F1 drop)\n",
    "perm = permutation_importance(\n",
    "           rf, X_test, y_test,\n",
    "           scoring=make_scorer(f1_score, average=\"macro\"),\n",
    "           n_repeats=5, random_state=RAND_SEED, n_jobs=-1)\n",
    "perm_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Perm_score\": perm.importances_mean})\n",
    "              .sort_values(\"Perm_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "perm_imp[\"Perm rank\"] = np.arange(1, len(perm_imp)+1)\n",
    "\n",
    "# 3) Merge the two views into one dashboard\n",
    "full_imp = (gini_imp.merge(perm_imp, on=\"Feature\", how=\"outer\")\n",
    "                     .loc[:, [\"Feature\",\n",
    "                              \"Gini rank\", \"Gini_score\",\n",
    "                              \"Perm rank\", \"Perm_score\"]]\n",
    "                     .sort_values(\"Perm rank\")\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "# ───────────────────── DISPLAY ALL FEATURES ─────────────\n",
    "pd.set_option(\"display.max_rows\", None,\n",
    "              \"display.float_format\", lambda v: f\"{v:10.4g}\")\n",
    "print(\"\\n🔍 Combined importance ranking (every feature):\")\n",
    "print(full_imp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊  Hold-out (20 %) macro report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.272     0.176     0.213      3249\n",
      "           1      0.606     0.874     0.716      9224\n",
      "           2      0.381     0.075     0.126      3658\n",
      "\n",
      "    accuracy                          0.552     16131\n",
      "   macro avg      0.420     0.375     0.352     16131\n",
      "weighted avg      0.488     0.552     0.481     16131\n",
      "\n",
      "\n",
      "🔍 Combined importance ranking (every feature):\n",
      "         Feature  Gini rank  Gini_score  Perm rank  Perm_score\n",
      "              tr          5     0.06286          1     0.03223\n",
      "          atr_14          4     0.06718          2     0.02571\n",
      "      Volume BTC          6     0.06219          3     0.02408\n",
      "      band_width          1     0.08162          4     0.02247\n",
      "price_above_ma50          7     0.05936          5     0.01435\n",
      "          boll_b         15     0.04908          6     0.00738\n",
      "     macd_signal         10     0.05798          7    0.006785\n",
      "    ret_over_atr         14     0.05071          8    0.005919\n",
      "    lower_shadow         17      0.0476          9     0.00545\n",
      "          roc_4h         13     0.05203         10    0.005335\n",
      "         roc_24h          8     0.05827         11    0.005319\n",
      "       roc_7days          2     0.07034         12    0.004179\n",
      "    upper_shadow         16     0.04803         13    0.002108\n",
      "            body         12     0.05234         14   0.0009089\n",
      "       macd_diff          9     0.05807         15    -0.00201\n",
      "   vol_ratio_24h         11      0.0546         16   -0.002105\n",
      "           close          3     0.06775         17   -0.009653\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── CONFIG ──────────────────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "DATA_PATH  = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "                  r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "\n",
    "HORIZON    = 4                        # hours forward you want to predict\n",
    "BINS       = [-np.inf, -0.005, 0.005, np.inf]   # three-way ↓ / flat / ↑\n",
    "TEST_SIZE  = 0.20                     # last 20 % = test\n",
    "RAND_SEED  = 40\n",
    "\n",
    "RF_PARAMS  = dict(\n",
    "    n_estimators = 300,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED,\n",
    ")\n",
    "\n",
    "# ───────────────────── LOAD & LABEL ──────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "# Drop open, high, low\n",
    "drop_cols = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\",'macd_line']\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "# ───────────────────── CHRONOLOGICAL SPLIT ──────────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────────────────── TRAIN & REPORT ───────────────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n📊  Hold-out ({int(TEST_SIZE*100)} %) macro report:\")\n",
    "print(classification_report(y_test, rf.predict(X_test),\n",
    "                            digits=3, zero_division=0))\n",
    "\n",
    "# ───────────────────── FEATURE IMPORTANCE ───────────────\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "# 1) Gini importance\n",
    "gini_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Gini_score\": rf.feature_importances_})\n",
    "              .sort_values(\"Gini_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "gini_imp[\"Gini rank\"] = np.arange(1, len(gini_imp)+1)\n",
    "\n",
    "# 2) Permutation importance\n",
    "perm = permutation_importance(\n",
    "           rf, X_test, y_test,\n",
    "           scoring=make_scorer(f1_score, average=\"macro\"),\n",
    "           n_repeats=5, random_state=RAND_SEED, n_jobs=-1)\n",
    "perm_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Perm_score\": perm.importances_mean})\n",
    "              .sort_values(\"Perm_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "perm_imp[\"Perm rank\"] = np.arange(1, len(perm_imp)+1)\n",
    "\n",
    "# 3) Combine\n",
    "full_imp = (gini_imp.merge(perm_imp, on=\"Feature\", how=\"outer\")\n",
    "                     .loc[:, [\"Feature\",\n",
    "                              \"Gini rank\", \"Gini_score\",\n",
    "                              \"Perm rank\", \"Perm_score\"]]\n",
    "                     .sort_values(\"Perm rank\")\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "# ───────────────────── DISPLAY ALL FEATURES ─────────────\n",
    "pd.set_option(\"display.max_rows\", None,\n",
    "              \"display.float_format\", lambda v: f\"{v:10.4g}\")\n",
    "print(\"\\n🔍 Combined importance ranking (every feature):\")\n",
    "print(full_imp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f8505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fbd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa0424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58671572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 200\n",
      "max_resources_: 1200\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 600\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      " Best CV F0.5: 0.5623\n",
      "Best hyper-parameters:\n",
      " {'bootstrap': False, 'class_weight': 'balanced', 'max_depth': 57, 'max_features': np.float64(0.5282172483721086), 'min_samples_leaf': 5, 'min_samples_split': 6, 'n_estimators': 600}\n",
      "\n",
      " Hold-out report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.255     0.474     0.331      3249\n",
      "           1      0.675     0.596     0.633      9224\n",
      "           2      0.303     0.160     0.209      3658\n",
      "\n",
      "    accuracy                          0.473     16131\n",
      "   macro avg      0.411     0.410     0.391     16131\n",
      "weighted avg      0.506     0.473     0.476     16131\n",
      "\n",
      "Weighted F0.5 on hold-out: 0.49061008861455113\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fast hyper-parameter sweep for RandomForestClassifier\n",
    "   • successive-halving on n_estimators   (200 → 600 → 1200)\n",
    "   • optimises weighted F0.5\n",
    "   • ~10 min on 4 logical cores\n",
    "\"\"\"\n",
    "\n",
    "# ────────── imports ──────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv   # noqa: F401\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ────────── data (same as before) ──────────\n",
    "DATA_PATH = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "\n",
    "HORIZON  = 4\n",
    "BINS     = [-np.inf, -0.005, 0.005, np.inf]\n",
    "TEST_SZ  = 0.20\n",
    "SEED     = 40\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df = (df.dropna(subset=[\"y\", \"rel_ret\"]).reset_index(drop=True))\n",
    "df[\"y\"] = df[\"y\"].astype(\"int8\")\n",
    "\n",
    "drop_cols = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "split  = int(len(df) * (1 - TEST_SZ))\n",
    "X_tr, X_te = X.iloc[:split], X.iloc[split:]\n",
    "y_tr, y_te = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ────────── precision-heavy scorer ──────────\n",
    "f05_weighted = make_scorer(\n",
    "    fbeta_score, beta=0.5, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "# ────────── parameter distributions  (NOTICE: no n_estimators) ──────────\n",
    "param_dist = {\n",
    "    \"max_depth\":         randint(5, 60),\n",
    "    \"max_features\":      uniform(0.2, 0.8),\n",
    "    \"min_samples_split\": randint(2, 16),\n",
    "    \"min_samples_leaf\":  randint(1, 6),\n",
    "    \"bootstrap\":         [True, False],\n",
    "    \"class_weight\":      [\"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "# ────────── halving random search ──────────\n",
    "base_rf = RandomForestClassifier(\n",
    "    n_jobs=-1, random_state=SEED\n",
    ")\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator           = base_rf,\n",
    "    param_distributions = param_dist,\n",
    "    scoring             = f05_weighted,\n",
    "    resource            = \"n_estimators\",   # RF will grow more trees each stage\n",
    "    min_resources       = 200,\n",
    "    max_resources       = 1200,\n",
    "    factor              = 3,\n",
    "    cv                  = StratifiedKFold(\n",
    "                              n_splits=3, shuffle=True, random_state=SEED),\n",
    "    random_state        = SEED,\n",
    "    n_jobs              = -1,\n",
    "    verbose             = 1,\n",
    ")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "search.fit(X_tr, y_tr)\n",
    "\n",
    "print(f\"\\n Best CV F0.5: {search.best_score_:6.4f}\")\n",
    "print(\"Best hyper-parameters:\\n\", search.best_params_)\n",
    "\n",
    "# ────────── evaluate on unseen data ──────────\n",
    "best_rf = search.best_estimator_\n",
    "y_pred  = best_rf.predict(X_te)\n",
    "\n",
    "print(\"\\n Hold-out report:\")\n",
    "print(classification_report(y_te, y_pred, digits=3, zero_division=0))\n",
    "print(\"Weighted F0.5 on hold-out:\",\n",
    "      fbeta_score(y_te, y_pred, beta=0.5,\n",
    "                  average=\"weighted\", zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d85f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 200\n",
      "max_resources_: 1200\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 600\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "  Best CV weighted-F0.5: 0.6095\n",
      "Best hyper-parameters:\n",
      " {'bootstrap': False, 'class_weight': 'balanced', 'max_depth': 57, 'max_features': np.float64(0.5282172483721086), 'min_samples_leaf': 5, 'min_samples_split': 6, 'n_estimators': 600}\n",
      "\n",
      " Hold-out classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.684     0.578      7808\n",
      "           1      0.548     0.360     0.434      8323\n",
      "\n",
      "    accuracy                          0.517     16131\n",
      "   macro avg      0.524     0.522     0.506     16131\n",
      "weighted avg      0.525     0.517     0.504     16131\n",
      "\n",
      "Weighted F0.5 on hold-out: 0.5118808150874548\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary-direction model (price ↑ vs ↓) with a quick HalvingRandomSearchCV\n",
    "-----------------------------------------------------------------------\n",
    "• Target: will the close price be HIGHER (class 1) or LOWER/UNCHANGED\n",
    "  (class 0) after HORIZON bars?\n",
    "• Optimises weighted F0.5 (precision 4× recall).\n",
    "• Same 10-min budget on a 4-core machine.\n",
    "\"\"\"\n",
    "\n",
    "# ────────── imports ──────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv   # noqa: F401\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ────────── paths & basic params ──────────\n",
    "DATA_PATH = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "\n",
    "HORIZON  = 4          # bars ahead\n",
    "TEST_SZ  = 0.20\n",
    "SEED     = 40\n",
    "\n",
    "# ────────── load & label ──────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "# class 1 = price up (or exactly flat), class 0 = price down\n",
    "df[\"y\"] = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "\n",
    "# remove bars without a future price (last HORIZON rows)\n",
    "df = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "drop_cols = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "# ────────── chronological split ──────────\n",
    "cut = int(len(df) * (1 - TEST_SZ))\n",
    "X_tr, X_te = X.iloc[:cut], X.iloc[cut:]\n",
    "y_tr, y_te = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ────────── precision-heavy scorer ──────────\n",
    "f05_weighted = make_scorer(\n",
    "    fbeta_score, beta=0.5, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "# ────────── parameter distributions (no n_estimators!) ──────────\n",
    "param_dist = {\n",
    "    \"max_depth\":         randint(5, 60),\n",
    "    \"max_features\":      uniform(0.2, 0.8),\n",
    "    \"min_samples_split\": randint(2, 16),\n",
    "    \"min_samples_leaf\":  randint(1, 6),\n",
    "    \"bootstrap\":         [True, False],\n",
    "    \"class_weight\":      [\"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "# ────────── halving random search ──────────\n",
    "base_rf = RandomForestClassifier(\n",
    "    n_jobs=-1, random_state=SEED\n",
    ")\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator           = base_rf,\n",
    "    param_distributions = param_dist,\n",
    "    scoring             = f05_weighted,\n",
    "    resource            = \"n_estimators\",      # 200 → 600 → 1200\n",
    "    min_resources       = 200,\n",
    "    max_resources       = 1200,\n",
    "    factor              = 3,\n",
    "    cv                  = StratifiedKFold(\n",
    "                              n_splits=3, shuffle=True, random_state=SEED),\n",
    "    random_state        = SEED,\n",
    "    n_jobs              = -1,\n",
    "    verbose             = 1,\n",
    ")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "search.fit(X_tr, y_tr)\n",
    "\n",
    "print(f\"\\n  Best CV weighted-F0.5: {search.best_score_:6.4f}\")\n",
    "print(\"Best hyper-parameters:\\n\", search.best_params_)\n",
    "\n",
    "# ────────── evaluate on hold-out ──────────\n",
    "best_rf = search.best_estimator_\n",
    "y_pred  = best_rf.predict(X_te)\n",
    "\n",
    "print(\"\\n Hold-out classification report:\")\n",
    "print(classification_report(y_te, y_pred, digits=3, zero_division=0))\n",
    "print(\"Weighted F0.5 on hold-out:\",\n",
    "      fbeta_score(y_te, y_pred, beta=0.5,\n",
    "                  average=\"weighted\", zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3791562",
   "metadata": {},
   "source": [
    "# Final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d122ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──── Run: best_halving ────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.502     0.685     0.579      7808\n",
      "           1      0.551     0.363     0.437      8323\n",
      "\n",
      "    accuracy                          0.518     16131\n",
      "   macro avg      0.526     0.524     0.508     16131\n",
      "weighted avg      0.527     0.518     0.506     16131\n",
      "\n",
      "Weighted F0.5: 0.5140337715316639\n",
      "\n",
      "──── Run: light_forest ────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.504     0.679     0.578      7808\n",
      "           1      0.553     0.372     0.445      8323\n",
      "\n",
      "    accuracy                          0.521     16131\n",
      "   macro avg      0.528     0.526     0.512     16131\n",
      "weighted avg      0.529     0.521     0.510     16131\n",
      "\n",
      "Weighted F0.5: 0.517125161427882\n",
      "\n",
      "──── Run: deep_bal_sub ────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.501     0.623     0.555      7808\n",
      "           1      0.542     0.418     0.472      8323\n",
      "\n",
      "    accuracy                          0.517     16131\n",
      "   macro avg      0.521     0.520     0.514     16131\n",
      "weighted avg      0.522     0.517     0.512     16131\n",
      "\n",
      "Weighted F0.5: 0.5162560187583244\n"
     ]
    }
   ],
   "source": [
    "# ░░░░░░░░░░░░░░  RANDOM-FOREST BTC DIRECTION MODEL  ░░░░░░░░░░░░░░\n",
    "#   • labels: 1 = price ≥ current close after HORIZON bars, else 0\n",
    "#   • metric: weighted F0.5  (precision ×4 recall)\n",
    "#   • runs:   any number of hyper-param dicts in RUNS at the bottom\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ────────────────  GLOBAL CONFIG  ────────────────\n",
    "DATA_PATH = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "HORIZON    = 4           # bars ahead for the target\n",
    "TEST_SIZE  = 0.20        # 20 % hold-out (chronological split)\n",
    "RAND_SEED  = 42\n",
    "\n",
    "# columns we never feed to the model\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# precision-heavy scorer for later CV use\n",
    "F05_WEIGHTED = make_scorer(\n",
    "    fbeta_score, beta=0.5, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "# ────────────────  DATA PIPELINE  ────────────────\n",
    "def load_data(path: Path, horizon: int):\n",
    "    \"\"\"Return X, y, keeping chronological order.\"\"\"\n",
    "    df = (pd.read_csv(path, parse_dates=[\"date\"])\n",
    "            .sort_values(\"date\"))\n",
    "\n",
    "    # future return\n",
    "    df[\"rel_ret\"] = (df[\"close\"].shift(-horizon) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "    # binary target: up/flat = 1, down = 0\n",
    "    df[\"y\"] = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "\n",
    "    # drop rows without future price (last `horizon`)\n",
    "    df = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "    X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "    y = df[\"y\"]\n",
    "    return X, y\n",
    "\n",
    "def chrono_split(X, y, test_frac: float):\n",
    "    \"\"\"Chronological train/hold-out split.\"\"\"\n",
    "    cut = int(len(X) * (1 - test_frac))\n",
    "    return X.iloc[:cut], X.iloc[cut:], y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "def eval_on_holdout(model, X_te, y_te, *, label: str):\n",
    "    y_pred = model.predict(X_te)\n",
    "    f05 = fbeta_score(y_te, y_pred, beta=0.5,\n",
    "                      average=\"weighted\", zero_division=0)\n",
    "    print(f\"\\n──── Run: {label} ────\")\n",
    "    print(classification_report(y_te, y_pred, digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5:\", f05)\n",
    "\n",
    "# ────────────────  TRAINING LOOP  ────────────────\n",
    "def run_forest(X_tr, y_tr, X_te, y_te, params: dict, label: str):\n",
    "    rf = RandomForestClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=RAND_SEED,\n",
    "            **params\n",
    "    ).fit(X_tr, y_tr)\n",
    "    eval_on_holdout(rf, X_te, y_te, label=label)\n",
    "\n",
    "# ────────────────  MAIN  ────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(RAND_SEED)\n",
    "\n",
    "    X, y = load_data(DATA_PATH, HORIZON)\n",
    "    X_tr, X_te, y_tr, y_te = chrono_split(X, y, TEST_SIZE)\n",
    "\n",
    "    # ---- DEFINE THE RUNS YOU WANT TO TRY -------------------------\n",
    "    RUNS = {\n",
    "        \"best_halving\": {          # ← your optimal set\n",
    "            \"bootstrap\": False,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"max_depth\": 57,\n",
    "            \"max_features\": 0.5282172483721086,\n",
    "            \"min_samples_leaf\": 5,\n",
    "            \"min_samples_split\": 6,\n",
    "            \"n_estimators\": 600\n",
    "        },\n",
    "        # Example alt-runs (edit / extend as you like) -------------\n",
    "        \"light_forest\": {          # fewer trees, shallower\n",
    "            \"bootstrap\": True,\n",
    "            \"class_weight\": None,\n",
    "            \"max_depth\": 25,\n",
    "            \"max_features\": 0.6,\n",
    "            \"min_samples_leaf\": 2,\n",
    "            \"min_samples_split\": 4,\n",
    "            \"n_estimators\": 300\n",
    "        },\n",
    "        \"deep_bal_sub\": {          # deeper & subsample weighting\n",
    "            \"bootstrap\": False,\n",
    "            \"class_weight\": \"balanced_subsample\",\n",
    "            \"max_depth\": 80,\n",
    "            \"max_features\": 0.4,\n",
    "            \"min_samples_leaf\": 1,\n",
    "            \"min_samples_split\": 2,\n",
    "            \"n_estimators\": 800\n",
    "        },\n",
    "    }\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    for label, params in RUNS.items():\n",
    "        run_forest(X_tr, y_tr, X_te, y_te, params, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confident rows kept (p ≥ 0.6): 3421 / 16131 (21.2%)\n",
      "Rows dropped: 12710\n",
      "\n",
      "📊  Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.517     0.818     0.634      1694\n",
      "           1      0.584     0.251     0.351      1727\n",
      "\n",
      "    accuracy                          0.532      3421\n",
      "   macro avg      0.551     0.534     0.492      3421\n",
      "weighted avg      0.551     0.532     0.491      3421\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.50953407611152\n"
     ]
    }
   ],
   "source": [
    "# ░░░  LIGHT-FOREST – PROBABILITY FILTER VERSION  ░░░0.6 2016\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.60           # probability confidence cut\n",
    "\n",
    "BEST_PARAMS = dict(          # “light_forest”\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── DATA ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "df            = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "cut = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**BEST_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba      = rf.predict_proba(X_test)\n",
    "best_prob  = proba.max(axis=1)\n",
    "y_pred_all = proba.argmax(axis=1)\n",
    "\n",
    "mask       = best_prob >= THRESH          # rows we keep\n",
    "kept       = mask.sum()\n",
    "dropped    = len(mask) - kept\n",
    "\n",
    "print(f\"\\nConfident rows kept (p ≥ {THRESH}): {kept} / {len(mask)} \"\n",
    "      f\"({kept/len(mask):.1%})\")\n",
    "print(f\"Rows dropped: {dropped}\")\n",
    "\n",
    "# Evaluate only on confident predictions\n",
    "y_true_conf  = y_test[mask]\n",
    "y_pred_conf  = y_pred_all[mask]\n",
    "\n",
    "if kept:\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"\\n Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"\\nNo rows met the confidence threshold – nothing to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2b66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confident rows kept (p ≥ 0.6): 2490 / 12671 (19.7%)\n",
      "Rows dropped: 10181\n",
      "\n",
      " Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.505     0.816     0.623      1204\n",
      "           1      0.592     0.250     0.352      1286\n",
      "\n",
      "    accuracy                          0.524      2490\n",
      "   macro avg      0.548     0.533     0.488      2490\n",
      "weighted avg      0.550     0.524     0.483      2490\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.5043289177377158\n"
     ]
    }
   ],
   "source": [
    "# ░░░  LIGHT-FOREST – PROBABILITY FILTER VERSION  ░░░ 0.6  2018\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2018_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.60           # probability confidence cut\n",
    "\n",
    "BEST_PARAMS = dict(          # “light_forest”\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── DATA ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "df            = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "cut = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**BEST_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba      = rf.predict_proba(X_test)\n",
    "best_prob  = proba.max(axis=1)\n",
    "y_pred_all = proba.argmax(axis=1)\n",
    "\n",
    "mask       = best_prob >= THRESH          # rows we keep\n",
    "kept       = mask.sum()\n",
    "dropped    = len(mask) - kept\n",
    "\n",
    "print(f\"\\nConfident rows kept (p ≥ {THRESH}): {kept} / {len(mask)} \"\n",
    "      f\"({kept/len(mask):.1%})\")\n",
    "print(f\"Rows dropped: {dropped}\")\n",
    "\n",
    "# Evaluate only on confident predictions\n",
    "y_true_conf  = y_test[mask]\n",
    "y_pred_conf  = y_pred_all[mask]\n",
    "\n",
    "if kept:\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"\\n Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"\\nNo rows met the confidence threshold – nothing to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d348d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confident rows kept (p ≥ 0.6): 1578 / 9175 (17.2%)\n",
      "Rows dropped: 7597\n",
      "\n",
      " Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.523     0.683     0.592       763\n",
      "           1      0.583     0.416     0.486       815\n",
      "\n",
      "    accuracy                          0.545      1578\n",
      "   macro avg      0.553     0.549     0.539      1578\n",
      "weighted avg      0.554     0.545     0.537      1578\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.5440060898294097\n"
     ]
    }
   ],
   "source": [
    "# ░░░  LIGHT-FOREST – PROBABILITY FILTER VERSION  ░░░ 0.6  2020\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2020_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.60           # probability confidence cut\n",
    "\n",
    "BEST_PARAMS = dict(          # “light_forest”\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── DATA ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "df            = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "cut = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**BEST_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba      = rf.predict_proba(X_test)\n",
    "best_prob  = proba.max(axis=1)\n",
    "y_pred_all = proba.argmax(axis=1)\n",
    "\n",
    "mask       = best_prob >= THRESH          # rows we keep\n",
    "kept       = mask.sum()\n",
    "dropped    = len(mask) - kept\n",
    "\n",
    "print(f\"\\nConfident rows kept (p ≥ {THRESH}): {kept} / {len(mask)} \"\n",
    "      f\"({kept/len(mask):.1%})\")\n",
    "print(f\"Rows dropped: {dropped}\")\n",
    "\n",
    "# Evaluate only on confident predictions\n",
    "y_true_conf  = y_test[mask]\n",
    "y_pred_conf  = y_pred_all[mask]\n",
    "\n",
    "if kept:\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"\\n Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"\\nNo rows met the confidence threshold – nothing to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f894f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confident rows kept (p ≥ 0.6): 1578 / 9175 (17.2%)\n",
      "Rows dropped: 7597\n",
      "\n",
      " Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.523     0.683     0.592       763\n",
      "           1      0.583     0.416     0.486       815\n",
      "\n",
      "    accuracy                          0.545      1578\n",
      "   macro avg      0.553     0.549     0.539      1578\n",
      "weighted avg      0.554     0.545     0.537      1578\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.5440060898294097\n"
     ]
    }
   ],
   "source": [
    "# ░░░  LIGHT-FOREST – PROBABILITY FILTER VERSION  ░░░ 0.6  2022\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2020_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.60           # probability confidence cut\n",
    "\n",
    "BEST_PARAMS = dict(          # “light_forest”\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── DATA ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "df            = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "cut = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**BEST_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba      = rf.predict_proba(X_test)\n",
    "best_prob  = proba.max(axis=1)\n",
    "y_pred_all = proba.argmax(axis=1)\n",
    "\n",
    "mask       = best_prob >= THRESH          # rows we keep\n",
    "kept       = mask.sum()\n",
    "dropped    = len(mask) - kept\n",
    "\n",
    "print(f\"\\nConfident rows kept (p ≥ {THRESH}): {kept} / {len(mask)} \"\n",
    "      f\"({kept/len(mask):.1%})\")\n",
    "print(f\"Rows dropped: {dropped}\")\n",
    "\n",
    "# Evaluate only on confident predictions\n",
    "y_true_conf  = y_test[mask]\n",
    "y_pred_conf  = y_pred_all[mask]\n",
    "\n",
    "if kept:\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"\\n Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"\\nNo rows met the confidence threshold – nothing to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93b405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confident rows kept (p ≥ 0.7): 109 / 16131 (0.7%)\n",
      "Rows dropped: 16022\n",
      "\n",
      " Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.539     0.857     0.662        56\n",
      "           1      0.600     0.226     0.329        53\n",
      "\n",
      "    accuracy                          0.550       109\n",
      "   macro avg      0.570     0.542     0.495       109\n",
      "weighted avg      0.569     0.550     0.500       109\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.5186342537558826\n"
     ]
    }
   ],
   "source": [
    "# ░░░  LIGHT-FOREST – PROBABILITY FILTER VERSION  ░░░ 0.7 2016\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.7          # probability confidence cut\n",
    "\n",
    "BEST_PARAMS = dict(          # “light_forest”\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── DATA ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = (df[\"rel_ret\"] >= 0).astype(\"int8\")\n",
    "df            = df.dropna(subset=[\"rel_ret\"]).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "cut = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**BEST_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba      = rf.predict_proba(X_test)\n",
    "best_prob  = proba.max(axis=1)\n",
    "y_pred_all = proba.argmax(axis=1)\n",
    "\n",
    "mask       = best_prob >= THRESH          # rows we keep\n",
    "kept       = mask.sum()\n",
    "dropped    = len(mask) - kept\n",
    "\n",
    "print(f\"\\nConfident rows kept (p ≥ {THRESH}): {kept} / {len(mask)} \"\n",
    "      f\"({kept/len(mask):.1%})\")\n",
    "print(f\"Rows dropped: {dropped}\")\n",
    "\n",
    "# Evaluate only on confident predictions\n",
    "y_true_conf  = y_test[mask]\n",
    "y_pred_conf  = y_pred_all[mask]\n",
    "\n",
    "if kept:\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"\\n Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"\\nNo rows met the confidence threshold – nothing to report.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8488d9b",
   "metadata": {},
   "source": [
    "# with bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b0bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows kept with p ≥ 0.60: 2760/16131  (17.1%)\n",
      "Rows dropped: 13371\n",
      "\n",
      "📊  Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       268\n",
      "           1      0.790     1.000     0.883      2178\n",
      "           2      0.667     0.006     0.013       314\n",
      "\n",
      "    accuracy                          0.789      2760\n",
      "   macro avg      0.486     0.335     0.298      2760\n",
      "weighted avg      0.699     0.789     0.698      2760\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.6543221261151448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random-Forest model, ±0.5 % 3-class target\n",
    "-----------------------------------------------------------------\n",
    "• Classes:\n",
    "      0  →  next-return < −0.50 %\n",
    "      1  →  −0.50 % ≤ next-return < +0.50 %\n",
    "      2  →  next-return ≥ +0.50 %\n",
    "• Evaluates only the rows whose predicted-class probability ≥ THRESH.\n",
    "• Reports how many rows were kept / dropped and the weighted-F0.5.\n",
    "\"\"\"\n",
    "\n",
    "# ───────── imports ─────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "BINS        = [-np.inf, -0.005, 0.005, np.inf]   # ±0.5 %\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.60                               # confidence cut-off\n",
    "\n",
    "# “light-forest” hyper-parameters  (change if you prefer another set)\n",
    "RF_PARAMS = dict(\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── LOAD & LABEL ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "df[\"y\"] = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df = df.dropna(subset=[\"y\", \"rel_ret\"]).reset_index(drop=True)\n",
    "df[\"y\"] = df[\"y\"].astype(\"int8\")        # 0,1,2\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "# ───────── CHRONOLOGICAL SPLIT ─────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba        = rf.predict_proba(X_test)\n",
    "best_prob    = proba.max(axis=1)          # highest class prob per row\n",
    "pred_classes = proba.argmax(axis=1)\n",
    "\n",
    "mask         = best_prob >= THRESH\n",
    "kept_rows    = int(mask.sum())\n",
    "total_rows   = len(mask)\n",
    "dropped_rows = total_rows - kept_rows\n",
    "\n",
    "print(f\"\\nRows kept with p ≥ {THRESH:.2f}: {kept_rows}/{total_rows}\"\n",
    "      f\"  ({kept_rows/total_rows:.1%})\")\n",
    "print(f\"Rows dropped: {dropped_rows}\")\n",
    "\n",
    "# ───────── EVALUATE on confident rows ─────────\n",
    "if kept_rows:\n",
    "    y_true_conf = y_test[mask]\n",
    "    y_pred_conf = pred_classes[mask]\n",
    "\n",
    "    print(\"\\n  Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"No test rows exceeded the probability threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5397c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows kept with p ≥ 0.50: 5205/16131  (32.3%)\n",
      "Rows dropped: 10926\n",
      "\n",
      "  Classification report (confident rows):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.221     0.053     0.085       643\n",
      "           1      0.738     0.980     0.842      3795\n",
      "           2      0.500     0.008     0.015       767\n",
      "\n",
      "    accuracy                          0.722      5205\n",
      "   macro avg      0.486     0.347     0.314      5205\n",
      "weighted avg      0.639     0.722     0.627      5205\n",
      "\n",
      "Weighted F0.5 (confident rows): 0.5883185563655046\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random-Forest model, ±0.5 % 3-class target\n",
    "-----------------------------------------------------------------\n",
    "• Classes:\n",
    "      0  →  next-return < −0.50 %\n",
    "      1  →  −0.50 % ≤ next-return < +0.50 %\n",
    "      2  →  next-return ≥ +0.50 %\n",
    "• Evaluates only the rows whose predicted-class probability ≥ THRESH.\n",
    "• Reports how many rows were kept / dropped and the weighted-F0.5.\n",
    "\"\"\"\n",
    "\n",
    "# ───────── imports ─────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "DATA_PATH  = Path(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "    r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "    r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\"\n",
    ")\n",
    "HORIZON     = 4\n",
    "BINS        = [-np.inf, -0.005, 0.005, np.inf]   # ±0.5 %\n",
    "TEST_SIZE   = 0.20\n",
    "RAND_SEED   = 42\n",
    "THRESH      = 0.50                               # confidence cut-off\n",
    "\n",
    "# “light-forest” hyper-parameters  (change if you prefer another set)\n",
    "RF_PARAMS = dict(\n",
    "    bootstrap          = True,\n",
    "    class_weight       = None,\n",
    "    max_depth          = 25,\n",
    "    max_features       = 0.6,\n",
    "    min_samples_leaf   = 2,\n",
    "    min_samples_split  = 4,\n",
    "    n_estimators       = 300,\n",
    "    n_jobs             = -1,\n",
    "    random_state       = RAND_SEED,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\", \"macd_line\"]\n",
    "\n",
    "# ───────── LOAD & LABEL ─────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = (pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\"))\n",
    "\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "df[\"y\"] = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df = df.dropna(subset=[\"y\", \"rel_ret\"]).reset_index(drop=True)\n",
    "df[\"y\"] = df[\"y\"].astype(\"int8\")        # 0,1,2\n",
    "\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "y = df[\"y\"]\n",
    "\n",
    "# ───────── CHRONOLOGICAL SPLIT ─────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────── TRAIN ─────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS).fit(X_train, y_train)\n",
    "\n",
    "# ───────── PROBABILITY FILTER ─────────\n",
    "proba        = rf.predict_proba(X_test)\n",
    "best_prob    = proba.max(axis=1)          # highest class prob per row\n",
    "pred_classes = proba.argmax(axis=1)\n",
    "\n",
    "mask         = best_prob >= THRESH\n",
    "kept_rows    = int(mask.sum())\n",
    "total_rows   = len(mask)\n",
    "dropped_rows = total_rows - kept_rows\n",
    "\n",
    "print(f\"\\nRows kept with p ≥ {THRESH:.2f}: {kept_rows}/{total_rows}\"\n",
    "      f\"  ({kept_rows/total_rows:.1%})\")\n",
    "print(f\"Rows dropped: {dropped_rows}\")\n",
    "\n",
    "# ───────── EVALUATE on confident rows ─────────\n",
    "if kept_rows:\n",
    "    y_true_conf = y_test[mask]\n",
    "    y_pred_conf = pred_classes[mask]\n",
    "\n",
    "    print(\"\\n  Classification report (confident rows):\")\n",
    "    print(classification_report(y_true_conf, y_pred_conf,\n",
    "                                digits=3, zero_division=0))\n",
    "\n",
    "    f05 = fbeta_score(y_true_conf, y_pred_conf,\n",
    "                      beta=0.5, average=\"weighted\", zero_division=0)\n",
    "    print(\"Weighted F0.5 (confident rows):\", f05)\n",
    "else:\n",
    "    print(\"No test rows exceeded the probability threshold.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae3faa",
   "metadata": {},
   "source": [
    "| Year | Threshold | Confident Rows | Total Rows | % Kept | Weighted F0.5 | Precision (Class 0) | Precision (Class 1) |\n",
    "|------|-----------|----------------|------------|--------|----------------|----------------------|----------------------|\n",
    "| 2020 | 0.60      | 1578           | 9175       | 17.2%  | 0.5440         | 0.523                | 0.583                |\n",
    "| 2022 | 0.60      | 1578           | 9175       | 17.2%  | 0.5440         | 0.523                | 0.583                |\n",
    "| 2016 | 0.70      | 109            | 16131      | 0.7%   | 0.5186         | 0.539                | 0.600                |\n",
    "| 2016 | 0.60      | 3421           | 16131      | 21.2%  | 0.5095         | 0.517                | 0.584                |\n",
    "| 2018 | 0.60      | 2490           | 12671      | 19.7%  | 0.5043         | 0.505                | 0.592                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c495c6",
   "metadata": {},
   "source": [
    "| Year | Threshold | Confident Rows | Total Rows | % Kept | Weighted F0.5 | Precision (Class 0) | Precision (Class 1) | Precision (Class 2) |\n",
    "|------|-----------|----------------|------------|--------|----------------|----------------------|----------------------|----------------------|\n",
    "| 2016 | 0.60      | 2760           | 16131      | 17.1%  | **0.6543**     | 0.000                | 0.790                | 0.667                |\n",
    "| 2016 | 0.50      | 5205           | 16131      | 32.3%  | 0.5883         | 0.221                | 0.738                | 0.500                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7e081",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb047d5a",
   "metadata": {},
   "source": [
    " 2016 0.5\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0      0.504     0.679     0.578      7808\n",
    "           1      0.553     0.372     0.445      8323\n",
    "\n",
    "    accuracy                          0.521     16131\n",
    "   macro avg      0.528     0.526     0.512     16131\n",
    "weighted avg      0.529     0.521     0.510     16131"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
