{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a84382f",
   "metadata": {},
   "source": [
    "# In this notebook we will train the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb96deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffeba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Quick RF baseline results (macro-averaged) ###\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m\n\u001b[0;32m     70\u001b[0m summary \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m     71\u001b[0m              \u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHorizonH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBin\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     72\u001b[0m              \u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Quick RF baseline results (macro-averaged) ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:2983\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[1;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2981\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtablefmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2982\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[1;32m-> 2983\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtabulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2984\u001b[0m result \u001b[38;5;241m=\u001b[39m tabulate\u001b[38;5;241m.\u001b[39mtabulate(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\.venv\\lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# ──────────────────── sweep_RF_baselines.py ───────────────────────────────\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- ❶ parameters to sweep ----------\n",
    "YEARS      = [2016,2018,2020,2022]          # add/remove years you have\n",
    "HORIZONS   = [1, 2, 4, 12]               # hours\n",
    "BIN_WIDTHS = [0.005, 0.01, 0.02]         # 0.5 %, 1 %, 2 %\n",
    "TEST_SIZE  = 0.20\n",
    "RAND_SEED  = 42\n",
    "BASE_DIR   = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\")\n",
    "\n",
    "RF_KWARGS  = dict(\n",
    "    n_estimators = 200,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED,\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "# ---------- ❷ sweep loops ----------\n",
    "for yr in YEARS:\n",
    "    file = BASE_DIR / f\"gemini_btc_data_final_version_with_features_{yr}_final.csv\"\n",
    "    if not file.exists():\n",
    "        print(f\"⚠️  {file.name} not found – skipped\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    for H in HORIZONS:\n",
    "        # label once per horizon, then reuse for the three bin widths\n",
    "        df_h = df.copy()\n",
    "        df_h[\"rel_ret\"] = (df_h[\"close\"].shift(-H) - df_h[\"close\"]) / df_h[\"close\"]\n",
    "\n",
    "        for b in BIN_WIDTHS:\n",
    "            bins = [-np.inf, -b, b, np.inf]\n",
    "            df_lab = df_h.copy()\n",
    "            df_lab[\"y\"] = pd.cut(df_lab[\"rel_ret\"], bins=bins,\n",
    "                                 labels=False, right=False)\n",
    "            df_lab.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "            X = df_lab.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "            y = df_lab[\"y\"].astype(int)\n",
    "\n",
    "            split = int(len(df_lab) * (1 - TEST_SIZE))\n",
    "            X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "            y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "            rf = RandomForestClassifier(**RF_KWARGS).fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "\n",
    "            p, r, f, _ = precision_recall_fscore_support(\n",
    "                y_test, y_pred, average=\"macro\", zero_division=0\n",
    "            )\n",
    "\n",
    "            results.append(dict(Year=yr, HorizonH=H, Bin=b,\n",
    "                                Precision=round(p,3),\n",
    "                                Recall=round(r,3),\n",
    "                                F1=round(f,3),\n",
    "                                Samples=len(df_lab)))\n",
    "\n",
    "# ---------- ❸ summary table ----------\n",
    "summary = (pd.DataFrame(results)\n",
    "             .sort_values([\"Year\", \"HorizonH\", \"Bin\"])\n",
    "             .reset_index(drop=True))\n",
    "print(\"\\n### Quick RF baseline results (macro-averaged) ###\")\n",
    "print(summary.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e71fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Year  HorizonH   Bin  Precision  Recall    F1  Samples\n",
      " 2016         1 0.005      0.807   0.335 0.301    80654\n",
      " 2016         1 0.010      0.315   0.333 0.324    80654\n",
      " 2016         1 0.020      0.330   0.333 0.332    80654\n",
      " 2016         2 0.005      0.425   0.338 0.287    80653\n",
      " 2016         2 0.010      0.629   0.334 0.314    80653\n",
      " 2016         2 0.020      0.325   0.333 0.329    80653\n",
      " 2016         4 0.005      0.409   0.378 0.353    80651\n",
      " 2016         4 0.010      0.414   0.334 0.297    80651\n",
      " 2016         4 0.020      0.312   0.333 0.322    80651\n",
      " 2016        12 0.005      0.395   0.374 0.339    80643\n",
      " 2016        12 0.010      0.369   0.366 0.341    80643\n",
      " 2016        12 0.020      0.396   0.335 0.305    80643\n",
      " 2018         1 0.005      0.592   0.335 0.295    63355\n",
      " 2018         1 0.010      0.311   0.333 0.322    63355\n",
      " 2018         1 0.020      0.329   0.333 0.331    63355\n",
      " 2018         2 0.005      0.448   0.339 0.282    63354\n",
      " 2018         2 0.010      0.624   0.334 0.311    63354\n",
      " 2018         2 0.020      0.323   0.333 0.328    63354\n",
      " 2018         4 0.005      0.391   0.368 0.338    63352\n",
      " 2018         4 0.010      0.422   0.334 0.291    63352\n",
      " 2018         4 0.020      0.309   0.333 0.321    63352\n",
      " 2018        12 0.005      0.363   0.346 0.279    63344\n",
      " 2018        12 0.010      0.352   0.352 0.320    63344\n",
      " 2018        12 0.020      0.379   0.334 0.298    63344\n",
      " 2020         1 0.005      0.388   0.335 0.292    45874\n",
      " 2020         1 0.010      0.310   0.333 0.321    45874\n",
      " 2020         1 0.020      0.329   0.333 0.331    45874\n",
      " 2020         2 0.005      0.449   0.344 0.290    45873\n",
      " 2020         2 0.010      0.287   0.333 0.309    45873\n",
      " 2020         2 0.020      0.322   0.333 0.327    45873\n",
      " 2020         4 0.005      0.415   0.377 0.346    45871\n",
      " 2020         4 0.010      0.418   0.334 0.287    45871\n",
      " 2020         4 0.020      0.307   0.333 0.320    45871\n",
      " 2020        12 0.005      0.341   0.340 0.328    45863\n",
      " 2020        12 0.010      0.359   0.346 0.319    45863\n",
      " 2020        12 0.020      0.465   0.335 0.296    45863\n",
      " 2022         1 0.005      0.462   0.336 0.292    28340\n",
      " 2022         1 0.010      0.309   0.333 0.321    28340\n",
      " 2022         1 0.020      0.329   0.333 0.331    28340\n",
      " 2022         2 0.005      0.439   0.345 0.290    28339\n",
      " 2022         2 0.010      0.287   0.333 0.309    28339\n",
      " 2022         2 0.020      0.321   0.333 0.327    28339\n",
      " 2022         4 0.005      0.399   0.360 0.312    28337\n",
      " 2022         4 0.010      0.251   0.333 0.286    28337\n",
      " 2022         4 0.020      0.306   0.333 0.319    28337\n",
      " 2022        12 0.005      0.347   0.344 0.336    28329\n",
      " 2022        12 0.010      0.364   0.361 0.330    28329\n",
      " 2022        12 0.020      0.296   0.335 0.307    28329\n"
     ]
    }
   ],
   "source": [
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d978de8",
   "metadata": {},
   "source": [
    "### 📊 Quick RF Baseline Results (macro-averaged)\n",
    "\n",
    "| Year | HorizonH |   Bin   | Precision | Recall |   F1   | Samples |\n",
    "|------|----------|---------|-----------|--------|--------|---------|\n",
    "| 2016 |        1 | 0.005   |     0.807 |  0.335 |  0.301 |   80654 |\n",
    "| 2016 |        1 | 0.010   |     0.315 |  0.333 |  0.324 |   80654 |\n",
    "| 2016 |        1 | 0.020   |     0.330 |  0.333 |  0.332 |   80654 |\n",
    "| 2016 |        2 | 0.005   |     0.425 |  0.338 |  0.287 |   80653 |\n",
    "| 2016 |        2 | 0.010   |     0.629 |  0.334 |  0.314 |   80653 |\n",
    "| 2016 |        2 | 0.020   |     0.325 |  0.333 |  0.329 |   80653 |\n",
    "| 2016 |        4 | 0.005   |     0.409 |  0.378 |  0.353 |   80651 |\n",
    "| 2016 |        4 | 0.010   |     0.414 |  0.334 |  0.297 |   80651 |\n",
    "| 2016 |        4 | 0.020   |     0.312 |  0.333 |  0.322 |   80651 |\n",
    "| 2016 |       12 | 0.005   |     0.395 |  0.374 |  0.339 |   80643 |\n",
    "| 2016 |       12 | 0.010   |     0.369 |  0.366 |  0.341 |   80643 |\n",
    "| 2016 |       12 | 0.020   |     0.396 |  0.335 |  0.305 |   80643 |\n",
    "| 2018 |        1 | 0.005   |     0.592 |  0.335 |  0.295 |   63355 |\n",
    "| 2018 |        1 | 0.010   |     0.311 |  0.333 |  0.322 |   63355 |\n",
    "| 2018 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   63355 |\n",
    "| 2018 |        2 | 0.005   |     0.448 |  0.339 |  0.282 |   63354 |\n",
    "| 2018 |        2 | 0.010   |     0.624 |  0.334 |  0.311 |   63354 |\n",
    "| 2018 |        2 | 0.020   |     0.323 |  0.333 |  0.328 |   63354 |\n",
    "| 2018 |        4 | 0.005   |     0.391 |  0.368 |  0.338 |   63352 |\n",
    "| 2018 |        4 | 0.010   |     0.422 |  0.334 |  0.291 |   63352 |\n",
    "| 2018 |        4 | 0.020   |     0.309 |  0.333 |  0.321 |   63352 |\n",
    "| 2018 |       12 | 0.005   |     0.363 |  0.346 |  0.279 |   63344 |\n",
    "| 2018 |       12 | 0.010   |     0.352 |  0.352 |  0.320 |   63344 |\n",
    "| 2018 |       12 | 0.020   |     0.379 |  0.334 |  0.298 |   63344 |\n",
    "| 2020 |        1 | 0.005   |     0.388 |  0.335 |  0.292 |   45874 |\n",
    "| 2020 |        1 | 0.010   |     0.310 |  0.333 |  0.321 |   45874 |\n",
    "| 2020 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   45874 |\n",
    "| 2020 |        2 | 0.005   |     0.449 |  0.344 |  0.290 |   45873 |\n",
    "| 2020 |        2 | 0.010   |     0.287 |  0.333 |  0.309 |   45873 |\n",
    "| 2020 |        2 | 0.020   |     0.322 |  0.333 |  0.327 |   45873 |\n",
    "| 2020 |        4 | 0.005   |     0.415 |  0.377 |  0.346 |   45871 |\n",
    "| 2020 |        4 | 0.010   |     0.418 |  0.334 |  0.287 |   45871 |\n",
    "| 2020 |        4 | 0.020   |     0.307 |  0.333 |  0.320 |   45871 |\n",
    "| 2020 |       12 | 0.005   |     0.341 |  0.340 |  0.328 |   45863 |\n",
    "| 2020 |       12 | 0.010   |     0.359 |  0.346 |  0.319 |   45863 |\n",
    "| 2020 |       12 | 0.020   |     0.465 |  0.335 |  0.296 |   45863 |\n",
    "| 2022 |        1 | 0.005   |     0.462 |  0.336 |  0.292 |   28340 |\n",
    "| 2022 |        1 | 0.010   |     0.309 |  0.333 |  0.321 |   28340 |\n",
    "| 2022 |        1 | 0.020   |     0.329 |  0.333 |  0.331 |   28340 |\n",
    "| 2022 |        2 | 0.005   |     0.439 |  0.345 |  0.290 |   28339 |\n",
    "| 2022 |        2 | 0.010   |     0.287 |  0.333 |  0.309 |   28339 |\n",
    "| 2022 |        2 | 0.020   |     0.321 |  0.333 |  0.327 |   28339 |\n",
    "| 2022 |        4 | 0.005   |     0.399 |  0.360 |  0.312 |   28337 |\n",
    "| 2022 |        4 | 0.010   |     0.251 |  0.333 |  0.286 |   28337 |\n",
    "| 2022 |        4 | 0.020   |     0.306 |  0.333 |  0.319 |   28337 |\n",
    "| 2022 |       12 | 0.005   |     0.347 |  0.344 |  0.336 |   28329 |\n",
    "| 2022 |       12 | 0.010   |     0.364 |  0.361 |  0.330 |   28329 |\n",
    "| 2022 |       12 | 0.020   |     0.296 |  0.335 |  0.307 |   28329 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Hold-out classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.276     0.238     0.256      3249\n",
      "           1      0.609     0.840     0.706      9224\n",
      "           2      0.341     0.057     0.098      3658\n",
      "\n",
      "    accuracy                          0.541     16131\n",
      "   macro avg      0.409     0.378     0.353     16131\n",
      "weighted avg      0.481     0.541     0.478     16131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── CONFIG & IMPORTS ─────────────────────────────\n",
    "import warnings, shap, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # prevent GPU errors for SHAP\n",
    "\n",
    "# ────────────────────────────── CONFIG  ─────────────────────────────────────\n",
    "DATA_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "HORIZON   = 4\n",
    "BINS      = [-np.inf, -0.005, 0.005, np.inf]\n",
    "TEST_SIZE = 0.2\n",
    "RAND_SEED = 42\n",
    "\n",
    "RF_KWARGS = dict(\n",
    "    n_estimators = 200,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED\n",
    ")\n",
    "\n",
    "# ─────────────────────── LOAD + LABEL ──────────────────────────────────────\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "# ─────────────────────── TRAIN/TEST SPLIT ──────────────────────────────────\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ─────────────────────── MODEL SETUP + TRAIN ───────────────────────────────\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "prep = ColumnTransformer([(\"num\", \"passthrough\", num_cols)], remainder=\"drop\", verbose_feature_names_out=False)]]\n",
    "\n",
    "model = Pipeline([(\"prep\", prep), (\"rf\", RandomForestClassifier(**RF_KWARGS))])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📊 Hold-out classification report:\")\n",
    "print(classification_report(y_test, model.predict(X_test), digits=3, zero_division=0))\n",
    "\n",
    "# ─────────────────────── SHAP ON 200 SAMPLES PER YEAR ──────────────────────\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "sampled_df = pd.concat([\n",
    "    df[df[\"year\"] == year].sample(n=200, random_state=RAND_SEED)\n",
    "    for year in df[\"year\"].unique()\n",
    "    if len(df[df[\"year\"] == year]) >= 200\n",
    "])\n",
    "\n",
    "X_shap = sampled_df.drop(columns=[\"date\", \"rel_ret\", \"y\", \"year\"])\n",
    "prepped_X = model.named_steps[\"prep\"].transform(X_shap)\n",
    "rf_model  = model.named_steps[\"rf\"]\n",
    "\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(prepped_X)\n",
    "\n",
    "print(f\"✅ SHAP input shape: {prepped_X.shape}, Classes: {len(shap_values)}\")\n",
    "\n",
    "# Summary plot — comment this if you don’t want graph immediately\n",
    "shap.summary_plot(shap_values, features=X_shap, feature_names=num_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558a9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 20 % hold-out metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.255     0.601     0.358      3249\n",
      "           1      0.711     0.529     0.607      9224\n",
      "           2      0.311     0.136     0.189      3658\n",
      "\n",
      "    accuracy                          0.454     16131\n",
      "   macro avg      0.426     0.422     0.385     16131\n",
      "weighted avg      0.528     0.454     0.462     16131\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ╭─ CONFIG ─╮\n",
    "DATA_PATH = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "HORIZON = 4\n",
    "BIN_WIDTH = 0.005\n",
    "TEST_SIZE = 0.20\n",
    "RAND_SEED = 42\n",
    "MAX_TRAIN_ROWS = 100_000\n",
    "SHAP_PER_YEAR = 120\n",
    "\n",
    "RF_KWARGS = dict(\n",
    "    n_estimators=100, max_depth=12, max_features=\"sqrt\", class_weight=\"balanced\",\n",
    "    max_samples=0.6, n_jobs=-1, random_state=RAND_SEED\n",
    ")\n",
    "\n",
    "# ─── LOAD & LABEL ───\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "if len(df) > MAX_TRAIN_ROWS:\n",
    "    df = df.iloc[:: int(len(df) / MAX_TRAIN_ROWS)]\n",
    "\n",
    "bins = [-np.inf, -BIN_WIDTH, BIN_WIDTH, np.inf]\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"] = pd.cut(df[\"rel_ret\"], bins=bins, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ─── PIPELINE ───\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "prep = ColumnTransformer([(\"num\", \"passthrough\", num_cols)], remainder=\"drop\", verbose_feature_names_out=False)\n",
    "model = Pipeline([(\"prep\", prep), (\"rf\", RandomForestClassifier(**RF_KWARGS))])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n📊 20 % hold-out metrics:\")\n",
    "print(classification_report(y_test, model.predict(X_test), digits=3, zero_division=0))\n",
    "\n",
    "# ─── LIGHT SHAP SAMPLE ───\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"y_lbl\"] = y\n",
    "samples = []\n",
    "for yr, g in df.groupby(\"year\", observed=True):\n",
    "    n = min(SHAP_PER_YEAR, len(g))\n",
    "    g_sample = (g.groupby(\"y_lbl\", group_keys=False, observed=True)\n",
    "                  .apply(lambda d: resample(d, n_samples=int(n * len(d) / len(g)),\n",
    "                                            replace=False, random_state=RAND_SEED)))\n",
    "    samples.append(g_sample)\n",
    "\n",
    "X_shap_df = pd.concat(samples).drop(columns=[\"date\", \"rel_ret\", \"y_lbl\", \"year\"])\n",
    "prepped_X = model.named_steps[\"prep\"].transform(X_shap_df)\n",
    "\n",
    "feat_names = model.named_steps[\"prep\"].get_feature_names_out()\n",
    "rf_model = model.named_steps[\"rf\"]\n",
    "\n",
    "# Force CPU-only SHAP explainability\n",
    "explainer = shap.TreeExplainer(rf_model, feature_perturbation=\"tree_path_dependent\", approximate=True)\n",
    "shap_list = explainer.shap_values(prepped_X, check_additivity=False)\n",
    "\n",
    "# ─── GLOBAL IMPORTANCE BAR PLOT ───\n",
    "mean_abs = np.mean([np.abs(s) for s in shap_list], axis=0)\n",
    "global_importance = np.mean(mean_abs, axis=0)\n",
    "sorted_idx = np.argsort(global_importance)[::-1]\n",
    "sorted_names = np.array(feat_names)[sorted_idx]\n",
    "sorted_values = global_importance[sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_names[:20][::-1], sorted_values[:20][::-1])\n",
    "plt.xlabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Top 20 Global Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c5fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊  Hold-out (20 %) macro report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.273     0.226     0.247      3249\n",
      "           1      0.608     0.848     0.708      9224\n",
      "           2      0.340     0.053     0.091      3658\n",
      "\n",
      "    accuracy                          0.542     16131\n",
      "   macro avg      0.407     0.376     0.349     16131\n",
      "weighted avg      0.480     0.542     0.475     16131\n",
      "\n",
      "\n",
      "🔍 Combined importance ranking (every feature):\n",
      "         Feature  Gini rank  Gini_score  Perm rank  Perm_score\n",
      "              tr          4     0.05405          1      0.0242\n",
      "          atr_14          3     0.05884          2     0.02261\n",
      "      Volume BTC          5     0.05348          3     0.02009\n",
      "            high         17     0.04176          4     0.01238\n",
      "      band_width          1     0.06914          5    0.009333\n",
      "price_above_ma50          6     0.04952          6    0.007758\n",
      "           close         15     0.04243          7    0.007549\n",
      "             low         19     0.04133          8    0.006551\n",
      "          boll_b         18     0.04155          9    0.006342\n",
      "    lower_shadow         21     0.04025         10    0.004556\n",
      "            open         16     0.04224         11    0.003963\n",
      "     macd_signal          9      0.0472         12    0.003402\n",
      "         roc_24h          8     0.04818         13    0.002415\n",
      "       macd_line         11     0.04525         14    0.001334\n",
      "       roc_7days          2     0.05924         15    0.001301\n",
      "    upper_shadow         20     0.04027         16   0.0005053\n",
      "   vol_ratio_24h         10     0.04563         17   0.0001707\n",
      "    ret_over_atr         14      0.0432         18  -0.0003266\n",
      "            body         13     0.04369         19   -0.001194\n",
      "          roc_4h         12     0.04383         20    -0.00254\n",
      "       macd_diff          7     0.04892         21   -0.003411\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── CONFIG ──────────────────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "DATA_PATH  = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "                  r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "\n",
    "HORIZON    = 4                        # hours forward you want to predict\n",
    "BINS       = [-np.inf, -0.005, 0.005, np.inf]   # three-way ↓ / flat / ↑\n",
    "TEST_SIZE  = 0.20                     # last 20 % = test\n",
    "RAND_SEED  = 42\n",
    "\n",
    "RF_PARAMS  = dict(                    # quick but decent forest\n",
    "    n_estimators = 300,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED,\n",
    ")\n",
    "# ───────────────────── LOAD & LABEL ──────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "# ───────────────────── CHRONOLOGICAL SPLIT ──────────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────────────────── TRAIN & REPORT ───────────────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n📊  Hold-out ({int(TEST_SIZE*100)} %) macro report:\")\n",
    "print(classification_report(y_test, rf.predict(X_test),\n",
    "                            digits=3, zero_division=0))\n",
    "\n",
    "# ───────────────────── FEATURE IMPORTANCE ───────────────\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "# 1) Gini / impurity importance\n",
    "gini_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Gini_score\": rf.feature_importances_})\n",
    "              .sort_values(\"Gini_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "gini_imp[\"Gini rank\"] = np.arange(1, len(gini_imp)+1)\n",
    "\n",
    "# 2) Permutation importance (macro-F1 drop)\n",
    "perm = permutation_importance(\n",
    "           rf, X_test, y_test,\n",
    "           scoring=make_scorer(f1_score, average=\"macro\"),\n",
    "           n_repeats=5, random_state=RAND_SEED, n_jobs=-1)\n",
    "perm_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Perm_score\": perm.importances_mean})\n",
    "              .sort_values(\"Perm_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "perm_imp[\"Perm rank\"] = np.arange(1, len(perm_imp)+1)\n",
    "\n",
    "# 3) Merge the two views into one dashboard\n",
    "full_imp = (gini_imp.merge(perm_imp, on=\"Feature\", how=\"outer\")\n",
    "                     .loc[:, [\"Feature\",\n",
    "                              \"Gini rank\", \"Gini_score\",\n",
    "                              \"Perm rank\", \"Perm_score\"]]\n",
    "                     .sort_values(\"Perm rank\")\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "# ───────────────────── DISPLAY ALL FEATURES ─────────────\n",
    "pd.set_option(\"display.max_rows\", None,\n",
    "              \"display.float_format\", lambda v: f\"{v:10.4g}\")\n",
    "print(\"\\n🔍 Combined importance ranking (every feature):\")\n",
    "print(full_imp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd0fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊  Hold-out (20 %) macro report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.264     0.153     0.193      3249\n",
      "           1      0.602     0.887     0.717      9224\n",
      "           2      0.382     0.068     0.115      3658\n",
      "\n",
      "    accuracy                          0.554     16131\n",
      "   macro avg      0.416     0.369     0.342     16131\n",
      "weighted avg      0.484     0.554     0.475     16131\n",
      "\n",
      "\n",
      "🔍 Combined importance ranking (every feature):\n",
      "         Feature  Gini rank  Gini_score  Perm rank  Perm_score\n",
      "              tr          5     0.06028          1     0.02796\n",
      "          atr_14          4     0.06411          2     0.02354\n",
      "      Volume BTC          6      0.0599          3     0.02294\n",
      "      band_width          1      0.0772          4      0.0204\n",
      "     macd_signal         11     0.05187          5     0.01058\n",
      "price_above_ma50          7     0.05632          6     0.01048\n",
      "          boll_b         16     0.04646          7    0.008672\n",
      "          roc_4h         14     0.04956          8      0.0064\n",
      "         roc_24h          8     0.05503          9     0.00455\n",
      "    lower_shadow         17     0.04614         10    0.004508\n",
      "    ret_over_atr         15     0.04845         11    0.003635\n",
      "            body         13     0.04965         12    0.003213\n",
      "    upper_shadow         18     0.04604         13    0.002111\n",
      "       roc_7days          2     0.06736         14    0.001818\n",
      "       macd_line         12     0.05071         15   9.822e-05\n",
      "   vol_ratio_24h         10     0.05201         16   -0.003327\n",
      "       macd_diff          9     0.05459         17   -0.005767\n",
      "           close          3     0.06433         18    -0.01162\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── CONFIG ──────────────────────\n",
    "from pathlib import Path\n",
    "import warnings, numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "DATA_PATH  = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "                  r\"\\Stock-Market-Prediction\\data\\processed\"\n",
    "                  r\"\\gemini_btc_data_final_version_with_features_2016_final.csv\")\n",
    "\n",
    "HORIZON    = 4                        # hours forward you want to predict\n",
    "BINS       = [-np.inf, -0.005, 0.005, np.inf]   # three-way ↓ / flat / ↑\n",
    "TEST_SIZE  = 0.20                     # last 20 % = test\n",
    "RAND_SEED  = 42\n",
    "\n",
    "RF_PARAMS  = dict(\n",
    "    n_estimators = 300,\n",
    "    max_depth    = None,\n",
    "    max_features = \"sqrt\",\n",
    "    class_weight = \"balanced\",\n",
    "    n_jobs       = -1,\n",
    "    random_state = RAND_SEED,\n",
    ")\n",
    "\n",
    "# ───────────────────── LOAD & LABEL ──────────────────────\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"]       = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "# Drop open, high, low\n",
    "drop_cols = [\"date\", \"rel_ret\", \"y\", \"open\", \"high\", \"low\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "# ───────────────────── CHRONOLOGICAL SPLIT ──────────────\n",
    "split = int(len(df) * (1 - TEST_SIZE))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# ───────────────────── TRAIN & REPORT ───────────────────\n",
    "rf = RandomForestClassifier(**RF_PARAMS)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n📊  Hold-out ({int(TEST_SIZE*100)} %) macro report:\")\n",
    "print(classification_report(y_test, rf.predict(X_test),\n",
    "                            digits=3, zero_division=0))\n",
    "\n",
    "# ───────────────────── FEATURE IMPORTANCE ───────────────\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "# 1) Gini importance\n",
    "gini_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Gini_score\": rf.feature_importances_})\n",
    "              .sort_values(\"Gini_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "gini_imp[\"Gini rank\"] = np.arange(1, len(gini_imp)+1)\n",
    "\n",
    "# 2) Permutation importance\n",
    "perm = permutation_importance(\n",
    "           rf, X_test, y_test,\n",
    "           scoring=make_scorer(f1_score, average=\"macro\"),\n",
    "           n_repeats=5, random_state=RAND_SEED, n_jobs=-1)\n",
    "perm_imp = (pd.DataFrame({\"Feature\": feat_names,\n",
    "                          \"Perm_score\": perm.importances_mean})\n",
    "              .sort_values(\"Perm_score\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "perm_imp[\"Perm rank\"] = np.arange(1, len(perm_imp)+1)\n",
    "\n",
    "# 3) Combine\n",
    "full_imp = (gini_imp.merge(perm_imp, on=\"Feature\", how=\"outer\")\n",
    "                     .loc[:, [\"Feature\",\n",
    "                              \"Gini rank\", \"Gini_score\",\n",
    "                              \"Perm rank\", \"Perm_score\"]]\n",
    "                     .sort_values(\"Perm rank\")\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "# ───────────────────── DISPLAY ALL FEATURES ─────────────\n",
    "pd.set_option(\"display.max_rows\", None,\n",
    "              \"display.float_format\", lambda v: f\"{v:10.4g}\")\n",
    "print(\"\\n🔍 Combined importance ranking (every feature):\")\n",
    "print(full_imp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86696e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👀 SHAP list shape(s): [(21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3), (21, 3)]\n",
      "❌ Unexpected SHAP format. Check shape of shap_list.\n"
     ]
    }
   ],
   "source": [
    "# Check what SHAP actually returned\n",
    "print(f\"\\n👀 SHAP list shape(s): {[np.array(s).shape for s in shap_list]}\")\n",
    "\n",
    "# Case 1: SHAP returned per-class arrays (expected shape: list of (n_samples, n_features))\n",
    "if isinstance(shap_list, list) and len(shap_list) == 3 and shap_list[0].ndim == 2:\n",
    "    class_contributions = {\n",
    "        f\"Class_{i}\": np.mean(np.abs(shap_list[i]), axis=0)\n",
    "        for i in range(len(shap_list))\n",
    "    }\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    contrib_df = pd.DataFrame(class_contributions, index=feat_names)\n",
    "    contrib_df[\"Global_Mean\"] = contrib_df.mean(axis=1)\n",
    "    contrib_df = contrib_df.sort_values(\"Global_Mean\", ascending=False)\n",
    "\n",
    "    print(\"\\n🔍 Top 15 Features by Class-wise SHAP Contributions:\")\n",
    "    print(contrib_df.head(15).round(5))\n",
    "\n",
    "# Case 2: SHAP returned a single 2D array (e.g., binary classification)\n",
    "elif isinstance(shap_list, np.ndarray) and shap_list.ndim == 2:\n",
    "    global_contrib = np.mean(np.abs(shap_list), axis=0)\n",
    "    contrib_df = pd.DataFrame({\n",
    "        \"Feature\": feat_names,\n",
    "        \"Mean |SHAP|\": global_contrib\n",
    "    }).sort_values(\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "    print(\"\\n🔍 SHAP Feature Importance (Binary classification):\")\n",
    "    print(contrib_df.head(15).round(5))\n",
    "\n",
    "else:\n",
    "    print(\"❌ Unexpected SHAP format. Check shape of shap_list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f8505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58671572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv   # noqa: E402  ★ add\n",
    "from sklearn.model_selection import HalvingRandomSearchCV   # now import works\n",
    "import numpy as np, pandas as pd, joblib, warnings\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────\n",
    "DATA_PATH   = Path(r\"...\\gemini_btc_data_final_version_with_features_2018.csv\")\n",
    "HORIZON     = 4\n",
    "BINS        = [-np.inf, -0.005, 0.005, np.inf]\n",
    "RAND_SEED   = 42\n",
    "MODEL_FILE  = \"rf_btc_classifier.joblib\"\n",
    "\n",
    "# ─── LOAD & LABEL (leak-free) ─────────────────────────────\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df[\"rel_ret\"] = (df[\"close\"].shift(-HORIZON) - df[\"close\"]) / df[\"close\"]\n",
    "df[\"y\"] = pd.cut(df[\"rel_ret\"], bins=BINS, labels=False, right=False)\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "\n",
    "X_all = df.drop(columns=[\"date\", \"rel_ret\", \"y\"])\n",
    "y_all = df[\"y\"].astype(int)\n",
    "\n",
    "# ─── ⇣⇣⇣  *make the split first*  ⇣⇣⇣ ─────────────────────\n",
    "split = int(len(df) * 0.8)\n",
    "X_train, X_test = X_all.iloc[:split], X_all.iloc[split:]\n",
    "y_train, y_test = y_all.iloc[:split], y_all.iloc[split:]\n",
    "\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "pre = ColumnTransformer(\n",
    "        [(\"num\", \"passthrough\", num_cols)],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False)\n",
    "\n",
    "# start with a tiny forest for the race\n",
    "rf_base = RandomForestClassifier(\n",
    "            n_estimators=50,        # sets min_resources\n",
    "            n_jobs=-1, random_state=RAND_SEED)\n",
    "\n",
    "pipe = Pipeline([(\"prep\", pre), (\"rf\", rf_base)])\n",
    "\n",
    "# ─── SUCCESSIVE-HALVING SEARCH ───────────────────────────\n",
    "param_dist = {\n",
    "    \"rf__max_depth\":      [None, 6, 12, 24],\n",
    "    \"rf__class_weight\":   [\"balanced\", \"balanced_subsample\"],\n",
    "    \"rf__max_features\":   [\"sqrt\", 0.3, 0.6],\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "            pipe, param_dist,\n",
    "            resource=\"rf__n_estimators\", max_resources=600, factor=3,\n",
    "            scoring=make_scorer(f1_score, average=\"macro\"),\n",
    "            cv=cv, random_state=RAND_SEED,\n",
    "            n_jobs=-1, verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(f\"Best CV macro-F1 : {search.best_score_:.3f}\")\n",
    "best = search.best_params_\n",
    "\n",
    "# ─── FINAL REFIT on TRAIN only ────────────────────────────\n",
    "best_core = {k.split(\"__\", 1)[1]: v for k, v in best.items()}\n",
    "best_core[\"n_estimators\"] = 1000      # upscale forest\n",
    "\n",
    "final_model = Pipeline([\n",
    "    (\"prep\", pre),\n",
    "    (\"rf\", RandomForestClassifier(**best_core,\n",
    "                                  n_jobs=-1, random_state=RAND_SEED))\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(final_model, MODEL_FILE)\n",
    "print(\"Model saved →\", MODEL_FILE)\n",
    "\n",
    "# ─── Hold-out report (never seen in tuning) ───────────────\n",
    "print(\"\\nHold-out 20 % report:\")\n",
    "print(classification_report(y_test,\n",
    "                            final_model.predict(X_test),\n",
    "                            digits=3, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
