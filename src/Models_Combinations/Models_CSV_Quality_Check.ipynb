{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61592290",
   "metadata": {},
   "source": [
    "# In this notebook we will check if the models csv fiels that we have are in a good quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14369b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª COLUMN CHECK:\n",
      "âœ… Columns match: catboost_trial57_predictions\n",
      "âœ… Columns match: cnn_predictions\n",
      "âœ… Columns match: gru_trial28_predictions\n",
      "âœ… Columns match: lgbm_predictions_formatted_backup\n",
      "âœ… Columns match: logisticreg_validation_predictions\n",
      "âœ… Columns match: lstm_test_predictions\n",
      "âœ… Columns match: RandomForest_predictions_custom_high_precision\n",
      "âœ… Columns match: TCN_Trial_36_predictions\n",
      "âœ… Columns match: xgboost_predictions_fixed\n",
      "\n",
      "âœ… All files have the same columns.\n",
      "\n",
      "ğŸ“… START DATES FROM 'timestamp':\n",
      "catboost_trial57_predictions: 2023-10-16 16:00:00\n",
      "cnn_predictions: 2023-10-20 16:00:00\n",
      "gru_trial28_predictions: 2024-03-15 12:00:00\n",
      "lgbm_predictions_formatted_backup: 2023-08-08 00:00:00\n",
      "logisticreg_validation_predictions: 2023-10-16 16:00:00\n",
      "lstm_test_predictions: 2023-10-20 12:00:00\n",
      "RandomForest_predictions_custom_high_precision: 2023-10-16 16:00:00\n",
      "TCN_Trial_36_predictions: 2023-10-18 16:00:00\n",
      "xgboost_predictions_fixed: 2023-05-07 12:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3180242638.py:30: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  start_dates[fname] = pd.to_datetime(df['timestamp']).min()\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3180242638.py:30: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  start_dates[fname] = pd.to_datetime(df['timestamp']).min()\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3180242638.py:30: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  start_dates[fname] = pd.to_datetime(df['timestamp']).min()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory containing the prediction CSVs\n",
    "base_path = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Final_runs_csv\")\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    \"catboost_trial57_predictions\",\n",
    "    \"cnn_predictions\",\n",
    "    \"gru_trial28_f05_preds\",\n",
    "    \"lgbm_predictions_formatted_backup\",\n",
    "    \"logisticreg_validation_predictions\",\n",
    "    \"lstm_test_predictions\",\n",
    "    \"RandomForest_predictions_custom_high_precision\",\n",
    "    \"TCN_Trial_36_predictions\",\n",
    "    \"xgboost_predictions_fixed\",\n",
    "    \"cnn_lstm_val_preds_20250614_142329\"\n",
    "]\n",
    "\n",
    "# Store columns and start dates\n",
    "column_sets = {}\n",
    "start_dates = {}\n",
    "\n",
    "for fname in file_names:\n",
    "    fpath = base_path / f\"{fname}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(fpath)\n",
    "        column_sets[fname] = list(df.columns)\n",
    "        if 'timestamp' in df.columns:\n",
    "            start_dates[fname] = pd.to_datetime(df['timestamp']).min()\n",
    "        else:\n",
    "            start_dates[fname] = \"âŒ No 'timestamp' column\"\n",
    "    except Exception as e:\n",
    "        column_sets[fname] = f\"âŒ Error reading file: {e}\"\n",
    "        start_dates[fname] = \"âŒ\"\n",
    "\n",
    "# Check column consistency\n",
    "print(\"\\nğŸ§ª COLUMN CHECK:\")\n",
    "first_cols = list(column_sets.values())[0]\n",
    "consistent = True\n",
    "for fname, cols in column_sets.items():\n",
    "    if cols != first_cols:\n",
    "        consistent = False\n",
    "        print(f\"âŒ Columns mismatch in: {fname}\")\n",
    "    else:\n",
    "        print(f\"âœ… Columns match: {fname}\")\n",
    "\n",
    "if consistent:\n",
    "    print(\"\\nâœ… All files have the same columns.\\n\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Not all files have matching columns.\\n\")\n",
    "\n",
    "# Print start dates\n",
    "print(\"ğŸ“… START DATES FROM 'timestamp':\")\n",
    "for fname, start in start_dates.items():\n",
    "    print(f\"{fname}: {start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a68722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE PREDICTION FILES ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ catboost_trial57_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,171\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 528 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ cnn_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,147\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-20 16:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 524 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ gru_trial28_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 2,266\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2024-03-15 12:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 377 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ lgbm_predictions_formatted_backup\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,589\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-08-08 00:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 598 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ logisticreg_validation_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,171\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 528 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ lstm_test_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,148\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-20 12:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 524 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ RandomForest_predictions_custom_high_precision\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,171\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-16 16:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 528 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ TCN_Trial_36_predictions\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 3,159\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-10-18 16:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 526 days\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“„ xgboost_predictions_fixed\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Successfully loaded\n",
      "   Rows: 4,144\n",
      "   Columns: ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
      "   Date range: 2023-05-07 12:00:00 to 2025-03-28 00:00:00\n",
      "   Duration: 690 days\n",
      "\n",
      "================================================================================\n",
      "COLUMN CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "âœ… All files have IDENTICAL column structure!\n",
      "   Columns: ['actual', 'prediction', 'prob_down', 'prob_up', 'timestamp', 'winning_prob']\n",
      "\n",
      "================================================================================\n",
      "DATE RANGE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“… Date Ranges (sorted by start date):\n",
      "File                                               Start Date           End Date             Days       Rows      \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "xgboost_predictions_fixed                          2023-05-07 12:00:00  2025-03-28 00:00:00  690        4,144     \n",
      "lgbm_predictions_formatted_backup                  2023-08-08 00:00:00  2025-03-28 00:00:00  598        3,589     \n",
      "catboost_trial57_predictions                       2023-10-16 16:00:00  2025-03-28 00:00:00  528        3,171     \n",
      "logisticreg_validation_predictions                 2023-10-16 16:00:00  2025-03-28 00:00:00  528        3,171     \n",
      "RandomForest_predictions_custom_high_precision     2023-10-16 16:00:00  2025-03-28 00:00:00  528        3,171     \n",
      "TCN_Trial_36_predictions                           2023-10-18 16:00:00  2025-03-28 00:00:00  526        3,159     \n",
      "lstm_test_predictions                              2023-10-20 12:00:00  2025-03-28 00:00:00  524        3,148     \n",
      "cnn_predictions                                    2023-10-20 16:00:00  2025-03-28 00:00:00  524        3,147     \n",
      "gru_trial28_predictions                            2024-03-15 12:00:00  2025-03-28 00:00:00  377        2,266     \n",
      "\n",
      "ğŸ“Š Common Date Range (overlap of all files):\n",
      "   Start: 2024-03-15 12:00:00\n",
      "   End: 2025-03-28 00:00:00\n",
      "   Duration: 377 days\n",
      "\n",
      "   Estimated rows in common range:\n",
      "     xgboost_predictions_fixed: 2,266 rows\n",
      "     lgbm_predictions_formatted_backup: 2,266 rows\n",
      "     catboost_trial57_predictions: 2,266 rows\n",
      "     logisticreg_validation_predictions: 2,266 rows\n",
      "     RandomForest_predictions_custom_high_precision: 2,266 rows\n",
      "     TCN_Trial_36_predictions: 2,266 rows\n",
      "     lstm_test_predictions: 2,266 rows\n",
      "     cnn_predictions: 2,266 rows\n",
      "     gru_trial28_predictions: 2,266 rows\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š File Status:\n",
      "   Total files: 9\n",
      "   âœ… Successful: 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS FOR ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "âœ… All files are ready for ensemble methods!\n",
      "   - Use date range: 2024-03-15 12:00:00 to 2025-03-28 00:00:00\n",
      "   - This ensures all models have predictions for the same period\n",
      "\n",
      "ğŸ’¾ Detailed report saved to: predictions_analysis_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:51: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:51: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:51: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:180: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:180: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_20216\\3314079376.py:180: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Directory containing the prediction CSVs\n",
    "base_path = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Final_runs_csv\")\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    \"catboost_trial57_predictions\",\n",
    "    \"cnn_predictions\",\n",
    "    \"gru_trial28_predictions\",\n",
    "    \"lgbm_predictions_formatted_backup\",\n",
    "    \"logisticreg_validation_predictions\",\n",
    "    \"lstm_test_predictions\",\n",
    "    \"RandomForest_predictions_custom_high_precision\",\n",
    "    \"TCN_Trial_36_predictions\",\n",
    "    \"xgboost_predictions_fixed\"\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "all_columns = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE PREDICTION FILES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process each file\n",
    "for file_name in file_names:\n",
    "    file_path = base_path / f\"{file_name}.csv\"\n",
    "    \n",
    "    print(f\"\\n{'â”€' * 60}\")\n",
    "    print(f\"ğŸ“„ {file_name}\")\n",
    "    print(f\"{'â”€' * 60}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get columns\n",
    "        columns = list(df.columns)\n",
    "        all_columns.append(set(columns))\n",
    "        \n",
    "        # Parse timestamp column\n",
    "        timestamp_col = 'timestamp'\n",
    "        if timestamp_col in df.columns:\n",
    "            # Try to parse timestamps\n",
    "            try:\n",
    "                df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "                \n",
    "                # Get date range\n",
    "                min_date = df[timestamp_col].min()\n",
    "                max_date = df[timestamp_col].max()\n",
    "                date_range_days = (max_date - min_date).days\n",
    "                \n",
    "                # Store results\n",
    "                results[file_name] = {\n",
    "                    'status': 'success',\n",
    "                    'columns': columns,\n",
    "                    'num_rows': len(df),\n",
    "                    'min_date': min_date,\n",
    "                    'max_date': max_date,\n",
    "                    'date_range_days': date_range_days,\n",
    "                    'has_nulls': df.isnull().any().any(),\n",
    "                    'null_counts': df.isnull().sum().to_dict() if df.isnull().any().any() else {}\n",
    "                }\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"âœ… Successfully loaded\")\n",
    "                print(f\"   Rows: {len(df):,}\")\n",
    "                print(f\"   Columns: {columns}\")\n",
    "                print(f\"   Date range: {min_date} to {max_date}\")\n",
    "                print(f\"   Duration: {date_range_days} days\")\n",
    "                if results[file_name]['has_nulls']:\n",
    "                    print(f\"   âš ï¸  Contains null values: {results[file_name]['null_counts']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[file_name] = {\n",
    "                    'status': 'timestamp_error',\n",
    "                    'error': str(e),\n",
    "                    'columns': columns,\n",
    "                    'num_rows': len(df)\n",
    "                }\n",
    "                print(f\"âŒ Timestamp parsing error: {e}\")\n",
    "                \n",
    "        else:\n",
    "            results[file_name] = {\n",
    "                'status': 'no_timestamp',\n",
    "                'columns': columns,\n",
    "                'num_rows': len(df)\n",
    "            }\n",
    "            print(f\"âŒ No timestamp column found\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        results[file_name] = {'status': 'not_found'}\n",
    "        print(f\"âŒ File not found\")\n",
    "    except Exception as e:\n",
    "        results[file_name] = {'status': 'error', 'error': str(e)}\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Column consistency check\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"COLUMN CONSISTENCY ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "if all_columns:\n",
    "    # Check if all have same columns\n",
    "    first_cols = all_columns[0]\n",
    "    all_same = all(cols == first_cols for cols in all_columns)\n",
    "    \n",
    "    if all_same:\n",
    "        print(\"\\nâœ… All files have IDENTICAL column structure!\")\n",
    "        print(f\"   Columns: {sorted(first_cols)}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Files have DIFFERENT column structures!\")\n",
    "        \n",
    "        # Find common columns\n",
    "        common_cols = set.intersection(*all_columns) if all_columns else set()\n",
    "        print(f\"\\n   Common columns: {sorted(common_cols)}\")\n",
    "        \n",
    "        # Show differences\n",
    "        print(\"\\n   Differences by file:\")\n",
    "        for file_name, result in results.items():\n",
    "            if result['status'] == 'success' and 'columns' in result:\n",
    "                file_cols = set(result['columns'])\n",
    "                unique_cols = file_cols - common_cols\n",
    "                missing_cols = first_cols - file_cols\n",
    "                \n",
    "                if unique_cols or missing_cols:\n",
    "                    print(f\"\\n   {file_name}:\")\n",
    "                    if unique_cols:\n",
    "                        print(f\"     + Unique: {sorted(unique_cols)}\")\n",
    "                    if missing_cols:\n",
    "                        print(f\"     - Missing: {sorted(missing_cols)}\")\n",
    "\n",
    "# Date range analysis\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"DATE RANGE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "successful_files = [(name, res) for name, res in results.items() \n",
    "                   if res['status'] == 'success' and 'min_date' in res]\n",
    "\n",
    "if successful_files:\n",
    "    # Sort by start date\n",
    "    successful_files.sort(key=lambda x: x[1]['min_date'])\n",
    "    \n",
    "    print(\"\\nğŸ“… Date Ranges (sorted by start date):\")\n",
    "    print(f\"{'File':<50} {'Start Date':<20} {'End Date':<20} {'Days':<10} {'Rows':<10}\")\n",
    "    print(\"â”€\" * 110)\n",
    "    \n",
    "    for file_name, info in successful_files:\n",
    "        print(f\"{file_name:<50} {str(info['min_date']):<20} {str(info['max_date']):<20} \"\n",
    "              f\"{info['date_range_days']:<10} {info['num_rows']:<10,}\")\n",
    "    \n",
    "    # Find common date range\n",
    "    all_starts = [info['min_date'] for _, info in successful_files]\n",
    "    all_ends = [info['max_date'] for _, info in successful_files]\n",
    "    \n",
    "    common_start = max(all_starts)\n",
    "    common_end = min(all_ends)\n",
    "    common_days = (common_end - common_start).days\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Common Date Range (overlap of all files):\")\n",
    "    print(f\"   Start: {common_start}\")\n",
    "    print(f\"   End: {common_end}\")\n",
    "    print(f\"   Duration: {common_days} days\")\n",
    "    \n",
    "    if common_days < 0:\n",
    "        print(\"\\n   âš ï¸  WARNING: No common overlap period! Files don't share any dates.\")\n",
    "    else:\n",
    "        # Calculate how many rows each file would have in common range\n",
    "        print(f\"\\n   Estimated rows in common range:\")\n",
    "        for file_name, info in successful_files:\n",
    "            # Read the file again to count rows in common range\n",
    "            file_path = base_path / f\"{file_name}.csv\"\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            common_rows = len(df[(df['timestamp'] >= common_start) & (df['timestamp'] <= common_end)])\n",
    "            print(f\"     {file_name}: {common_rows:,} rows\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "total_files = len(file_names)\n",
    "successful = sum(1 for r in results.values() if r['status'] == 'success')\n",
    "failed = total_files - successful\n",
    "\n",
    "print(f\"\\nğŸ“Š File Status:\")\n",
    "print(f\"   Total files: {total_files}\")\n",
    "print(f\"   âœ… Successful: {successful}\")\n",
    "print(f\"   âŒ Failed: {failed}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\n   Failed files:\")\n",
    "    for name, res in results.items():\n",
    "        if res['status'] != 'success':\n",
    "            print(f\"     - {name}: {res['status']}\")\n",
    "            if 'error' in res:\n",
    "                print(f\"       Error: {res['error']}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"RECOMMENDATIONS FOR ENSEMBLE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "if all_same and common_days > 0:\n",
    "    print(\"\\nâœ… All files are ready for ensemble methods!\")\n",
    "    print(f\"   - Use date range: {common_start} to {common_end}\")\n",
    "    print(f\"   - This ensures all models have predictions for the same period\")\n",
    "elif not all_same:\n",
    "    print(\"\\nâš ï¸  Fix column inconsistencies before ensemble\")\n",
    "elif common_days <= 0:\n",
    "    print(\"\\nâš ï¸  No common date overlap - check your train/test splits\")\n",
    "\n",
    "# Save detailed report\n",
    "report_path = base_path / \"predictions_analysis_report.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"PREDICTION FILES ANALYSIS REPORT\\n\")\n",
    "    f.write(f\"Generated: {datetime.now()}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for file_name, result in results.items():\n",
    "        f.write(f\"{file_name}:\\n\")\n",
    "        for key, value in result.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Detailed report saved to: predictions_analysis_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f204605",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final problems list\n",
    "\n",
    "1)lgbm has a column name mismatch\n",
    "2) xgboost has a column name mismatch\n",
    "\n",
    "3) gru has bad validation date start\n",
    "4) lgbm has no date start at all\n",
    "\n",
    "5) CNN-LSTM hybrid model has no predictions file at all - need to re run the model on a better combination then make the csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6da1b",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61028ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIXING DATE FORMAT + COLUMN ORDER\n",
      "================================================================================\n",
      "\n",
      "Processing lgbm_predictions_formatted...\n",
      "Original format sample: 08/08/2023 00:00\n",
      "Converted: 2023-08-08 00:00:00\n",
      "Date range: 2023-08-08 00:00:00 to 2025-03-28 00:00:00\n",
      "âœ“ Backup saved to: lgbm_predictions_formatted_backup.csv\n",
      "âœ— Error processing lgbm_predictions_formatted: [Errno 13] Permission denied: 'C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Coding_projects\\\\stock_market_prediction\\\\Stock-Market-Prediction\\\\Final_runs_csv\\\\lgbm_predictions_formatted.csv'\n",
      "\n",
      "Processing xgboost_predictions...\n",
      "Original format sample: 07/05/2023 12:00\n",
      "âœ— Error processing xgboost_predictions: time data \"13/05/2023 00:00\" doesn't match format \"%m/%d/%Y %H:%M\", at position 33. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "\n",
      "================================================================================\n",
      "All fixes complete. You're ready to run ensemble voting!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path\n",
    "base_path = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\Final_runs_csv\"\n",
    "\n",
    "# Files with per-file date parsing options\n",
    "files_to_fix = {\n",
    "    \"lgbm_predictions_formatted\": {\"dayfirst\": True},\n",
    "    \"xgboost_predictions\": {\"dayfirst\": False}  # assumed US format\n",
    "}\n",
    "\n",
    "# Standard column order expected for ensemble\n",
    "standard_cols = ['timestamp', 'prob_up', 'prob_down', 'winning_prob', 'prediction', 'actual']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIXING DATE FORMAT + COLUMN ORDER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for file_name, parse_opts in files_to_fix.items():\n",
    "    file_path = os.path.join(base_path, f\"{file_name}.csv\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nProcessing {file_name}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Find timestamp column\n",
    "        timestamp_col = next((col for col in df.columns if 'timestamp' in col.lower() or 'date' in col.lower()), None)\n",
    "        if not timestamp_col:\n",
    "            print(f\"âœ— No timestamp column found.\")\n",
    "            continue\n",
    "\n",
    "        # Show current format and parse\n",
    "        print(f\"Original format sample: {df[timestamp_col].iloc[0]}\")\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], **parse_opts)\n",
    "        print(f\"Converted: {df[timestamp_col].iloc[0]}\")\n",
    "        print(f\"Date range: {df[timestamp_col].min()} to {df[timestamp_col].max()}\")\n",
    "\n",
    "        # Backup original\n",
    "        backup_path = file_path.replace(\".csv\", \"_backup.csv\")\n",
    "        df.to_csv(backup_path, index=False)\n",
    "        print(f\"âœ“ Backup saved to: {os.path.basename(backup_path)}\")\n",
    "\n",
    "        # Reorder if all standard columns are present\n",
    "        if set(standard_cols).issubset(df.columns):\n",
    "            df = df[standard_cols]\n",
    "        else:\n",
    "            print(\"âš ï¸ Skipped column reordering: missing expected columns.\")\n",
    "\n",
    "        # Save final\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"âœ“ Fixed and saved: {file_name}.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All fixes complete. You're ready to run ensemble voting!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c055e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Checking lgbm_predictions_formatted.csv\n",
      "   â†’ 0 of 3,589 rows failed to parse\n",
      "\n",
      "   Raw strings (first 5):\n",
      "['08/08/2023 00:00', '08/08/2023 04:00', '08/08/2023 08:00', '08/08/2023 12:00', '08/08/2023 16:00']\n",
      "\n",
      "   Parsed values (first 5):\n",
      "[Timestamp('2023-08-08 00:00:00+0000', tz='UTC'), Timestamp('2023-08-08 04:00:00+0000', tz='UTC'), Timestamp('2023-08-08 08:00:00+0000', tz='UTC'), Timestamp('2023-08-08 12:00:00+0000', tz='UTC'), Timestamp('2023-08-08 16:00:00+0000', tz='UTC')]\n",
      "\n",
      "   âœ… earliest timestamp: 2023-08-08 00:00:00+00:00\n",
      "\n",
      "ğŸ” Checking xgboost_predictions.csv\n",
      "   â†’ 2,527 of 4,144 rows failed to parse\n",
      "\n",
      "   Raw strings (first 5):\n",
      "['07/05/2023 12:00', '07/05/2023 16:00', '07/05/2023 20:00', '08/05/2023 00:00', '08/05/2023 04:00']\n",
      "\n",
      "   Parsed values (first 5):\n",
      "[Timestamp('2023-07-05 12:00:00+0000', tz='UTC'), Timestamp('2023-07-05 16:00:00+0000', tz='UTC'), Timestamp('2023-07-05 20:00:00+0000', tz='UTC'), Timestamp('2023-08-05 00:00:00+0000', tz='UTC'), Timestamp('2023-08-05 04:00:00+0000', tz='UTC')]\n",
      "\n",
      "   âŒ examples of unparsable values:\n",
      "       13/05/2023 00:00\n",
      "       13/05/2023 04:00\n",
      "       13/05/2023 08:00\n",
      "       13/05/2023 12:00\n",
      "       13/05/2023 16:00\n",
      "\n",
      "ğŸ” Checking xgboost_predictions.csv\n",
      "   â†’ 0 of 4,144 rows failed to parse\n",
      "\n",
      "   Raw strings (first 5):\n",
      "['07/05/2023 12:00', '07/05/2023 16:00', '07/05/2023 20:00', '08/05/2023 00:00', '08/05/2023 04:00']\n",
      "\n",
      "   Parsed values (first 5):\n",
      "[Timestamp('2023-05-07 12:00:00+0000', tz='UTC'), Timestamp('2023-05-07 16:00:00+0000', tz='UTC'), Timestamp('2023-05-07 20:00:00+0000', tz='UTC'), Timestamp('2023-05-08 00:00:00+0000', tz='UTC'), Timestamp('2023-05-08 04:00:00+0000', tz='UTC')]\n",
      "\n",
      "   âœ… earliest timestamp: 2023-05-07 12:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def check_timestamp_csv(csv_path: str, dayfirst=False):\n",
    "    path = Path(csv_path)\n",
    "    print(f\"\\nğŸ” Checking {path.name}\")\n",
    "    try:\n",
    "        # read only timestamp column\n",
    "        ts = pd.read_csv(path, usecols=['timestamp'])['timestamp']\n",
    "    except ValueError:\n",
    "        print(\"âŒ  No column literally named 'timestamp'. Double-check header.\")\n",
    "        return\n",
    "    \n",
    "    # parse\n",
    "    ts_parsed = pd.to_datetime(ts, dayfirst=dayfirst, errors='coerce', utc=True)\n",
    "    \n",
    "    bad = ts_parsed.isna().sum()\n",
    "    print(f\"   â†’ {bad:,} of {len(ts_parsed):,} rows failed to parse\")\n",
    "    \n",
    "    # show a few examples\n",
    "    print(\"\\n   Raw strings (first 5):\")\n",
    "    print(ts.head().tolist())\n",
    "    print(\"\\n   Parsed values (first 5):\")\n",
    "    print(ts_parsed.head().tolist())\n",
    "    \n",
    "    if bad == 0:\n",
    "        print(f\"\\n   âœ… earliest timestamp: {ts_parsed.min()}\")\n",
    "    else:\n",
    "        # show up to 5 examples of unparsable rows\n",
    "        bad_examples = ts[ts_parsed.isna()].head().tolist()\n",
    "        print(\"\\n   âŒ examples of unparsable values:\")\n",
    "        for s in bad_examples:\n",
    "            print(\"      \", s)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "BASE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "            r\"\\Stock-Market-Prediction\\Final_runs_csv\")\n",
    "\n",
    "check_timestamp_csv(BASE / \"lgbm_predictions_formatted.csv\", dayfirst=True)\n",
    "check_timestamp_csv(BASE / \"xgboost_predictions.csv\", dayfirst=False)   # try False first\n",
    "check_timestamp_csv(BASE / \"xgboost_predictions.csv\", dayfirst=True)    # â€¦then True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8edb8",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c60c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
