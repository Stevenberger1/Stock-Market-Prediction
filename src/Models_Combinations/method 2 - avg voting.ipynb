{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3730f0f",
   "metadata": {},
   "source": [
    "# in this file we will check the avg voting method of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709e74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:61: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  ts_dmy = pd.to_datetime(raw, dayfirst=True,  utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:61: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  ts_dmy = pd.to_datetime(raw, dayfirst=True,  utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:61: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  ts_dmy = pd.to_datetime(raw, dayfirst=True,  utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:60: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ts_mdy = pd.to_datetime(raw, dayfirst=False, utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:60: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ts_mdy = pd.to_datetime(raw, dayfirst=False, utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:61: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  ts_dmy = pd.to_datetime(raw, dayfirst=True,  utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:60: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ts_mdy = pd.to_datetime(raw, dayfirst=False, utc=True, errors=\"coerce\")\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5968\\3355244158.py:60: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ts_mdy = pd.to_datetime(raw, dayfirst=False, utc=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading prediction files â€¦\n",
      "Rows after inner-join & trim: 3,105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [00:03<00:00, 273.78combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ completed 1013 soft-vote combos in 3.7s\n",
      "\n",
      "Top-10 soft ensembles by F1:\n",
      "                    models  n_models  precision   recall       f1\n",
      "                  gru+lgbm         2   0.637500 0.031481 0.060000\n",
      "                gru+rf+tcn         3   0.631222 0.172222 0.270611\n",
      "              gru+lgbm+tcn         3   0.630435 0.035802 0.067757\n",
      "                    gru+rf         2   0.628770 0.167284 0.264261\n",
      "     catboost+gru+lgbm+tcn         4   0.625000 0.098765 0.170576\n",
      "    catboost+gru+lgbm+lstm         4   0.624464 0.179630 0.279003\n",
      "catboost+gru+lgbm+lstm+tcn         5   0.623950 0.183333 0.283397\n",
      "           catboost+gru+rf         3   0.619863 0.223457 0.328494\n",
      "         catboost+gru+lgbm         3   0.619433 0.094444 0.163899\n",
      "               gru+lgbm+rf         3   0.619342 0.185802 0.285850\n",
      "             gru+lgbm+lstm         3   0.618384 0.137037 0.224356\n",
      "                  gru+lstm         2   0.617544 0.108642 0.184777\n",
      "       catboost+gru+rf+tcn         4   0.617450 0.227160 0.332130\n",
      "     catboost+gru+lstm+tcn         4   0.617284 0.154321 0.246914\n",
      "  gru+lgbm+rf+tcn+cnn_lstm         5   0.616788 0.208642 0.311808\n",
      "\n",
      "Best single model : tcn  F1=0.6857\n",
      "Best soft ensemble: gru+lgbm  F1=0.0600\n",
      "Improvement       : -0.6257 (-91.25%)\n",
      "\n",
      "ðŸ’¾ Files written: soft_vote_results.csv, individual_model_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Soft-Voting Grid Search (10 models, combos 2â€“10) â€“ FINAL\n",
    "=======================================================\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "soft_vote_results.csv          â€“ metrics for 1 013 soft-vote subsets\n",
    "individual_model_metrics.csv   â€“ metrics for each single model\n",
    "\"\"\"\n",
    "import itertools, sys, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONFIG\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE = Path(r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\"\n",
    "            r\"\\Stock-Market-Prediction\\Final_runs_csv\")\n",
    "\n",
    "FILES = {\n",
    "    \"catboost\" : \"catboost_trial57_predictions.csv\",\n",
    "    \"cnn\"      : \"cnn_predictions.csv\",\n",
    "    \"gru\"      : \"gru_trial28_f05_preds.csv\",\n",
    "    \"lgbm\"     : \"lgbm_predictions_formatted_backup.csv\",\n",
    "    \"logreg\"   : \"logisticreg_validation_predictions.csv\",\n",
    "    \"lstm\"     : \"lstm_test_predictions.csv\",\n",
    "    \"rf\"       : \"RandomForest_predictions_custom_high_precision.csv\",\n",
    "    \"tcn\"      : \"TCN_Trial_36_predictions.csv\",\n",
    "    \"xgb\"      : \"xgboost_predictions_fixed.csv\",\n",
    "    \"cnn_lstm\" : \"cnn_lstm_val_preds_20250614_142329.csv\",\n",
    "}\n",
    "\n",
    "THRESH = 0.50   # avg_prob > THRESH â†’ UP\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. LOAD & ALIGN\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "earliest = {}\n",
    "\n",
    "def load_one(path: Path, key: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return dataframe with robust timestamp parse:\n",
    "        timestamp | <key> (prob_up) | <key>_actual\n",
    "    Tries both dayfirst=False and dayfirst=True, keeps the better parse.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    need = {\"timestamp\", \"prob_up\", \"actual\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        sys.exit(f\"âŒ {path.name}: missing {miss}\")\n",
    "\n",
    "    raw = df[\"timestamp\"]\n",
    "    ts_mdy = pd.to_datetime(raw, dayfirst=False, utc=True, errors=\"coerce\")\n",
    "    ts_dmy = pd.to_datetime(raw, dayfirst=True,  utc=True, errors=\"coerce\")\n",
    "\n",
    "    if ts_mdy.notna().sum() == ts_dmy.notna().sum() == 0:\n",
    "        sys.exit(f\"âŒ unparsable timestamps in {path.name}\")\n",
    "\n",
    "    ts = ts_dmy if ts_dmy.notna().sum() > ts_mdy.notna().sum() else ts_mdy\n",
    "    earliest[key] = ts.min()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"timestamp\": ts,\n",
    "        key: df[\"prob_up\"].astype(np.float32),\n",
    "        f\"{key}_actual\": df[\"actual\"].astype(np.uint8)\n",
    "    })\n",
    "\n",
    "print(\"ðŸ”§ Loading prediction files â€¦\")\n",
    "merged = None\n",
    "for key, fname in FILES.items():\n",
    "    fp = BASE / fname\n",
    "    if not fp.exists():\n",
    "        sys.exit(f\"âŒ missing {fp}\")\n",
    "    part = load_one(fp, key)\n",
    "    merged = part if merged is None else merged.merge(part, on=\"timestamp\", how=\"inner\")\n",
    "\n",
    "# trim to latest common start date\n",
    "start_cut = max(earliest.values())\n",
    "merged = merged[merged[\"timestamp\"] >= start_cut].reset_index(drop=True)\n",
    "print(f\"Rows after inner-join & trim: {len(merged):,}\")\n",
    "\n",
    "# verify identical actuals\n",
    "act_cols = [c for c in merged.columns if c.endswith(\"_actual\")]\n",
    "if not (merged[act_cols].nunique(axis=1) == 1).values.all():\n",
    "    sys.exit(\"âŒ 'actual' columns differ â€“ fix before continuing\")\n",
    "\n",
    "y_true = merged[act_cols[0]].to_numpy(dtype=np.uint8)\n",
    "merged.drop(columns=act_cols, inplace=True)\n",
    "\n",
    "model_keys = list(FILES.keys())\n",
    "prob_mat = merged[model_keys].to_numpy(dtype=np.float32)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. EXHAUSTIVE SOFT-VOTE SEARCH\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def avg_vote(mat: np.ndarray) -> np.ndarray:\n",
    "    return (mat.mean(axis=1) > THRESH).astype(np.uint8)\n",
    "\n",
    "n = len(model_keys)\n",
    "total = (2**n) - 1 - n\n",
    "results = []\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=total, unit=\"combo\") as bar:\n",
    "    for k in range(2, n+1):\n",
    "        for idx in itertools.combinations(range(n), k):\n",
    "            y_pred = avg_vote(prob_mat[:, idx])\n",
    "\n",
    "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "            rec  = recall_score   (y_true, y_pred, zero_division=0)\n",
    "            f1   = f1_score       (y_true, y_pred, zero_division=0)\n",
    "            acc  = accuracy_score(y_true, y_pred)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "            results.append({\n",
    "                \"models\"   : \"+\".join(model_keys[i] for i in idx),\n",
    "                \"n_models\" : k,\n",
    "                \"precision\": round(prec, 6),\n",
    "                \"recall\"   : round(rec, 6),\n",
    "                \"f1\"       : round(f1, 6),\n",
    "                \"accuracy\" : round(acc, 6),\n",
    "                \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn\n",
    "            })\n",
    "            bar.update(1)\n",
    "\n",
    "print(f\"ðŸ completed {total} soft-vote combos in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. SINGLE-MODEL METRICS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "indiv = []\n",
    "for i, name in enumerate(model_keys):\n",
    "    y_pred = (prob_mat[:, i] > THRESH).astype(np.uint8)\n",
    "    indiv.append({\n",
    "        \"models\": name, \"n_models\": 1,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\"   : recall_score  (y_true, y_pred, zero_division=0),\n",
    "        \"f1\"       : f1_score      (y_true, y_pred, zero_division=0),\n",
    "        \"accuracy\" : accuracy_score(y_true, y_pred)\n",
    "    })\n",
    "indiv_df = pd.DataFrame(indiv).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. SAVE OUTPUTS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "out_soft = BASE / \"soft_vote_results.csv\"\n",
    "out_ind  = BASE / \"individual_model_metrics.csv\"\n",
    "\n",
    "(pd.DataFrame(results)\n",
    "   .sort_values([\"precision\", \"recall\",\"f1\"],\n",
    "\n",
    "                ascending=[False, False, True])\n",
    "   .to_csv(out_soft, index=False))\n",
    "indiv_df.to_csv(out_ind, index=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. CONSOLE SUMMARY\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nTop-10 soft ensembles by F1:\")\n",
    "top10 = pd.read_csv(out_soft).head(15)\n",
    "print(top10[[\"models\",\"n_models\",\"precision\",\"recall\",\"f1\"]].to_string(index=False))\n",
    "\n",
    "best_single = indiv_df.iloc[0]\n",
    "best_combo  = top10.iloc[0]\n",
    "improv = best_combo[\"f1\"] - best_single[\"f1\"]\n",
    "\n",
    "print(\"\\nBest single model :\",\n",
    "      f\"{best_single['models']}  F1={best_single['f1']:.4f}\")\n",
    "print(\"Best soft ensemble:\",\n",
    "      f\"{best_combo['models']}  F1={best_combo['f1']:.4f}\")\n",
    "print(f\"Improvement       : {improv:+.4f} \"\n",
    "      f\"({improv/best_single['f1']*100:+.2f}%)\")\n",
    "print(f\"\\nðŸ’¾ Files written: {out_soft.name}, {out_ind.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
