{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains functions related to loading, cleaning, and preproessing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to where the data was dowloanded from : https://www.cryptodatadownload.com/data/poloniex/#google_vignette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code loads the data from a path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(csv_path):\n",
    "    # If the first row of your CSV is just a note (e.g. “Data provided by...”),\n",
    "    # then skip it with skiprows=1. If not, set skiprows=0 or remove that parameter.\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        skiprows=1,            # Adjust if your first row is already column headers\n",
    "        parse_dates=['date'],  # Tells Pandas to convert the 'date' column to datetime64 objects.\n",
    "        infer_datetime_format=True  #Lets Pandas guess the date format more efficiently, which can speed up parsing if your file is big.\n",
    "\n",
    "    )\n",
    "    \n",
    "    #Sets the \"date\" column as the index of the DataFrame.\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    #sorts the DataFrame by the date/time index, just in case the rows were out of order.\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of the load_data function \n",
    "path_to_csv = r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\raw\\2025-2015 Gemini_BTCUSD_1h.csv\"\n",
    "df_prices = load_data(path_to_csv)\n",
    "    \n",
    "print(\"DataFrame shape:\", df_prices.shape)\n",
    "print(df_prices.head())\n",
    "print(df_prices.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "series models:\n",
    "\n",
    "Date/Time : Usually kept as an index.\n",
    "\n",
    "Open, High, Low, Close (OHLC) : Core price features for any trading or forecasting model.\n",
    "\n",
    "Volume : Overall volume is often a good indicator of market interest and liquidity.\n",
    "\n",
    "Trade Count (Optional) : Helps distinguish whether volume came from many small trades or fewer large trades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code Keep only the columns that are most useful for ur dataframe\n",
    "\n",
    "def keep_important_columns(df):\n",
    "\n",
    "    # Define the columns you want to keep\n",
    "    columns_to_keep = [\n",
    "        \"open\",\n",
    "        \"high\",\n",
    "        \"low\",\n",
    "        \"close\",\n",
    "        \"Volume BTC\",  \n",
    "    ]\n",
    "    \n",
    "    # Intersect with what actually exists in your DataFrame to avoid KeyErrors\n",
    "    existing_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "    \n",
    "    # Create a reduced DataFrame with only these columns\n",
    "    df_reduced = df[existing_cols].copy()\n",
    "    return df_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of keep_important_columns\n",
    "df_prices = keep_important_columns(df_prices)\n",
    "print(df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "df_prices.to_csv('gemini_btc_data_v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a data-quality checker , It identifies common issues in a crypto price DataFrame, such as:\n",
    "\n",
    "Missing Values: Looks for NaN/None in columns.\n",
    "\n",
    "Duplicate Rows: Checks whether any exact duplicates exist.\n",
    "\n",
    "Negative or Zero Price/Volume: Flags rows where prices or volumes are invalid.\n",
    "\n",
    "Out-of-Order Date Index: Ensures that your date index is strictly increasing (important for time-series).\n",
    "\n",
    "Unexpected Data Types: Verifies that “open”, “high”, “low”, “close”, and “volume” columns are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_data_problems(df):\n",
    "    problems_summary = {}\n",
    "\n",
    "    # 1) Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\">> MISSING VALUES found per column:\")\n",
    "        print(missing[missing > 0])\n",
    "        problems_summary['missing_values'] = missing[missing > 0].to_dict()\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "        problems_summary['missing_values'] = {}\n",
    "    \n",
    "    # 2) Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\">> DUPLICATE ROWS found: {duplicate_count}\")\n",
    "        problems_summary['duplicate_rows'] = duplicate_count\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "        problems_summary['duplicate_rows'] = 0\n",
    "    \n",
    "    # 3) Negative or zero price/volume checks\n",
    "    price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in df.columns]\n",
    "    volume_cols = [col for col in ['Volume BTC', 'Volume USDT', 'Volume USD', 'volume'] if col in df.columns]\n",
    "\n",
    "    invalid_prices = {}\n",
    "    for col in price_cols:\n",
    "        non_pos = (df[col] <= 0).sum()\n",
    "        if non_pos > 0:\n",
    "            invalid_prices[col] = non_pos\n",
    "    \n",
    "    invalid_volumes = {}\n",
    "    for col in volume_cols:\n",
    "        non_pos = (df[col] <= 0).sum()\n",
    "        if non_pos > 0:\n",
    "            invalid_volumes[col] = non_pos\n",
    "\n",
    "    if invalid_prices:\n",
    "        print(\">> INVALID (≤0) PRICE VALUES found:\")\n",
    "        for c, count in invalid_prices.items():\n",
    "            print(f\"   Column '{c}': {count} rows\")\n",
    "        problems_summary['invalid_prices'] = invalid_prices\n",
    "    else:\n",
    "        print(\"No invalid (zero/negative) price values found.\")\n",
    "        problems_summary['invalid_prices'] = {}\n",
    "    \n",
    "    if invalid_volumes:\n",
    "        print(\">> INVALID (≤0) VOLUME VALUES found:\")\n",
    "        for c, count in invalid_volumes.items():\n",
    "            print(f\"   Column '{c}': {count} rows\")\n",
    "        problems_summary['invalid_volumes'] = invalid_volumes\n",
    "    else:\n",
    "        print(\"No invalid (zero/negative) volume values found.\")\n",
    "        problems_summary['invalid_volumes'] = {}\n",
    "\n",
    "    # 4) Out-of-order date index check (only if index is datetime-like)\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "            print(\">> The date index is NOT strictly increasing. Some timestamps may be out of order.\")\n",
    "            problems_summary['date_order'] = \"Not strictly increasing\"\n",
    "        else:\n",
    "            print(\"Date index is in ascending order (strictly increasing).\")\n",
    "            problems_summary['date_order'] = \"Ascending\"\n",
    "    else:\n",
    "        print(\"Index is not a DatetimeIndex (skipping date-order check).\")\n",
    "        problems_summary['date_order'] = None\n",
    "\n",
    "    # 5) Data-type checks for numeric columns\n",
    "    numeric_checks = {}\n",
    "    for col in price_cols + volume_cols:\n",
    "        if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numeric_checks[col] = \"Non-numeric type\"\n",
    "    if numeric_checks:\n",
    "        print(\">> NON-NUMERIC COLUMNS found (expected numeric):\")\n",
    "        for c, msg in numeric_checks.items():\n",
    "            print(f\"   Column '{c}' => {msg}\")\n",
    "    else:\n",
    "        print(\"All price/volume columns have numeric types.\")\n",
    "    problems_summary['non_numeric_columns'] = numeric_checks\n",
    "\n",
    "    print(\"\\n=== DATA QUALITY CHECK COMPLETE ===\\n\")\n",
    "    return problems_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run the data-quality checker using your df_prices DataFrame\n",
    "problems_report = check_data_problems(df_cleaned)\n",
    "\n",
    "# 4. If you want to do something programmatic with the results:\n",
    "print(\"Problems Summary (as dict):\")\n",
    "print(problems_report)\n",
    "\n",
    "# Additionally, display the rows where invalid (≤0) PRICE VALUES are found\n",
    "if 'open' in df_cleaned.columns:\n",
    "    invalid_open = df_cleaned[df_cleaned['open'] <= 0]\n",
    "    print(\"\\nRows with invalid 'open' values (≤0):\")\n",
    "    print(invalid_open)\n",
    "\n",
    "if 'low' in df_cleaned.columns:\n",
    "    invalid_low = df_cleaned[df_cleaned['low'] <= 0]\n",
    "    print(\"\\nRows with invalid 'low' values (≤0):\")\n",
    "    print(invalid_low)\n",
    "    \n",
    "\n",
    "'''\n",
    "No missing values detected.\n",
    ">> DUPLICATE ROWS found: 480\n",
    ">> INVALID (≤0) PRICE VALUES found:\n",
    "   Column 'open': 1 rows\n",
    "   Column 'low': 1 rows\n",
    ">> INVALID (≤0) VOLUME VALUES found:\n",
    "   Column 'Volume BTC': 1178 rows\n",
    "Date index is in ascending order (strictly increasing).\n",
    "All price/volume columns have numeric types.\n",
    "\n",
    "=== DATA QUALITY CHECK COMPLETE ===\n",
    "\n",
    "Problems Summary (as dict):\n",
    "{'missing_values': {}, 'duplicate_rows': 480, 'invalid_prices': {'open': 1, 'low': 1}, 'invalid_volumes': {'Volume BTC': 1178}, 'date_order': 'Ascending', 'non_numeric_columns': {}}\n",
    "\n",
    "Rows with invalid 'open' values (≤0):\n",
    "                     open   high  low  close  Volume BTC\n",
    "date                                                    \n",
    "2015-10-08 13:00:00   0.0  245.0  0.0  245.0    0.606654\n",
    "\n",
    "Rows with invalid 'low' values (≤0):\n",
    "                     open   high  low  close  Volume BTC\n",
    "date                                                    \n",
    "2015-10-08 13:00:00   0.0  245.0  0.0  245.0    0.606654\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code reads the dataframe from the saved csv\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\ADMIN\\Desktop\\Coding_projects\\stock_market_prediction\\Stock-Market-Prediction\\data\\processed\\gemini_btc_data_v1.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True\n",
    ")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return every row that has a duplicate somewhere else\n",
    "duplicates_all = df[df.duplicated(keep=False)]\n",
    "\n",
    "print(f\"Total rows considered duplicates (including the first occurrence): {len(duplicates_all)}\")\n",
    "print(duplicates_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows have been saved to duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the duplicate rows to a CSV file\n",
    "#duplicates_all.to_csv(\"duplicates.csv\", index=True)\n",
    "print(\"Duplicate rows have been saved to duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping duplicates, the dataframe has 82520 rows.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows (keeps the first occurrence)\n",
    "df_dedup = df.drop_duplicates()\n",
    "\n",
    "print(\"After dropping duplicates, the dataframe has\", len(df_dedup), \"rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows considered duplicates (including the first occurrence): 0\n",
      "Empty DataFrame\n",
      "Columns: [open, high, low, close, Volume BTC]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# This will return every row that has a duplicate somewhere else\n",
    "duplicates_all = df_dedup[df_dedup.duplicated(keep=False)]\n",
    "\n",
    "print(f\"Total rows considered duplicates (including the first occurrence): {len(duplicates_all)}\")\n",
    "print(duplicates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is an example of a separate Jupyter Notebook cell that first identifies the row(s) where the price value (for example, in the 'open' or 'low' column)\n",
    "# 0 is 0 and then removes it from the DataFrame:\n",
    "\n",
    "# Identify the row(s) where 'open' or 'low' is 0 (or less)\n",
    "invalid_price_rows = df_dedup[(df_dedup['open'] <= 0) | (df_dedup['low'] <= 0)]\n",
    "print(\"Rows with invalid price values (open or low ≤ 0):\")\n",
    "print(invalid_price_rows)\n",
    "\n",
    "# Remove these rows from the DataFrame\n",
    "df_cleaned = df_dedup[(df_dedup['open'] > 0) & (df_dedup['low'] > 0)]\n",
    "print(\"\\nDataFrame after removing the row(s) with invalid price values:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "'''\n",
    "rows with invalid price values (open or low ≤ 0):\n",
    "                     open   high  low  close  Volume BTC\n",
    "date                                                    \n",
    "2015-10-08 13:00:00   0.0  245.0  0.0  245.0    0.606654\n",
    "\n",
    "DataFrame after removing the row(s) with invalid price values:\n",
    "                         open      high       low     close  Volume BTC\n",
    "date                                                                   \n",
    "2015-10-08 14:00:00    245.00    245.00    244.50    245.00    4.453649\n",
    "2015-10-08 15:00:00    245.00    245.00    244.92    244.92    3.016926\n",
    "2015-10-08 16:00:00    244.92    244.92    244.25    244.25    3.895252\n",
    "2015-10-08 17:00:00    244.25    244.99    244.02    244.99    3.920632\n",
    "2015-10-08 18:00:00    244.99    244.99    244.00    244.00    3.690472\n",
    "...                       ...       ...       ...       ...         ...\n",
    "2025-03-27 19:00:00  87192.76  87433.40  87000.00  87063.30   40.926363\n",
    "2025-03-27 20:00:00  87063.30  87335.11  86930.18  87310.36   18.377515\n",
    "2025-03-27 21:00:00  87310.36  87710.97  87226.71  87551.64   18.607755\n",
    "2025-03-27 22:00:00  87551.64  87603.05  87298.29  87346.45   10.438136\n",
    "2025-03-27 23:00:00  87346.45  87346.45  87101.82  87223.20   12.074964\n",
    "\n",
    "[82519 rows x 5 columns]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "df_cleaned.to_csv('gemini_btc_data_final_version')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count duplicates by year and month\n",
    "# Ensure index is datetime (just in case)\n",
    "duplicates_all.index = pd.to_datetime(duplicates_all.index)\n",
    "\n",
    "# Create year and month columns\n",
    "duplicates_all['year'] = duplicates_all.index.year\n",
    "duplicates_all['month'] = duplicates_all.index.month\n",
    "\n",
    "# Group by year and month, then count rows\n",
    "duplicate_counts = duplicates_all.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "\n",
    "# Display\n",
    "print(duplicate_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code compares bettwen the 3 datasets\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Set your CSV file paths here, named for each exchange\n",
    "# -------------------------------------------------------------------\n",
    "poloniex_csv = r\"C:\\Users\\ADMIN\\Downloads\\Poloniex_BTCUSDT_1h.csv\"\n",
    "bitstamp_csv = r\"C:\\Users\\ADMIN\\Downloads\\Bitstamp_BTCUSD_1h (1).csv\"\n",
    "gemini_csv   = r\"C:\\Users\\ADMIN\\Downloads\\2025-2015 Gemini_BTCUSD_1h.csv\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Read each dataset into a DataFrame\n",
    "# -------------------------------------------------------------------\n",
    "df_poloniex = pd.read_csv(\n",
    "    poloniex_csv,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    sep=',',\n",
    "    parse_dates=['date'],\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "df_bitstamp = pd.read_csv(\n",
    "    bitstamp_csv,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    sep=',',\n",
    "    parse_dates=['date'],\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "df_gemini = pd.read_csv(\n",
    "    gemini_csv,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    sep=',',\n",
    "    parse_dates=['date'],\n",
    "    index_col='date'\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Align the DataFrames to the common hourly timestamps\n",
    "#    (only keep rows where all three have data)\n",
    "# -------------------------------------------------------------------\n",
    "common_index = df_poloniex.index.intersection(df_bitstamp.index).intersection(df_gemini.index)\n",
    "df_poloniex = df_poloniex.loc[common_index]\n",
    "df_bitstamp = df_bitstamp.loc[common_index]\n",
    "df_gemini   = df_gemini.loc[common_index]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Combine into one comparison table\n",
    "#    Renaming columns to reflect each exchange name clearly\n",
    "# -------------------------------------------------------------------\n",
    "comparison_df = pd.DataFrame(index=common_index)\n",
    "comparison_df['Poloniex_Close'] = df_poloniex['close']\n",
    "comparison_df['Bitstamp_Close'] = df_bitstamp['close']\n",
    "comparison_df['Gemini_Close']   = df_gemini['close']\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Calculate the price differences\n",
    "# -------------------------------------------------------------------\n",
    "comparison_df['Poloniex_minus_Bitstamp'] = comparison_df['Poloniex_Close'] - comparison_df['Bitstamp_Close']\n",
    "comparison_df['Bitstamp_minus_Gemini']   = comparison_df['Bitstamp_Close'] - comparison_df['Gemini_Close']\n",
    "comparison_df['Poloniex_minus_Gemini']   = comparison_df['Poloniex_Close'] - comparison_df['Gemini_Close']\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) Display the final comparison DataFrame (first 100 rows)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n=== Comparison of Poloniex, Bitstamp, and Gemini (Hourly) ===\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Optionally, you can save the comparison to a CSV file:\n",
    "# comparison_df.to_csv('comparison_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "#comparison_df.to_csv('comparison_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poloniex_minus_Bitstamp    -9.114675\n",
       "Bitstamp_minus_Gemini      -1.109679\n",
       "Poloniex_minus_Gemini     -10.224354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df[['Poloniex_minus_Bitstamp', 'Bitstamp_minus_Gemini', 'Poloniex_minus_Gemini']].mean()\n",
    "\n",
    "\n",
    "#Poloniex_minus_Bitstamp    -9.114675\n",
    "#Bitstamp_minus_Gemini      -1.109679\n",
    "#Poloniex_minus_Gemini     -10.224354\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poloniex_minus_Bitstamp    -9.114675\n",
    "Bitstamp_minus_Gemini      -1.109679\n",
    "Poloniex_minus_Gemini     -10.224354\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Poloniex_Close vs Bitstamp_Close ---\n",
      "  MAE : 31.4501\n",
      "  RMSE: 78.8900\n",
      "  Corr: 1.0000\n",
      "\n",
      "--- Bitstamp_Close vs Gemini_Close ---\n",
      "  MAE : 10.5856\n",
      "  RMSE: 31.9917\n",
      "  Corr: 1.0000\n",
      "\n",
      "--- Poloniex_Close vs Gemini_Close ---\n",
      "  MAE : 33.4455\n",
      "  RMSE: 84.3110\n",
      "  Corr: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pairs = [\n",
    "    ('Poloniex_Close', 'Bitstamp_Close'),\n",
    "    ('Bitstamp_Close', 'Gemini_Close'),\n",
    "    ('Poloniex_Close', 'Gemini_Close')\n",
    "]\n",
    "\n",
    "for colA, colB in pairs:\n",
    "    diffs = comparison_df[colA] - comparison_df[colB]\n",
    "    \n",
    "    mae  = np.mean(np.abs(diffs))\n",
    "    rmse = np.sqrt(np.mean(diffs**2))\n",
    "    corr = comparison_df[colA].corr(comparison_df[colB])\n",
    "    \n",
    "    print(f\"--- {colA} vs {colB} ---\")\n",
    "    print(f\"  MAE : {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  Corr: {corr:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Poloniex_Close vs Bitstamp_Close ---\n",
    "  MAE : 31.4501\n",
    "  RMSE: 78.8900\n",
    "  Corr: 1.0000\n",
    "\n",
    "--- Bitstamp_Close vs Gemini_Close ---\n",
    "  MAE : 10.5856\n",
    "  RMSE: 31.9917\n",
    "  Corr: 1.0000\n",
    "\n",
    "--- Poloniex_Close vs Gemini_Close ---\n",
    "  MAE : 33.4455\n",
    "  RMSE: 84.3110\n",
    "  Corr: 1.0000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
